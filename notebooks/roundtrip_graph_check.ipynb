{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-03T15:47:18.197469200Z",
     "start_time": "2023-11-03T15:47:14.127476200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "from py2neo.data import Relationship, Node\n",
    "from py2neo import Graph, NodeMatcher, RelationshipMatcher\n",
    "from typing import Iterable\n",
    "\n",
    "from wawr.llm_interface import KGPromptWriter, KGFeedback, KGGenerator, KGRecomposer\n",
    "\n",
    "\n",
    "OPENAI_API = \"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\"\n",
    "AURA_CONNECTION_URI = \"neo4j+s://6af62c90.databases.neo4j.io\"\n",
    "AURA_USERNAME = \"neo4j\"\n",
    "AURA_PASSWORD = \"sdjmcGlnKaiqYXUExuFLSZTY52KKSnv8LYJ4pJer4oo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#llm = OpenAI(model_name = 'gpt-3.5-turbo-16', openai_api_key=OPENAI_API)\n",
    "cllm = ChatOpenAI(openai_api_key=OPENAI_API, model_name='gpt-3.5-turbo-16k-0613')\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "n4jdriver = GraphDatabase.driver(\n",
    "    AURA_CONNECTION_URI,\n",
    "    auth=(AURA_USERNAME, AURA_PASSWORD)\n",
    ")\n",
    "\n",
    "graph = Graph(AURA_CONNECTION_URI, auth=(AURA_USERNAME, AURA_PASSWORD))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T15:47:18.245469Z",
     "start_time": "2023-11-03T15:47:18.200971200Z"
    }
   },
   "id": "b2e39714d0f5eaea"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "ds = pd.read_csv('../data/2023_09_20_17_39_lm.csv')\n",
    "ds['abstract'] = ds['abstract'].str.replace('\\n', ' ')\n",
    "ds['update_date'] = pd.to_datetime(ds['update_date'])\n",
    "ds = ds.sort_values(by='update_date', ascending=False)\n",
    "ds = ds[ds['update_date'].dt.year == 2023]\n",
    "with open('../data/kg_extraction_examples/1', 'r') as f:\n",
    "    examples = f.read()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T15:47:18.710971Z",
     "start_time": "2023-11-03T15:47:18.248975500Z"
    }
   },
   "id": "ed3071927ed6a01b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def query_one_abstract(abstract):\n",
    "\n",
    "    system_msg = f'You are an agent helping to build a knowledge graph of machine learning advancements extracted from research papers. You receive a research paper abstract. Answer with knowledge graph paths you would extract from it, in json format, as per the example below. Limit to at most 20 nodes. Keep names short. Name the nodes such that that you maximise the chances of consistent naming across multiple requests. As an example, for this abstract: \"{ds[\"abstract\"].iloc[2]}\" a part of the answer should look like this: . '\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=\"\"#system_msg\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content = examples + '\\\\nAbstract:\\\\n' + abstract + '\\\\nKnowledge graph representation:\\\\n'\n",
    "        ),\n",
    "    ]\n",
    "    response = cllm(messages, temperature=0.1).content\n",
    "    return response\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T15:47:18.724471200Z",
     "start_time": "2023-11-03T15:47:18.711977200Z"
    }
   },
   "id": "ad2338a8451788df"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KG:\n",
      " [\n",
      "{\"from\":{\"type\":\"model\", \"name\":\"diffusion model\"}, \"relation\":{\"name\":\"generate\", \"summary\":\"diffusion models demonstrate a remarkable capability for generating high-quality images\"}, \"to\":{\"type\":\"concept\",\"name\":\"high-quality images\"}},\n",
      "\n",
      "{\"from\":{\"type\":\"model\", \"name\":\"diffusion model\"}, \"relation\":{\"name\":\"replicate\", \"summary\":\"their tendency to 'replicate' training data raises privacy concerns\"}, \"to\":{\"type\":\"concept\",\"name\":\"privacy concerns\"}},\n",
      "\n",
      "{\"from\":{\"type\":\"model\", \"name\":\"caption generality score\"}, \"relation\":{\"name\":\"measures\", \"summary\":\"our paper first introduces a generality score that measures the caption generality\"}, \"to\":{\"type\":\"concept\",\"name\":\"caption generality\"}},\n",
      "\n",
      "{\"from\":{\"type\":\"model\", \"name\":\"large language model\"}, \"relation\":{\"name\":\"generalize\", \"summary\":\"employ large language model (LLM) to generalize training captions\"}, \"to\":{\"type\":\"concept\",\"name\":\"generalized captions\"}},\n",
      "\n",
      "{\"from\":{\"type\":\"model\", \"name\":\"dual fusion enhancement approach\"}, \"relation\":{\"name\":\"mitigate\", \"summary\":\"we leverage generalized captions and propose a novel dual fusion enhancement approach to mitigate the replication of diffusion models\"}, \"to\":{\"type\":\"model\",\"name\":\"diffusion model\"}},\n",
      "\n",
      "{\"from\":{\"type\":\"model\", \"name\":\"dual fusion enhancement approach\"}, \"relation\":{\"name\":\"mitigate\", \"summary\":\"we leverage generalized captions and propose a novel dual fusion enhancement approach to mitigate the replication of diffusion models\"}, \"to\":{\"type\":\"concept\",\"name\":\"replication\"}},\n",
      "\n",
      "{\"from\":{\"type\":\"model\", \"name\":\"dual fusion enhancement approach\"}, \"relation\":{\"name\":\"reduce\", \"summary\":\"our proposed methods can significantly reduce replication by 43.5% compared to the original diffusion model\"}, \"to\":{\"type\":\"model\",\"name\":\"diffusion model\"}},\n",
      "\n",
      "{\"from\":{\"type\":\"model\", \"name\":\"dual fusion enhancement approach\"}, \"relation\":{\"name\":\"maintain\", \"summary\":\"our proposed methods can significantly reduce replication by 43.5% compared to the original diffusion model while maintaining the diversity and quality of generations\"}, \"to\":{\"type\":\"concept\",\"name\":\"diversity\"}},\n",
      "\n",
      "{\"from\":{\"type\":\"model\", \"name\":\"dual fusion enhancement approach\"}, \"relation\":{\"name\":\"maintain\", \"summary\":\"our proposed methods can significantly reduce replication by 43.5% compared to the original diffusion model while maintaining the diversity and quality of generations\"}, \"to\":{\"type\":\"concept\",\"name\":\"quality of generations\"}}\n",
      "]\n",
      "Abstract:\n",
      "   While diffusion models demonstrate a remarkable capability for generating high-quality images, their tendency to `replicate' training data raises privacy concerns. Although recent research suggests that this replication may stem from the insufficient generalization of training data captions and duplication of training images, effective mitigation strategies remain elusive. To address this gap, our paper first introduces a generality score that measures the caption generality and employ large language model (LLM) to generalize training captions. Subsequently, we leverage generalized captions and propose a novel dual fusion enhancement approach to mitigate the replication of diffusion models. Our empirical results demonstrate that our proposed methods can significantly reduce replication by 43.5% compared to the original diffusion model while maintaining the diversity and quality of generations.  \n",
      "Recomposition:\n",
      " This paper introduces a novel dual fusion enhancement approach to mitigate the replication of diffusion models. Diffusion models have the remarkable capability of generating high-quality images, but their tendency to 'replicate' training data raises privacy concerns. To address this issue, the paper leverages generalized captions and proposes a dual fusion enhancement approach. The approach reduces replication by 43.5% compared to the original diffusion model while maintaining the diversity and quality of generations. The paper also introduces a generality score that measures the caption generality and employs a large language model (LLM) to generalize training captions. Overall, the proposed methods significantly reduce replication and maintain the diversity and quality of generations.\n",
      "Feedback:\n",
      " Similarity Score: 95%\n",
      "\n",
      "What does not match or is missing from the second abstract compared to the first:\n",
      "- The second abstract does not mention the gap in effective mitigation strategies for replication of diffusion models.\n",
      "- The second abstract does not explicitly state that the generality score is introduced in the paper.\n",
      "- The second abstract does not mention the use of a large language model (LLM) to generalize training captions as a part of addressing the replication issue.\n",
      "- The second abstract does not mention the empirical results that demonstrate the effectiveness of the proposed methods.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_writer = KGPromptWriter(cllm)\n",
    "feedback_generator = KGFeedback(cllm)\n",
    "kg_generator = KGGenerator(cllm )\n",
    "kg_recomposer = KGRecomposer(cllm)\n",
    "\n",
    "paper = ds.iloc[6]\n",
    "#instructions = prompt_writer.act(paper, None, None, None)\n",
    "#print(\"Instructions:\\n\", instructions)\n",
    "#generated_kg = kg_generator.act(paper, instructions)\n",
    "generated_kg = query_one_abstract(paper['abstract'])\n",
    "print(\"KG:\\n\", generated_kg)\n",
    "recomposed_abstract = kg_recomposer.act(generated_kg)\n",
    "print(\"Abstract:\\n\", paper['abstract'], \"\\nRecomposition:\\n\", recomposed_abstract)\n",
    "feedback = feedback_generator.act(paper['abstract'], recomposed_abstract)\n",
    "print(\"Feedback:\\n\", feedback)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T15:48:54.094240400Z",
     "start_time": "2023-11-03T15:48:31.993931900Z"
    }
   },
   "id": "47de0f70121e42a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4c68fec0019e14cf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
