{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-09T19:03:55.031672Z",
     "start_time": "2023-12-09T19:03:51.460648900Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "from py2neo.data import Relationship, Node\n",
    "from py2neo import Graph, NodeMatcher, RelationshipMatcher\n",
    "from typing import Iterable\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "\n",
    "OPENAI_API = \"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\"\n",
    "AURA_CONNECTION_URI = \"neo4j+ssc://6af62c90.databases.neo4j.io\"\n",
    "AURA_USERNAME = \"neo4j\"\n",
    "AURA_PASSWORD = \"sdjmcGlnKaiqYXUExuFLSZTY52KKSnv8LYJ4pJer4oo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#llm = OpenAI(model_name = 'gpt-3.5-turbo-16', openai_api_key=OPENAI_API)\n",
    "cllm = ChatOpenAI(openai_api_key=OPENAI_API, model_name='gpt-4-1106-preview')\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "n4jdriver = GraphDatabase.driver(\n",
    "    AURA_CONNECTION_URI,\n",
    "    auth=(AURA_USERNAME, AURA_PASSWORD)\n",
    ")\n",
    "\n",
    "graph = Graph(AURA_CONNECTION_URI, auth=(AURA_USERNAME, AURA_PASSWORD))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T19:03:56.766622200Z",
     "start_time": "2023-12-09T19:03:56.710861500Z"
    }
   },
   "id": "6329ed8c69303348"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "\"I am an AI language model created by OpenAI, known as ChatGPT. I don't have a personal name like a human, but you can call me ChatGPT or simply AI if you prefer. How can I assist you today?\""
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cllm.predict(\n",
    "        'What is your name?'\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T19:04:04.486339400Z",
     "start_time": "2023-12-09T19:03:59.001009600Z"
    }
   },
   "id": "9e0625665cc1cfcf"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "ds = pd.read_csv('../data/2023_12_09_16_24_lm.csv')\n",
    "ds['abstract'] = ds['abstract'].str.replace('\\n', ' ').str.lower()\n",
    "ds['date'] = pd.to_datetime(ds['date'])\n",
    "ds = ds.sort_values(by='date', ascending=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T19:04:08.741846600Z",
     "start_time": "2023-12-09T19:04:07.788343600Z"
    }
   },
   "id": "e0695e2ee2c690cb"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "ds = ds[ds['date'].dt.year >= 2014]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T19:04:10.034794900Z",
     "start_time": "2023-12-09T19:04:09.999296900Z"
    }
   },
   "id": "64c636c058719139"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(15664, 17)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T19:04:10.856555200Z",
     "start_time": "2023-12-09T19:04:10.794559300Z"
    }
   },
   "id": "35a11185248c8264"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# 2311.13095\n",
    "# 2311.12699\n",
    "ds[ds['id'] == '2311.12699']['abstract'].iloc[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T18:42:33.078301500Z",
     "start_time": "2023-12-09T18:41:32.803590900Z"
    }
   },
   "id": "638082021a20f8d8"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "with open('../data/kg_extraction_prompt.txt', 'r') as f:\n",
    "    examples = f.read()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T19:04:16.703843800Z",
     "start_time": "2023-12-09T19:04:16.673845Z"
    }
   },
   "id": "62543ef0f98b88d7"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def query_one_abstract(abstract):\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=\"\"\"\n",
    "                You are a helpful assistant generating a knowledge graph in json format, according to the examples received. Here are a few additional guidelines:\n",
    "\n",
    "                1. Transform node names to their most abstract form. For instance: \"large language models\", \"large language model (llm)\", \"very large language model\" should all resolve to \"large language model\"\n",
    "\n",
    "                2. Respect the style from the examples and reuse as many nodes as you can.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content = examples + abstract + '\\\\n\\\\nResulting knowledge graph in json format:```json\\\\n\\\\n'\n",
    "        ),\n",
    "    ]\n",
    "    with get_openai_callback() as usage:\n",
    "        response = cllm(messages, temperature=0.1)\n",
    "    return response, usage\n",
    "\n",
    "def get_or_create_node(node_dict, existing_nodes):\n",
    "    if 'name' not in node_dict or 'type' not in node_dict:\n",
    "        return None\n",
    "    to_ret = existing_nodes.get(node_dict['name']) or Node(node_dict['type'], name=node_dict['name'])\n",
    "    existing_nodes[node_dict['name']] = to_ret\n",
    "    return to_ret\n",
    "\n",
    "def create_relation(node1, rel, node2, existing_relations, properties):\n",
    "    to_ret = Relationship(node1, rel['name'], node2, **rel)\n",
    "    existing_relations.append(to_ret)\n",
    "    return to_ret\n",
    "\n",
    "def convert_to_kg(response, existing_nodes, existing_relations, properties):\n",
    "    try:\n",
    "        response_json = json.loads(response)\n",
    "    except Exception:\n",
    "        response_json = json.loads(response.replace('\\\\', ''))\n",
    "    for rel in response_json:\n",
    "        node1 = get_or_create_node(rel['from'], existing_nodes)\n",
    "        if not node1:\n",
    "            return \n",
    "        node2 = get_or_create_node(rel['to'], existing_nodes)\n",
    "        if not node2:\n",
    "            return \n",
    "        properties['summary'] = rel['relation'].get('summary', 'No sunmmary provided')\n",
    "        link = create_relation(node1, rel['relation'], node2, existing_relations, properties)\n",
    "         \n",
    "def get_existing_rel_names(existing_relations):\n",
    "    rel_names = [type(er).__name__ for er in existing_relations]\n",
    "    return set(rel_names)\n",
    "\n",
    "def get_existing_node_names(existing_nodes):\n",
    "    node_names = [er['name'] for er in existing_nodes.values()]\n",
    "    return set(node_names)\n",
    "\n",
    "def recreate_db(graph, nodes, relations):\n",
    "    tx=graph.begin()\n",
    "    graph.delete_all()\n",
    "    graph.commit(tx)\n",
    "    tx=graph.begin()\n",
    "    for i, elem in enumerate(list(nodes.values()) + relations):\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        tx.create(elem)\n",
    "    graph.commit(tx)\n",
    "    \n",
    "def relations_to_str(rel):\n",
    "    rel_itr = [rel,] if not isinstance(rel, Iterable) else rel        \n",
    "    to_ret = [f\"{r.nodes[0]} - {type(r).__name__} - {r.nodes[1]}\" for r in rel]\n",
    "    return to_ret[0] if not isinstance(rel, Iterable) else to_ret       "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T09:37:16.413250700Z",
     "start_time": "2023-12-10T09:37:16.354253700Z"
    }
   },
   "id": "d6678a249d321300"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "15718    2311.13601\n15717    2311.13581\n15716    2311.13577\n15715    2311.13565\n15714    2311.13562\n            ...    \n59        1402.1128\n58        1402.0574\n57        1401.5382\n56        1401.3896\n55        1401.2258\nName: id, Length: 15664, dtype: object"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['id']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T19:04:20.003389200Z",
     "start_time": "2023-12-09T19:04:19.976889700Z"
    }
   },
   "id": "bc82a6adc4d11c62"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "args = [ds.iloc[i] for i in range(2000, 3000)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T01:10:15.776163800Z",
     "start_time": "2023-12-10T01:10:15.515167800Z"
    }
   },
   "id": "4ae6d9525979121c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def date_for_json(d: datetime):\n",
    "    return d.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def fix_response(response):\n",
    "    if 'json```' in response:\n",
    "        response = response.split('json```')[1]\n",
    "    elif '```json' in response:\n",
    "        response = response.split('```json')[1]\n",
    "    if '```' in response:\n",
    "        response = response.split('```')[0]\n",
    "    return response\n",
    "\n",
    "def process_paper(paper):\n",
    "    atts=0\n",
    "    while atts<10:\n",
    "        atts += 1\n",
    "        try:\n",
    "            file_name = f'../data/kg_json_v2/{paper[\"id\"]}.json'\n",
    "            if os.path.exists(file_name):\n",
    "                logging.info(f'Skipping {paper[\"id\"]}')\n",
    "                break\n",
    "            print(\"Starting attempt \",atts, \" \", paper['id'], paper['title'])\n",
    "            llm_response, _ = query_one_abstract(paper['abstract'])\n",
    "            llm_response = fix_response(llm_response.content)\n",
    "            properties=dict(paper[['date', 'title', 'id']])\n",
    "            kg = json.loads(llm_response)\n",
    "            for rel in kg:\n",
    "                rel['relation'].update(properties)\n",
    "            ds.loc[ds['id'] == paper['id'], 'processed'] = True\n",
    "            with open(file_name, 'w') as f:\n",
    "                f.write(json.dumps(kg, default=date_for_json))\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(e)\n",
    "            logging.info(llm_response)\n",
    "            time.sleep(5)\n",
    "    print(\"Ending \", paper['id'], paper['title'])\n",
    "\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "pool = ThreadPool(5)\n",
    "with get_openai_callback() as usage:\n",
    "    pool.map(process_paper, args)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "print(usage)\n",
    " "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b44a206c5f8bdd"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "Tokens Used: 0\n\tPrompt Tokens: 0\n\tCompletion Tokens: 0\nSuccessful Requests: 0\nTotal Cost (USD): $0.0"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " usage"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T01:09:55.480253300Z",
     "start_time": "2023-12-10T01:09:55.461256800Z"
    }
   },
   "id": "4855e333fa1a132e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "97fb1a66fdb9476e"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "1999\n",
      "2999\n"
     ]
    }
   ],
   "source": [
    "ds['processed'] = False\n",
    "existing_nodes = {}\n",
    "existing_relations = []\n",
    "files = os.listdir('../data/kg_json_v2')\n",
    "for i, file in enumerate(files):\n",
    "    if (i+1)%1000 == 0:\n",
    "        print(i)\n",
    "    with open(os.path.join('..\\\\data\\\\kg_json_v2', file), 'r') as f:\n",
    "        data_str = f.read()\n",
    "    convert_to_kg(data_str, existing_nodes, existing_relations, {})\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T09:37:38.499272800Z",
     "start_time": "2023-12-10T09:37:22.091329900Z"
    }
   },
   "id": "25a3c6510ee6db87"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "29529"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(existing_relations)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T09:39:35.733800700Z",
     "start_time": "2023-12-10T09:39:35.689484Z"
    }
   },
   "id": "ab73c2e41b92eee3"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "nodes = list(existing_nodes.values())\n",
    "nodes = [[str(n.labels), n['name']] for n in nodes]\n",
    "nodes = pd.DataFrame(nodes, columns=['labels', 'name'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T12:33:18.078596900Z",
     "start_time": "2023-12-10T12:33:17.780595900Z"
    }
   },
   "id": "12b48e8e0c189935"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "def fix_name(x):\n",
    "    return x.replace('\\'','')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T12:33:21.872461400Z",
     "start_time": "2023-12-10T12:33:21.818460500Z"
    }
   },
   "id": "bb292fc155d5b6a0"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "nodes['name'] = nodes['name'].apply(fix_name)\n",
    "nodes['name'].str.contains('\\'').sum()\n",
    "nodes['labels'] = nodes['labels'].str.replace(':','').str.replace('`','')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T12:38:06.442236900Z",
     "start_time": "2023-12-10T12:38:06.329070Z"
    }
   },
   "id": "f3f6ac3eaeaf58ba"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "               labels                                         name\n0                task    clinical trial eligibility identification\n1      characteristic        natural language eligibility criteria\n2            solution                  text classification methods\n3             dataset            Phase III cancer trial exclusions\n4                data                  764 Phase III cancer trials\n...               ...                                          ...\n29234           input                     reference image segments\n29235      capability           referring and generic segmentation\n29236           model  universal visual in-context prompting model\n29237          metric                        PQ (Panoptic Quality)\n29238             url          https://github.com/ux-decoder/dinov\n\n[29239 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>task</td>\n      <td>clinical trial eligibility identification</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>characteristic</td>\n      <td>natural language eligibility criteria</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>solution</td>\n      <td>text classification methods</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>dataset</td>\n      <td>Phase III cancer trial exclusions</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>data</td>\n      <td>764 Phase III cancer trials</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>29234</th>\n      <td>input</td>\n      <td>reference image segments</td>\n    </tr>\n    <tr>\n      <th>29235</th>\n      <td>capability</td>\n      <td>referring and generic segmentation</td>\n    </tr>\n    <tr>\n      <th>29236</th>\n      <td>model</td>\n      <td>universal visual in-context prompting model</td>\n    </tr>\n    <tr>\n      <th>29237</th>\n      <td>metric</td>\n      <td>PQ (Panoptic Quality)</td>\n    </tr>\n    <tr>\n      <th>29238</th>\n      <td>url</td>\n      <td>https://github.com/ux-decoder/dinov</td>\n    </tr>\n  </tbody>\n</table>\n<p>29239 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T12:38:10.096814800Z",
     "start_time": "2023-12-10T12:38:10.049812700Z"
    }
   },
   "id": "dea298712092fc61"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "nodes.to_csv('..\\\\data\\\\nodes.csv', index=False, quotechar='\"')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T12:38:17.426045600Z",
     "start_time": "2023-12-10T12:38:17.331046500Z"
    }
   },
   "id": "aa41521517a9884d"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "Node('task', name='clinical trial eligibility identification')"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_relations[0].nodes[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T10:41:33.306295900Z",
     "start_time": "2023-12-10T10:41:33.257740100Z"
    }
   },
   "id": "fadb1ef6ae1947a0"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'from': 'clinical trial eligibility identification',\n  'to': 'natural language eligibility criteria',\n  'name': 'is complicated by',\n  'research': 'problem',\n  'summary': 'Automatic identification of clinical trials for which a patient is eligible is complicated by the fact that trial eligibility is stated in natural language',\n  'date': '2023-09-14',\n  'title': 'Text Classification of Cancer Clinical Trial Eligibility Criteria',\n  'id': '2309.07812'},\n {'from': 'text classification methods',\n  'to': 'clinical trial eligibility identification',\n  'name': 'employed for',\n  'research': 'approach',\n  'summary': 'Text classification methods are employed for common types of eligibility criteria in clinical trials',\n  'date': '2023-09-14',\n  'title': 'Text Classification of Cancer Clinical Trial Eligibility Criteria',\n  'id': '2309.07812'},\n {'from': 'Phase III cancer trial exclusions',\n  'to': '764 Phase III cancer trials',\n  'name': 'consists of',\n  'research': 'context',\n  'summary': 'The dataset consists of 764 Phase III cancer trials with annotated exclusion criteria',\n  'date': '2023-09-14',\n  'title': 'Text Classification of Cancer Clinical Trial Eligibility Criteria',\n  'id': '2309.07812'},\n {'from': 'Transformer',\n  'to': 'exclusion criteria classification',\n  'name': 'experimented with',\n  'research': 'method',\n  'summary': 'Common Transformer models are experimented with for classifying common exclusion criteria',\n  'date': '2023-09-14',\n  'title': 'Text Classification of Cancer Clinical Trial Eligibility Criteria',\n  'id': '2309.07812'},\n {'from': 'Clinical Trial BERT',\n  'to': 'Transformer',\n  'name': 'is a',\n  'research': 'context',\n  'summary': 'A new pre-trained Clinical Trial BERT model is used for classifying common exclusion criteria',\n  'date': '2023-09-14',\n  'title': 'Text Classification of Cancer Clinical Trial Eligibility Criteria',\n  'id': '2309.07812'},\n {'from': 'feasibility of automatic classification',\n  'to': 'exclusion criteria classification',\n  'name': 'demonstrated for',\n  'research': 'finding',\n  'summary': 'The results demonstrate the feasibility of automatically classifying common exclusion criteria in cancer trials',\n  'date': '2023-09-14',\n  'title': 'Text Classification of Cancer Clinical Trial Eligibility Criteria',\n  'id': '2309.07812'},\n {'from': 'Clinical Trial BERT',\n  'to': 'highest average performance',\n  'name': 'yields',\n  'research': 'finding',\n  'summary': 'Clinical Trial BERT yields the highest average performance across all criteria for classifying exclusion criteria',\n  'date': '2023-09-14',\n  'title': 'Text Classification of Cancer Clinical Trial Eligibility Criteria',\n  'id': '2309.07812'},\n {'from': 'common exclusion criteria in cancer trials',\n  'to': 'prior malignancy, HIV, hepatitis B, hepatitis C, psychiatric illness, drug/substance abuse, autoimmune illness',\n  'name': 'include',\n  'research': 'context',\n  'summary': 'Common exclusion criteria in cancer trials include prior malignancy, HIV, hepatitis B, hepatitis C, psychiatric illness, drug/substance abuse, and autoimmune illness',\n  'date': '2023-09-14',\n  'title': 'Text Classification of Cancer Clinical Trial Eligibility Criteria',\n  'id': '2309.07812'},\n {'from': 'large language model',\n  'to': 'data augmentation',\n  'name': 'used for',\n  'research': 'contribution',\n  'summary': 'LLMs are used to augment training data of small language models with automatically generated counterfactual instances',\n  'date': '2023-09-14',\n  'title': 'CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain\\n  Performance and Calibration',\n  'id': '2309.07822'},\n {'from': 'small language model',\n  'to': 'out-of-domain performance',\n  'name': 'improved by',\n  'research': 'finding',\n  'summary': 'The out-of-domain performance of SLMs in the extractive QA setup is improved by data augmentation with CF instances',\n  'date': '2023-09-14',\n  'title': 'CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain\\n  Performance and Calibration',\n  'id': '2309.07822'},\n {'from': 'counterfactual instance generation',\n  'to': 'out-of-domain performance',\n  'name': 'enhances',\n  'research': 'finding',\n  'summary': 'Data augmentation with CF instances enhances OOD performance and model calibration',\n  'date': '2023-09-14',\n  'title': 'CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain\\n  Performance and Calibration',\n  'id': '2309.07822'},\n {'from': 'confidence-based calibrator',\n  'to': 'data augmentation',\n  'name': 'improved by',\n  'research': 'finding',\n  'summary': 'Model calibration for confidence-based calibrators is improved through CF data augmentation',\n  'date': '2023-09-14',\n  'title': 'CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain\\n  Performance and Calibration',\n  'id': '2309.07822'},\n {'from': 'rationale-augmented calibrator',\n  'to': 'data augmentation',\n  'name': 'improved by',\n  'research': 'finding',\n  'summary': 'Model calibration for rationale-augmented calibrators is improved through CF data augmentation',\n  'date': '2023-09-14',\n  'title': 'CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain\\n  Performance and Calibration',\n  'id': '2309.07822'},\n {'from': 'counterfactual instance diversity',\n  'to': 'out-of-domain performance',\n  'name': 'correlates with',\n  'research': 'finding',\n  'summary': 'Higher diversity of CF instances correlates with performance improvements in SLMs',\n  'date': '2023-09-14',\n  'title': 'CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain\\n  Performance and Calibration',\n  'id': '2309.07822'},\n {'from': 'counterfactual augmented model',\n  'to': 'low entropy in importance assignment',\n  'name': 'exhibits',\n  'research': 'finding',\n  'summary': 'CF augmented models that are easier to calibrate exhibit much lower entropy when assigning importance',\n  'date': '2023-09-14',\n  'title': 'CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain\\n  Performance and Calibration',\n  'id': '2309.07822'},\n {'from': 'rationale-augmented calibrator',\n  'to': 'concise explanations',\n  'name': 'prefers',\n  'research': 'finding',\n  'summary': 'Rationale-augmented calibrators prefer concise explanations, indicated by lower entropy in importance assignment',\n  'date': '2023-09-14',\n  'title': 'CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain\\n  Performance and Calibration',\n  'id': '2309.07822'},\n {'from': 'extractive question answering',\n  'to': 'data augmentation',\n  'name': 'setup improved by',\n  'research': 'application',\n  'summary': 'The extractive QA setup is improved by augmenting SLMs with CF instances generated by LLMs',\n  'date': '2023-09-14',\n  'title': 'CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain\\n  Performance and Calibration',\n  'id': '2309.07822'},\n {'from': 'blockchain',\n  'to': 'modern technology landscape',\n  'name': 'has relevance in',\n  'research': 'context',\n  'summary': 'Blockchain technology has modern relevance',\n  'date': '2023-09-14',\n  'title': \"Two Timin': Repairing Smart Contracts With A Two-Layered Approach\",\n  'id': '2309.07841'},\n {'from': 'smart contract',\n  'to': 'risks and benefits',\n  'name': 'presents',\n  'research': 'context',\n  'summary': 'Smart contracts present both substantial risks and benefits',\n  'date': '2023-09-14',\n  'title': \"Two Timin': Repairing Smart Contracts With A Two-Layered Approach\",\n  'id': '2309.07841'},\n {'from': 'smart contract vulnerability',\n  'to': 'significant losses',\n  'name': 'can trigger',\n  'research': 'context',\n  'summary': 'Vulnerabilities within smart contracts can trigger a cascade of consequences, resulting in significant losses',\n  'date': '2023-09-14',\n  'title': \"Two Timin': Repairing Smart Contracts With A Two-Layered Approach\",\n  'id': '2309.07841'},\n {'from': 'current papers on smart contracts',\n  'to': 'classifying smart contracts',\n  'name': 'focus on',\n  'research': 'context',\n  'summary': 'Many current papers primarily focus on classifying smart contracts for malicious intent',\n  'date': '2023-09-14',\n  'title': \"Two Timin': Repairing Smart Contracts With A Two-Layered Approach\",\n  'id': '2309.07841'},\n {'from': 'two-layered framework for smart contracts',\n  'to': 'repairing malicious contracts',\n  'name': 'proposed for',\n  'research': 'contribution',\n  'summary': 'A novel, two-layered framework is proposed for classifying and directly repairing malicious contracts',\n  'date': '2023-09-14',\n  'title': \"Two Timin': Repairing Smart Contracts With A Two-Layered Approach\",\n  'id': '2309.07841'},\n {'from': 'Slither',\n  'to': 'vulnerability report',\n  'name': 'provides',\n  'research': 'method',\n  'summary': \"Slither's vulnerability report is combined with source code for classification and repair\",\n  'date': '2023-09-14',\n  'title': \"Two Timin': Repairing Smart Contracts With A Two-Layered Approach\",\n  'id': '2309.07841'},\n {'from': 'RandomForestClassifier',\n  'to': 'classifying vulnerabilities',\n  'name': 'used in',\n  'research': 'method',\n  'summary': 'A pre-trained RandomForestClassifier is used to classify smart contract vulnerabilities',\n  'date': '2023-09-14',\n  'title': \"Two Timin': Repairing Smart Contracts With A Two-Layered Approach\",\n  'id': '2309.07841'},\n {'from': 'large language model',\n  'to': 'repairing vulnerabilities',\n  'name': 'used in',\n  'research': 'method',\n  'summary': 'LLMs are used for classifying and repairing smart contract vulnerabilities',\n  'date': '2023-09-14',\n  'title': \"Two Timin': Repairing Smart Contracts With A Two-Layered Approach\",\n  'id': '2309.07841'},\n {'from': 'GPT-3.5-turbo',\n  'to': 'smart contract repair model',\n  'name': 'used to build',\n  'research': 'contribution',\n  'summary': 'A smart contract repair model built from pre-trained GPT-3.5-turbo reduced overall vulnerability count by 97.5%',\n  'date': '2023-09-14',\n  'title': \"Two Timin': Repairing Smart Contracts With A Two-Layered Approach\",\n  'id': '2309.07841'},\n {'from': 'LLaMA-2-7B',\n  'to': 'smart contract repair model',\n  'name': 'used to build',\n  'research': 'contribution',\n  'summary': 'A smart contract repair model built from fine-tuned LLaMA-2-7B reduced overall vulnerability count by 96.7%',\n  'date': '2023-09-14',\n  'title': \"Two Timin': Repairing Smart Contracts With A Two-Layered Approach\",\n  'id': '2309.07841'},\n {'from': 'smart contract repair model',\n  'to': 'functionality retention',\n  'name': 'retains',\n  'research': 'finding',\n  'summary': 'Manual inspection of repaired contracts shows all retain functionality',\n  'date': '2023-09-14',\n  'title': \"Two Timin': Repairing Smart Contracts With A Two-Layered Approach\",\n  'id': '2309.07841'},\n {'from': 'automatic batch classification and repair',\n  'to': 'repairing vulnerabilities',\n  'name': 'appropriate for',\n  'research': 'conclusion',\n  'summary': 'The proposed method is appropriate for automatic batch classification and repair of vulnerabilities in smart contracts',\n  'date': '2023-09-14',\n  'title': \"Two Timin': Repairing Smart Contracts With A Two-Layered Approach\",\n  'id': '2309.07841'},\n {'from': 'language model',\n  'to': 'sophisticated and diverse users',\n  'name': 'adapted by',\n  'research': 'context',\n  'summary': 'Language models are adapted by a sophisticated and diverse set of users',\n  'date': '2023-09-14',\n  'title': 'ExpertQA: Expert-Curated Questions and Attributed Answers',\n  'id': '2309.07852'},\n {'from': 'language model',\n  'to': 'factuality',\n  'name': 'must provide',\n  'research': 'context',\n  'summary': 'It is critical for language models to provide factually correct information supported by verifiable sources',\n  'date': '2023-09-14',\n  'title': 'ExpertQA: Expert-Curated Questions and Attributed Answers',\n  'id': '2309.07852'},\n {'from': 'factuality',\n  'to': 'various fields of study and professions',\n  'name': 'important for',\n  'research': 'context',\n  'summary': 'Guaranteeing factuality is critical across fields of study & professions',\n  'date': '2023-09-14',\n  'title': 'ExpertQA: Expert-Curated Questions and Attributed Answers',\n  'id': '2309.07852'},\n {'from': 'high-stakes fields',\n  'to': 'propagating false information',\n  'name': 'risk of',\n  'research': 'context',\n  'summary': 'High-stakes fields like medicine and law have a high risk of propagating false information with undesirable societal consequences',\n  'date': '2023-09-14',\n  'title': 'ExpertQA: Expert-Curated Questions and Attributed Answers',\n  'id': '2309.07852'},\n {'from': 'previous work on factuality and attribution',\n  'to': 'domain-specific scenarios',\n  'name': 'does not focus on',\n  'research': 'gap',\n  'summary': 'Previous work has not focused on analyzing factuality and attribution in domain-specific scenarios',\n  'date': '2023-09-14',\n  'title': 'ExpertQA: Expert-Curated Questions and Attributed Answers',\n  'id': '2309.07852'},\n {'from': 'evaluation study on factuality and attribution',\n  'to': 'factuality and attribution in responses',\n  'name': 'analyzes',\n  'research': 'contribution',\n  'summary': 'The evaluation study analyzes various axes of factuality and attribution in language model responses with domain experts involved',\n  'date': '2023-09-14',\n  'title': 'ExpertQA: Expert-Curated Questions and Attributed Answers',\n  'id': '2309.07852'},\n {'from': 'ExpertQA',\n  'to': 'expert curation and revision',\n  'name': 'created by',\n  'research': 'method',\n  'summary': 'ExpertQA, a high-quality long-form QA dataset, is created with expert-curated questions and verified answers with attributions',\n  'date': '2023-09-14',\n  'title': 'ExpertQA: Expert-Curated Questions and Attributed Answers',\n  'id': '2309.07852'},\n {'from': 'ExpertQA',\n  'to': '32 fields of study',\n  'name': 'spans',\n  'research': 'context',\n  'summary': 'ExpertQA spans 32 fields with 2177 questions and verified answers',\n  'date': '2023-09-14',\n  'title': 'ExpertQA: Expert-Curated Questions and Attributed Answers',\n  'id': '2309.07852'},\n {'from': 'domain experts',\n  'to': 'evaluation and revision of language model responses',\n  'name': 'evaluate',\n  'research': 'method',\n  'summary': 'Domain experts evaluate generated responses to their own questions and revise answers produced by language models',\n  'date': '2023-09-14',\n  'title': 'ExpertQA: Expert-Curated Questions and Attributed Answers',\n  'id': '2309.07852'},\n {'from': 'artificial intelligence',\n  'to': 'human-level AI',\n  'name': 'pursued for',\n  'research': 'context',\n  'summary': 'Humanity has pursued AI equivalent to or surpassing human level',\n  'date': '2023-09-14',\n  'title': 'The Rise and Potential of Large Language Model Based Agents: A Survey',\n  'id': '2309.07864'},\n {'from': 'AI agent',\n  'to': 'artificial entity',\n  'name': 'is',\n  'research': 'context',\n  'summary': 'AI agents are artificial entities that sense their environment, make decisions, and take actions',\n  'date': '2023-09-14',\n  'title': 'The Rise and Potential of Large Language Model Based Agents: A Survey',\n  'id': '2309.07864'},\n {'from': 'AI community',\n  'to': 'developing intelligent agents',\n  'name': 'focuses on',\n  'research': 'context',\n  'summary': 'The AI community mainly focuses on advancement in algorithms or training strategies for specific capabilities or tasks',\n  'date': '2023-09-14',\n  'title': 'The Rise and Potential of Large Language Model Based Agents: A Survey',\n  'id': '2309.07864'},\n {'from': 'large language model',\n  'to': 'Artificial General Intelligence (AGI)',\n  'name': 'considered as',\n  'research': 'context',\n  'summary': 'LLMs are regarded as potential sparks for Artificial General Intelligence (AGI)',\n  'date': '2023-09-14',\n  'title': 'The Rise and Potential of Large Language Model Based Agents: A Survey',\n  'id': '2309.07864'},\n {'from': 'large language model',\n  'to': 'AI agent',\n  'name': 'foundation for',\n  'research': 'context',\n  'summary': 'LLMs are used as the foundation to build AI agents',\n  'date': '2023-09-14',\n  'title': 'The Rise and Potential of Large Language Model Based Agents: A Survey',\n  'id': '2309.07864'},\n {'from': 'LLM-based agents',\n  'to': 'large language model',\n  'name': 'performed on',\n  'research': 'contribution',\n  'summary': 'A comprehensive survey on LLM-based agents is performed',\n  'date': '2023-09-14',\n  'title': 'The Rise and Potential of Large Language Model Based Agents: A Survey',\n  'id': '2309.07864'},\n {'from': 'general framework for LLM-based agents',\n  'to': 'large language model',\n  'name': 'comprises',\n  'research': 'contribution',\n  'summary': 'The framework comprises three main components: brain, perception, and action',\n  'date': '2023-09-14',\n  'title': 'The Rise and Potential of Large Language Model Based Agents: A Survey',\n  'id': '2309.07864'},\n {'from': 'LLM-based agent applications',\n  'to': 'large language model',\n  'name': 'explored in',\n  'research': 'contribution',\n  'summary': 'Applications of LLM-based agents are explored in single-agent scenarios, multi-agent scenarios, and human-agent cooperation',\n  'date': '2023-09-14',\n  'title': 'The Rise and Potential of Large Language Model Based Agents: A Survey',\n  'id': '2309.07864'},\n {'from': 'agent society',\n  'to': 'large language model',\n  'name': 'explored for',\n  'research': 'contribution',\n  'summary': 'Agent societies are explored, including the behavior and personality of LLM-based agents and the social phenomena that emerge',\n  'date': '2023-09-14',\n  'title': 'The Rise and Potential of Large Language Model Based Agents: A Survey',\n  'id': '2309.07864'},\n {'from': 'LLM-agent-paper-list',\n  'to': 'https://github.com/woooodyy/llm-agent-paper-list',\n  'name': 'provides',\n  'research': 'resource',\n  'summary': 'A repository for related papers on LLM-based agents is provided',\n  'date': '2023-09-14',\n  'title': 'The Rise and Potential of Large Language Model Based Agents: A Survey',\n  'id': '2309.07864'},\n {'from': 'trade-off in generalizable prompt learning',\n  'to': 'vision-language model',\n  'name': 'targets',\n  'research': 'context',\n  'summary': 'The paper targets a novel trade-off problem in generalizable prompt learning for VLMs, focusing on improving performance on unseen classes while maintaining performance on seen classes',\n  'date': '2023-09-14',\n  'title': 'Gradient constrained sharpness-aware prompt learning for vision-language\\n  models',\n  'id': '2309.07866'},\n {'from': 'existing generalizable methods',\n  'to': 'seen classes degradation',\n  'name': 'neglect',\n  'research': 'limitation',\n  'summary': 'Existing generalizable methods neglect the seen classes degradation',\n  'date': '2023-09-14',\n  'title': 'Gradient constrained sharpness-aware prompt learning for vision-language\\n  models',\n  'id': '2309.07866'},\n {'from': 'optimization',\n  'to': 'trade-off in generalizable prompt learning',\n  'name': 'used to solve',\n  'research': 'approach',\n  'summary': 'The paper leverages the relationship between loss landscape geometry and model generalization ability from an optimization perspective',\n  'date': '2023-09-14',\n  'title': 'Gradient constrained sharpness-aware prompt learning for vision-language\\n  models',\n  'id': '2309.07866'},\n {'from': 'sharpness-aware minimization (SAM)',\n  'to': 'loss landscape geometry',\n  'name': 'analyzed for',\n  'research': 'analysis',\n  'summary': 'The loss landscapes of the state-of-the-art method and vanilla SAM-based method are analyzed',\n  'date': '2023-09-14',\n  'title': 'Gradient constrained sharpness-aware prompt learning for vision-language\\n  models',\n  'id': '2309.07866'},\n {'from': 'loss sharpness',\n  'to': 'trade-off performance',\n  'name': 'correlates to',\n  'research': 'finding',\n  'summary': 'The trade-off performance correlates to both loss value and loss sharpness',\n  'date': '2023-09-14',\n  'title': 'Gradient constrained sharpness-aware prompt learning for vision-language\\n  models',\n  'id': '2309.07866'},\n {'from': 'optimizing gradient relevance',\n  'to': 'trade-off performance',\n  'name': 'affects',\n  'research': 'limitation',\n  'summary': 'The optimizing gradient of existing methods cannot maintain high relevance to both loss value and loss sharpness, which severely affects their trade-off performance',\n  'date': '2023-09-14',\n  'title': 'Gradient constrained sharpness-aware prompt learning for vision-language\\n  models',\n  'id': '2309.07866'},\n {'from': 'Gradient Constrained Sharpness-Aware Context Optimization (GCSCOOP)',\n  'to': 'trade-off in generalizable prompt learning',\n  'name': 'proposed for',\n  'research': 'contribution',\n  'summary': 'GCSCOOP is proposed to dynamically constrain the optimizing gradient, achieving the two-fold optimization objective simultaneously',\n  'date': '2023-09-14',\n  'title': 'Gradient constrained sharpness-aware prompt learning for vision-language\\n  models',\n  'id': '2309.07866'},\n {'from': 'Gradient Constrained Sharpness-Aware Context Optimization (GCSCOOP)',\n  'to': 'extensive experiments',\n  'name': 'verified by',\n  'research': 'validation',\n  'summary': 'Extensive experiments verify the effectiveness of GCSCOOP in the trade-off problem',\n  'date': '2023-09-14',\n  'title': 'Gradient constrained sharpness-aware prompt learning for vision-language\\n  models',\n  'id': '2309.07866'},\n {'from': 'large language model',\n  'to': 'autonomous language agent',\n  'name': 'enable',\n  'research': 'context',\n  'summary': 'Recent advances on LLMs enable the building of autonomous language agents',\n  'date': '2023-09-14',\n  'title': 'Agents: An Open-source Framework for Autonomous Language Agents',\n  'id': '2309.07870'},\n {'from': 'autonomous language agent',\n  'to': 'artificial general intelligence',\n  'name': 'considered as',\n  'research': 'context',\n  'summary': 'Language agents are considered a promising direction towards artificial general intelligence',\n  'date': '2023-09-14',\n  'title': 'Agents: An Open-source Framework for Autonomous Language Agents',\n  'id': '2309.07870'},\n {'from': 'Agents',\n  'to': 'non-specialist audience',\n  'name': 'released for',\n  'research': 'contribution',\n  'summary': 'Agents, an open-source library, is released to open up advances in autonomous language agents to a wider non-specialist audience',\n  'date': '2023-09-14',\n  'title': 'Agents: An Open-source Framework for Autonomous Language Agents',\n  'id': '2309.07870'},\n {'from': 'Agents',\n  'to': 'advanced agent features',\n  'name': 'supports',\n  'research': 'feature',\n  'summary': 'Agents supports features like planning, memory, tool usage, multi-agent communication, and fine-grained symbolic control',\n  'date': '2023-09-14',\n  'title': 'Agents: An Open-source Framework for Autonomous Language Agents',\n  'id': '2309.07870'},\n {'from': 'Agents',\n  'to': 'building autonomous language agents',\n  'name': 'enables',\n  'research': 'feature',\n  'summary': 'Agents enables non-specialists to build, customize, test, tune, and deploy state-of-the-art autonomous language agents with minimal coding',\n  'date': '2023-09-14',\n  'title': 'Agents: An Open-source Framework for Autonomous Language Agents',\n  'id': '2309.07870'},\n {'from': 'Agents',\n  'to': 'research-friendly',\n  'name': 'is',\n  'research': 'feature',\n  'summary': 'Agents is research-friendly with a modularized design for easy extensibility',\n  'date': '2023-09-14',\n  'title': 'Agents: An Open-source Framework for Autonomous Language Agents',\n  'id': '2309.07870'},\n {'from': 'Agents',\n  'to': 'GitHub',\n  'name': 'available at',\n  'research': 'resource',\n  'summary': 'Agents library is available on GitHub',\n  'date': '2023-09-14',\n  'title': 'Agents: An Open-source Framework for Autonomous Language Agents',\n  'id': '2309.07870'},\n {'from': 'large language model',\n  'to': 'helpfulness',\n  'name': 'improved by',\n  'research': 'context',\n  'summary': 'Training large language models to follow instructions generally makes them more helpful',\n  'date': '2023-09-14',\n  'title': 'Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language\\n  Models that Follow Instructions',\n  'id': '2309.07875'},\n {'from': 'helpfulness',\n  'to': 'harmful content generation',\n  'name': 'leads to',\n  'research': 'problem',\n  'summary': 'A perfectly helpful model may follow malicious instructions and generate harmful content',\n  'date': '2023-09-14',\n  'title': 'Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language\\n  Models that Follow Instructions',\n  'id': '2309.07875'},\n {'from': 'large language model',\n  'to': 'safety',\n  'name': 'critiqued for',\n  'research': 'claim',\n  'summary': 'Concerns are raised over the safety of models that only emphasize helpfulness, not safety, in their instruction-tuning',\n  'date': '2023-09-14',\n  'title': 'Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language\\n  Models that Follow Instructions',\n  'id': '2309.07875'},\n {'from': 'instruction-tuned large language model',\n  'to': 'unsafe',\n  'name': 'found to be',\n  'research': 'finding',\n  'summary': 'Several popular instruction-tuned models are found to be highly unsafe',\n  'date': '2023-09-14',\n  'title': 'Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language\\n  Models that Follow Instructions',\n  'id': '2309.07875'},\n {'from': 'safety-tuning',\n  'to': 'safety',\n  'name': 'improves',\n  'research': 'contribution',\n  'summary': 'Adding a small percentage of safety examples in training can substantially improve model safety',\n  'date': '2023-09-14',\n  'title': 'Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language\\n  Models that Follow Instructions',\n  'id': '2309.07875'},\n {'from': 'LLaMA',\n  'to': 'safety-tuning',\n  'name': 'benefits from',\n  'research': 'contribution',\n  'summary': 'Fine-tuning LLaMA with safety examples improves its safety without significantly reducing helpfulness',\n  'date': '2023-09-14',\n  'title': 'Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language\\n  Models that Follow Instructions',\n  'id': '2309.07875'},\n {'from': 'safety',\n  'to': 'exaggerated safety',\n  'name': 'can lead to',\n  'research': 'finding',\n  'summary': 'Excessive safety-tuning can result in models refusing to respond to reasonable prompts',\n  'date': '2023-09-14',\n  'title': 'Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language\\n  Models that Follow Instructions',\n  'id': '2309.07875'},\n {'from': 'trade-offs in training LLMS',\n  'to': 'instruction following vs safety',\n  'name': 'sheds light on',\n  'research': 'contribution',\n  'summary': 'The study sheds light on trade-offs in training LLMs to follow instructions and exhibit safe behavior',\n  'date': '2023-09-14',\n  'title': 'Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language\\n  Models that Follow Instructions',\n  'id': '2309.07875'},\n {'from': 'vision-language model',\n  'to': 'large language model',\n  'name': 'enhanced by',\n  'research': 'context',\n  'summary': 'Vision-language models (VLMs) are enhanced by large language models (LLMs)',\n  'date': '2023-09-14',\n  'title': 'MMICL: Empowering Vision-language Model with Multi-Modal In-Context\\n  Learning',\n  'id': '2309.07915'},\n {'from': 'large language model',\n  'to': 'in-context learning',\n  'name': 'utilize',\n  'research': 'context',\n  'summary': 'LLMs can utilize extensive background knowledge and task information with in-context learning',\n  'date': '2023-09-14',\n  'title': 'MMICL: Empowering Vision-language Model with Multi-Modal In-Context\\n  Learning',\n  'id': '2309.07915'},\n {'from': 'vision-language model',\n  'to': 'complex multi-modal prompt understanding',\n  'name': 'struggle with',\n  'research': 'problem',\n  'summary': 'Most VLMs struggle with understanding complex multi-modal prompts with multiple images',\n  'date': '2023-09-14',\n  'title': 'MMICL: Empowering Vision-language Model with Multi-Modal In-Context\\n  Learning',\n  'id': '2309.07915'},\n {'from': 'MMICL',\n  'to': 'vision-language model',\n  'name': 'introduced to',\n  'research': 'contribution',\n  'summary': 'MMICL is introduced to allow the VLM to deal with multi-modal inputs efficiently',\n  'date': '2023-09-14',\n  'title': 'MMICL: Empowering Vision-language Model with Multi-Modal In-Context\\n  Learning',\n  'id': '2309.07915'},\n {'from': 'MMICL',\n  'to': 'in-context learning',\n  'name': 'proposes',\n  'research': 'contribution',\n  'summary': 'MMICL proposes a novel context scheme to augment the in-context learning ability of the VLM',\n  'date': '2023-09-14',\n  'title': 'MMICL: Empowering Vision-language Model with Multi-Modal In-Context\\n  Learning',\n  'id': '2309.07915'},\n {'from': 'Multi-Modal In-Context Learning (MIC) dataset',\n  'to': 'complex multi-modal prompt understanding',\n  'name': 'constructed to',\n  'research': 'contribution',\n  'summary': \"The MIC dataset is designed to enhance the VLM's ability to understand complex multi-modal prompts\",\n  'date': '2023-09-14',\n  'title': 'MMICL: Empowering Vision-language Model with Multi-Modal In-Context\\n  Learning',\n  'id': '2309.07915'},\n {'from': 'MMICL',\n  'to': 'state-of-the-art zero-shot performance',\n  'name': 'achieves',\n  'research': 'finding',\n  'summary': 'MMICL achieves new state-of-the-art zero-shot performance on a wide range of general vision-language tasks',\n  'date': '2023-09-14',\n  'title': 'MMICL: Empowering Vision-language Model with Multi-Modal In-Context\\n  Learning',\n  'id': '2309.07915'},\n {'from': 'MMICL',\n  'to': 'complex multi-modal prompt understanding',\n  'name': 'effective for',\n  'research': 'finding',\n  'summary': 'MMICL effectively tackles the challenge of complex multi-modal prompt understanding',\n  'date': '2023-09-14',\n  'title': 'MMICL: Empowering Vision-language Model with Multi-Modal In-Context\\n  Learning',\n  'id': '2309.07915'},\n {'from': 'language bias in VLMs',\n  'to': 'MMICL',\n  'name': 'alleviated by',\n  'research': 'finding',\n  'summary': 'MMICL successfully alleviates language bias in VLMs, which often leads to hallucination when faced with extensive textual context',\n  'date': '2023-09-14',\n  'title': 'MMICL: Empowering Vision-language Model with Multi-Modal In-Context\\n  Learning',\n  'id': '2309.07915'},\n {'from': 'human-scene interaction',\n  'to': 'embodied AI',\n  'name': 'is a component of',\n  'research': 'context',\n  'summary': 'Human-scene interaction is a vital component of fields like embodied AI and virtual reality',\n  'date': '2023-09-14',\n  'title': 'Unified Human-Scene Interaction via Prompted Chain-of-Contacts',\n  'id': '2309.07918'},\n {'from': 'human-scene interaction',\n  'to': 'virtual reality',\n  'name': 'is a component of',\n  'research': 'context',\n  'summary': 'Human-scene interaction is a vital component of fields like embodied AI and virtual reality',\n  'date': '2023-09-14',\n  'title': 'Unified Human-Scene Interaction via Prompted Chain-of-Contacts',\n  'id': '2309.07918'},\n {'from': 'versatile interaction control',\n  'to': 'practical application of HSI',\n  'name': 'requires',\n  'research': 'challenge',\n  'summary': 'Versatile interaction control requires further exploration for practical application of HSI',\n  'date': '2023-09-14',\n  'title': 'Unified Human-Scene Interaction via Prompted Chain-of-Contacts',\n  'id': '2309.07918'},\n {'from': 'user-friendly interface development',\n  'to': 'practical application of HSI',\n  'name': 'requires',\n  'research': 'challenge',\n  'summary': 'The development of a user-friendly interface requires further exploration for practical application of HSI',\n  'date': '2023-09-14',\n  'title': 'Unified Human-Scene Interaction via Prompted Chain-of-Contacts',\n  'id': '2309.07918'},\n {'from': 'UniHSI',\n  'to': 'unified control of diverse interactions',\n  'name': 'supports',\n  'research': 'contribution',\n  'summary': 'UniHSI supports unified control of diverse interactions through language commands',\n  'date': '2023-09-14',\n  'title': 'Unified Human-Scene Interaction via Prompted Chain-of-Contacts',\n  'id': '2309.07918'},\n {'from': 'interaction as Chain of Contacts (CoC)',\n  'to': 'interaction types and human-object contact regions',\n  'name': 'inspired by',\n  'research': 'concept',\n  'summary': 'The definition of interaction as CoC is inspired by the correlation between interaction types and human-object contact regions',\n  'date': '2023-09-14',\n  'title': 'Unified Human-Scene Interaction via Prompted Chain-of-Contacts',\n  'id': '2309.07918'},\n {'from': 'LLM planner',\n  'to': 'task plans in the form of CoC',\n  'name': 'translates',\n  'research': 'function',\n  'summary': 'The LLM planner in UniHSI translates language prompts into task plans in the form of CoC',\n  'date': '2023-09-14',\n  'title': 'Unified Human-Scene Interaction via Prompted Chain-of-Contacts',\n  'id': '2309.07918'},\n {'from': 'unified controller',\n  'to': 'uniform task execution',\n  'name': 'turns into',\n  'research': 'function',\n  'summary': 'The unified controller in UniHSI turns CoC into uniform task execution',\n  'date': '2023-09-14',\n  'title': 'Unified Human-Scene Interaction via Prompted Chain-of-Contacts',\n  'id': '2309.07918'},\n {'from': 'ScenePlan',\n  'to': 'training and evaluation of UniHSI',\n  'name': 'collected for',\n  'research': 'method',\n  'summary': 'The ScenePlan dataset is collected to facilitate training and evaluation, encompassing thousands of task plans generated by LLMs based on diverse scenarios',\n  'date': '2023-09-14',\n  'title': 'Unified Human-Scene Interaction via Prompted Chain-of-Contacts',\n  'id': '2309.07918'},\n {'from': 'UniHSI',\n  'to': 'effectiveness in versatile task execution',\n  'name': 'demonstrates',\n  'research': 'finding',\n  'summary': 'Comprehensive experiments demonstrate the effectiveness of UniHSI in versatile task execution and generalizability to real scanned scenes',\n  'date': '2023-09-14',\n  'title': 'Unified Human-Scene Interaction via Prompted Chain-of-Contacts',\n  'id': '2309.07918'},\n {'from': 'UniHSI',\n  'to': 'generalizability to real scanned scenes',\n  'name': 'demonstrates',\n  'research': 'finding',\n  'summary': 'Comprehensive experiments demonstrate the effectiveness of UniHSI in versatile task execution and generalizability to real scanned scenes',\n  'date': '2023-09-14',\n  'title': 'Unified Human-Scene Interaction via Prompted Chain-of-Contacts',\n  'id': '2309.07918'},\n {'from': 'UniHSI',\n  'to': 'https://github.com/openrobotlab/unihsi',\n  'name': 'has page at',\n  'research': 'resource',\n  'summary': 'The project page for UniHSI is available on GitHub',\n  'date': '2023-09-14',\n  'title': 'Unified Human-Scene Interaction via Prompted Chain-of-Contacts',\n  'id': '2309.07918'},\n {'from': 'task-oriented grasping',\n  'to': 'grasping by specific part',\n  'name': 'requires',\n  'research': 'context',\n  'summary': 'Grasping objects by a specific part is often crucial for safety and executing downstream tasks',\n  'date': '2023-09-14',\n  'title': 'Language Embedded Radiance Fields for Zero-Shot Task-Oriented Grasping',\n  'id': '2309.07970'},\n {'from': 'learning-based grasp planner',\n  'to': 'grasping by specific part',\n  'name': 'lacks',\n  'research': 'problem',\n  'summary': 'Learning-based grasp planners lack the behavior of grasping by specific parts unless trained on specific object part data',\n  'date': '2023-09-14',\n  'title': 'Language Embedded Radiance Fields for Zero-Shot Task-Oriented Grasping',\n  'id': '2309.07970'},\n {'from': 'LERF-ToGo',\n  'to': 'task-oriented grasping',\n  'name': 'proposed for',\n  'research': 'contribution',\n  'summary': 'LERF-ToGo uses vision-language models zero-shot to output a grasp distribution over an object given a natural language query',\n  'date': '2023-09-14',\n  'title': 'Language Embedded Radiance Fields for Zero-Shot Task-Oriented Grasping',\n  'id': '2309.07970'},\n {'from': 'Language Embedded Radiance Fields (LERF)',\n  'to': '3D language field',\n  'name': 'reconstructs',\n  'research': 'method',\n  'summary': 'LERF reconstructs a scene and distills CLIP embeddings into a multi-scale 3D language field queryable with text',\n  'date': '2023-09-14',\n  'title': 'Language Embedded Radiance Fields for Zero-Shot Task-Oriented Grasping',\n  'id': '2309.07970'},\n {'from': 'Language Embedded Radiance Fields (LERF)',\n  'to': 'objectness',\n  'name': 'lacks',\n  'research': 'problem',\n  'summary': 'LERF has no sense of objectness, often returning incomplete activations over an object',\n  'date': '2023-09-14',\n  'title': 'Language Embedded Radiance Fields for Zero-Shot Task-Oriented Grasping',\n  'id': '2309.07970'},\n {'from': 'LERF-ToGo',\n  'to': 'objectness',\n  'name': 'mitigates',\n  'research': 'solution',\n  'summary': 'LERF-ToGo mitigates the lack of spatial grouping by extracting a 3D object mask via DINO features',\n  'date': '2023-09-14',\n  'title': 'Language Embedded Radiance Fields for Zero-Shot Task-Oriented Grasping',\n  'id': '2309.07970'},\n {'from': 'LERF-ToGo',\n  'to': 'grasp ranking',\n  'name': 'enables',\n  'research': 'contribution',\n  'summary': 'LERF-ToGo conditionally queries LERF on a 3D object mask to obtain a semantic distribution over the object for ranking grasps',\n  'date': '2023-09-14',\n  'title': 'Language Embedded Radiance Fields for Zero-Shot Task-Oriented Grasping',\n  'id': '2309.07970'},\n {'from': 'LERF-ToGo evaluation',\n  'to': 'grasp success rate',\n  'name': 'shows',\n  'research': 'finding',\n  'summary': 'LERF-ToGo selects grasps on the correct part in 81% of trials and grasps successfully in 69% on 31 different physical objects',\n  'date': '2023-09-14',\n  'title': 'Language Embedded Radiance Fields for Zero-Shot Task-Oriented Grasping',\n  'id': '2309.07970'},\n {'from': 'project website',\n  'to': 'LERF-ToGo',\n  'name': 'provides information on',\n  'research': 'resource',\n  'summary': 'The project website provides information on LERF-ToGo',\n  'date': '2023-09-14',\n  'title': 'Language Embedded Radiance Fields for Zero-Shot Task-Oriented Grasping',\n  'id': '2309.07970'},\n {'from': 'machine learning',\n  'to': 'reasoning tasks',\n  'name': 'used for',\n  'research': 'context',\n  'summary': 'Recent progress in machine learning has been driven by novel model architectures, large-scale pre-training, and reasoning datasets',\n  'date': '2023-09-14',\n  'title': 'A Data Source for Reasoning Embodied Agents',\n  'id': '2309.07974'},\n {'from': 'novel model architecture',\n  'to': 'machine learning',\n  'name': 'contributes to',\n  'research': 'context',\n  'summary': 'Novel model architectures contribute to advances in machine learning for reasoning tasks',\n  'date': '2023-09-14',\n  'title': 'A Data Source for Reasoning Embodied Agents',\n  'id': '2309.07974'},\n {'from': 'large-scale pre-training',\n  'to': 'machine learning',\n  'name': 'contributes to',\n  'research': 'context',\n  'summary': 'Large-scale pre-training protocols contribute to advances in machine learning for reasoning tasks',\n  'date': '2023-09-14',\n  'title': 'A Data Source for Reasoning Embodied Agents',\n  'id': '2309.07974'},\n {'from': 'dedicated reasoning datasets',\n  'to': 'reasoning tasks',\n  'name': 'used for',\n  'research': 'context',\n  'summary': 'Dedicated reasoning datasets are used for fine-tuning models in machine learning for reasoning tasks',\n  'date': '2023-09-14',\n  'title': 'A Data Source for Reasoning Embodied Agents',\n  'id': '2309.07974'},\n {'from': 'data generator for machine reasoning',\n  'to': 'machine reasoning',\n  'name': 'introduced',\n  'research': 'contribution',\n  'summary': 'A new data generator for machine reasoning that integrates with an embodied agent is introduced',\n  'date': '2023-09-14',\n  'title': 'A Data Source for Reasoning Embodied Agents',\n  'id': '2309.07974'},\n {'from': 'templated text queries and answers',\n  'to': 'data generator for machine reasoning',\n  'name': 'generated by',\n  'research': 'method',\n  'summary': 'The data generator produces templated text queries and answers',\n  'date': '2023-09-14',\n  'title': 'A Data Source for Reasoning Embodied Agents',\n  'id': '2309.07974'},\n {'from': 'world-states',\n  'to': 'database',\n  'name': 'encoded into',\n  'research': 'method',\n  'summary': 'World-states are encoded into a database and are a result of world dynamics and the actions of the agent',\n  'date': '2023-09-14',\n  'title': 'A Data Source for Reasoning Embodied Agents',\n  'id': '2309.07974'},\n {'from': 'pre-trained language model',\n  'to': 'text-formatted database representation',\n  'name': 'fine-tuned on',\n  'research': 'method',\n  'summary': 'Pre-trained language models are fine-tuned on a text-formatted representation of the database',\n  'date': '2023-09-14',\n  'title': 'A Data Source for Reasoning Embodied Agents',\n  'id': '2309.07974'},\n {'from': 'graph-structured Transformer',\n  'to': 'knowledge-graph database representation',\n  'name': 'operates on',\n  'research': 'method',\n  'summary': 'Graph-structured Transformers operate on a knowledge-graph representation of the database',\n  'date': '2023-09-14',\n  'title': 'A Data Source for Reasoning Embodied Agents',\n  'id': '2309.07974'},\n {'from': 'neural reasoning model',\n  'to': 'reasoning tasks',\n  'name': 'research direction for',\n  'research': 'reflection',\n  'summary': 'The results hint at new research directions in designing neural reasoning models and database representations',\n  'date': '2023-09-14',\n  'title': 'A Data Source for Reasoning Embodied Agents',\n  'id': '2309.07974'},\n {'from': 'code',\n  'to': 'GitHub',\n  'name': 'will be released at',\n  'research': 'additional information',\n  'summary': 'Code to generate the data will be released on GitHub',\n  'date': '2023-09-14',\n  'title': 'A Data Source for Reasoning Embodied Agents',\n  'id': '2309.07974'},\n {'from': 'salient entity',\n  'to': 'document aboutness',\n  'name': 'provides cues on',\n  'research': 'context',\n  'summary': 'Salient entities provide useful cues of the aboutness of a document to a reader',\n  'date': '2023-09-14',\n  'title': 'Leveraging Contextual Information for Effective Entity Salience\\n  Detection',\n  'id': '2309.07990'},\n {'from': 'salient entity detection',\n  'to': 'downstream applications',\n  'name': 'helpful for',\n  'research': 'context',\n  'summary': 'Identifying the salience of entities is helpful in downstream applications such as search, ranking, and entity-centric summarization',\n  'date': '2023-09-14',\n  'title': 'Leveraging Contextual Information for Effective Entity Salience\\n  Detection',\n  'id': '2309.07990'},\n {'from': 'feature engineering',\n  'to': 'salient entity detection',\n  'name': 'used in',\n  'research': 'context',\n  'summary': 'Prior work on salient entity detection mainly focused on machine learning models that require heavy feature engineering',\n  'date': '2023-09-14',\n  'title': 'Leveraging Contextual Information for Effective Entity Salience\\n  Detection',\n  'id': '2309.07990'},\n {'from': 'medium-sized language model',\n  'to': 'cross-encoder',\n  'name': 'fine-tuned with',\n  'research': 'contribution',\n  'summary': 'Fine-tuning medium-sized language models with a cross-encoder style architecture yields substantial performance gains over feature engineering approaches',\n  'date': '2023-09-14',\n  'title': 'Leveraging Contextual Information for Effective Entity Salience\\n  Detection',\n  'id': '2309.07990'},\n {'from': 'comprehensive benchmarking',\n  'to': 'publicly available datasets',\n  'name': 'conducted on',\n  'research': 'method',\n  'summary': 'A comprehensive benchmarking of four publicly available datasets is conducted using models representative of the medium-sized pre-trained language model family',\n  'date': '2023-09-14',\n  'title': 'Leveraging Contextual Information for Effective Entity Salience\\n  Detection',\n  'id': '2309.07990'},\n {'from': 'zero-shot prompting',\n  'to': 'salient entity detection',\n  'name': 'yields results for',\n  'research': 'finding',\n  'summary': 'Zero-shot prompting of instruction-tuned language models yields inferior results for salient entity detection',\n  'date': '2023-09-14',\n  'title': 'Leveraging Contextual Information for Effective Entity Salience\\n  Detection',\n  'id': '2309.07990'},\n {'from': 'instruction-tuned language model',\n  'to': 'salient entity detection',\n  'name': 'used in',\n  'research': 'finding',\n  'summary': \"Zero-shot prompting of instruction-tuned language models yields inferior results, indicating the task's uniqueness and complexity\",\n  'date': '2023-09-14',\n  'title': 'Leveraging Contextual Information for Effective Entity Salience\\n  Detection',\n  'id': '2309.07990'},\n {'from': 'large language model',\n  'to': 'Natural Language Processing (NLP)',\n  'name': 'demonstrate capabilities in',\n  'research': 'context',\n  'summary': 'LLMs have shown remarkable capabilities in NLP, especially in domains with scarce labeled data like the clinical domain',\n  'date': '2023-09-14',\n  'title': 'An Empirical Evaluation of Prompting Strategies for Large Language\\n  Models in Zero-Shot Clinical Natural Language Processing',\n  'id': '2309.08008'},\n {'from': 'clinical NLP tasks',\n  'to': 'in-context learning',\n  'name': 'require',\n  'research': 'challenge',\n  'summary': 'Effective prompts are needed to guide LLMs to perform specific clinical NLP tasks without task-specific training data',\n  'date': '2023-09-14',\n  'title': 'An Empirical Evaluation of Prompting Strategies for Large Language\\n  Models in Zero-Shot Clinical Natural Language Processing',\n  'id': '2309.08008'},\n {'from': 'in-context learning',\n  'to': 'prompt engineering',\n  'name': 'involves',\n  'research': 'context',\n  'summary': 'In-context learning is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches',\n  'date': '2023-09-14',\n  'title': 'An Empirical Evaluation of Prompting Strategies for Large Language\\n  Models in Zero-Shot Clinical Natural Language Processing',\n  'id': '2309.08008'},\n {'from': 'this paper',\n  'to': 'clinical NLP tasks',\n  'name': 'presents',\n  'research': 'contribution',\n  'summary': 'A comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks',\n  'date': '2023-09-14',\n  'title': 'An Empirical Evaluation of Prompting Strategies for Large Language\\n  Models in Zero-Shot Clinical Natural Language Processing',\n  'id': '2309.08008'},\n {'from': 'prompt engineering',\n  'to': 'clinical NLP tasks',\n  'name': 'assessed for',\n  'research': 'method',\n  'summary': 'Assessed prompts include simple prefix, simple cloze, chain of thought, anticipatory prompts, and introduced heuristic and ensemble prompting',\n  'date': '2023-09-14',\n  'title': 'An Empirical Evaluation of Prompting Strategies for Large Language\\n  Models in Zero-Shot Clinical Natural Language Processing',\n  'id': '2309.08008'},\n {'from': 'GPT-3.5',\n  'to': 'clinical NLP tasks',\n  'name': 'evaluated for',\n  'research': 'method',\n  'summary': 'GPT-3.5 is evaluated for its performance on various prompts in clinical NLP tasks',\n  'date': '2023-09-14',\n  'title': 'An Empirical Evaluation of Prompting Strategies for Large Language\\n  Models in Zero-Shot Clinical Natural Language Processing',\n  'id': '2309.08008'},\n {'from': 'BARD',\n  'to': 'clinical NLP tasks',\n  'name': 'evaluated for',\n  'research': 'method',\n  'summary': 'BARD is evaluated for its performance on various prompts in clinical NLP tasks',\n  'date': '2023-09-14',\n  'title': 'An Empirical Evaluation of Prompting Strategies for Large Language\\n  Models in Zero-Shot Clinical Natural Language Processing',\n  'id': '2309.08008'},\n {'from': 'LLaMA2',\n  'to': 'clinical NLP tasks',\n  'name': 'evaluated for',\n  'research': 'method',\n  'summary': 'LLaMA2 is evaluated for its performance on various prompts in clinical NLP tasks',\n  'date': '2023-09-14',\n  'title': 'An Empirical Evaluation of Prompting Strategies for Large Language\\n  Models in Zero-Shot Clinical Natural Language Processing',\n  'id': '2309.08008'},\n {'from': 'zero-shot prompting',\n  'to': 'few-shot prompting',\n  'name': 'contrasted with',\n  'research': 'analysis',\n  'summary': 'Zero-shot prompting is contrasted with few-shot prompting in the context of clinical NLP tasks',\n  'date': '2023-09-14',\n  'title': 'An Empirical Evaluation of Prompting Strategies for Large Language\\n  Models in Zero-Shot Clinical Natural Language Processing',\n  'id': '2309.08008'},\n {'from': 'this paper',\n  'to': 'prompt engineering',\n  'name': 'provides',\n  'research': 'contribution',\n  'summary': 'Novel insights and guidelines for prompt engineering for LLMs in clinical NLP',\n  'date': '2023-09-14',\n  'title': 'An Empirical Evaluation of Prompting Strategies for Large Language\\n  Models in Zero-Shot Clinical Natural Language Processing',\n  'id': '2309.08008'},\n {'from': 'summarization',\n  'to': 'large language model',\n  'name': 'is an application of',\n  'research': 'context',\n  'summary': 'Summarization is an important application of large language models (LLMs)',\n  'date': '2023-09-14',\n  'title': 'Investigating Gender Bias in News Summarization',\n  'id': '2309.08047'},\n {'from': 'summarization model evaluation',\n  'to': 'content selection, grammaticality, coherence',\n  'name': 'has focused on',\n  'research': 'context',\n  'summary': 'Most previous evaluation of summarization models has focused on content selection, grammaticality, and coherence',\n  'date': '2023-09-14',\n  'title': 'Investigating Gender Bias in News Summarization',\n  'id': '2309.08047'},\n {'from': 'large language model',\n  'to': 'social biases',\n  'name': 'reproduce and reinforce',\n  'research': 'problem',\n  'summary': 'LLMs reproduce and reinforce harmful social biases',\n  'date': '2023-09-14',\n  'title': 'Investigating Gender Bias in News Summarization',\n  'id': '2309.08047'},\n {'from': 'social biases',\n  'to': 'summarization',\n  'name': 'question raised',\n  'research': 'question',\n  'summary': 'Do social biases affect model outputs in a relatively constrained setting like summarization?',\n  'date': '2023-09-14',\n  'title': 'Investigating Gender Bias in News Summarization',\n  'id': '2309.08047'},\n {'from': 'biased behaviours in summarization models',\n  'to': 'quantifying biased behaviours',\n  'name': 'definitions and measures introduced',\n  'research': 'contribution',\n  'summary': 'Definitions for biased behaviours in summarization models are introduced, along with practical measures to quantify them',\n  'date': '2023-09-14',\n  'title': 'Investigating Gender Bias in News Summarization',\n  'id': '2309.08047'},\n {'from': 'controlled demographic attributes in input documents',\n  'to': 'bias confounding',\n  'name': 'proposed to sidestep bias confounding',\n  'research': 'method',\n  'summary': 'A method to generate input documents with carefully controlled demographic attributes is proposed to sidestep bias confounding in the analysis',\n  'date': '2023-09-14',\n  'title': 'Investigating Gender Bias in News Summarization',\n  'id': '2309.08047'},\n {'from': 'purpose-built summarization model',\n  'to': 'quantifying biased behaviours',\n  'name': 'applied measures to',\n  'research': 'finding',\n  'summary': 'Measures are applied to summaries generated by purpose-built summarization models and general purpose chat models',\n  'date': '2023-09-14',\n  'title': 'Investigating Gender Bias in News Summarization',\n  'id': '2309.08047'},\n {'from': 'general purpose chat model',\n  'to': 'quantifying biased behaviours',\n  'name': 'applied measures to',\n  'research': 'finding',\n  'summary': 'Measures are applied to summaries generated by purpose-built summarization models and general purpose chat models',\n  'date': '2023-09-14',\n  'title': 'Investigating Gender Bias in News Summarization',\n  'id': '2309.08047'},\n {'from': 'content selection in single document summarization',\n  'to': 'social biases',\n  'name': 'largely unaffected by',\n  'research': 'finding',\n  'summary': 'Content selection in single document summarization seems to be largely unaffected by bias',\n  'date': '2023-09-14',\n  'title': 'Investigating Gender Bias in News Summarization',\n  'id': '2309.08047'},\n {'from': 'hallucinations in summarization',\n  'to': 'social biases',\n  'name': 'exhibit evidence of',\n  'research': 'finding',\n  'summary': 'Hallucinations in summarization exhibit evidence of biases propagating to generated summaries',\n  'date': '2023-09-14',\n  'title': 'Investigating Gender Bias in News Summarization',\n  'id': '2309.08047'},\n {'from': 'artificial intelligence',\n  'to': 'online education',\n  'name': 'applied in',\n  'research': 'context',\n  'summary': 'Artificial intelligence has been applied in various aspects of online education',\n  'date': '2023-09-15',\n  'title': 'Empowering Private Tutoring by Chaining Large Language Models',\n  'id': '2309.08112'},\n {'from': 'AI-powered tutoring system',\n  'to': 'large language model',\n  'name': 'development explored',\n  'research': 'contribution',\n  'summary': 'This work explores the development of a full-fledged intelligent tutoring system powered by state-of-the-art large language models',\n  'date': '2023-09-15',\n  'title': 'Empowering Private Tutoring by Chaining Large Language Models',\n  'id': '2309.08112'},\n {'from': 'intelligent tutoring system',\n  'to': 'education process automation',\n  'name': 'covers',\n  'research': 'contribution',\n  'summary': 'The intelligent tutoring system covers automatic course planning and adjusting, tailored instruction, and flexible quiz evaluation',\n  'date': '2023-09-15',\n  'title': 'Empowering Private Tutoring by Chaining Large Language Models',\n  'id': '2309.08112'},\n {'from': 'intelligent tutoring system',\n  'to': 'core processes',\n  'name': 'decomposed into',\n  'research': 'method',\n  'summary': 'The system is decomposed into three inter-connected core processes: interaction, reflection, and reaction',\n  'date': '2023-09-15',\n  'title': 'Empowering Private Tutoring by Chaining Large Language Models',\n  'id': '2309.08112'},\n {'from': 'core processes',\n  'to': 'interaction, reflection, reaction',\n  'name': 'consists of',\n  'research': 'method',\n  'summary': 'Core processes consist of interaction, reflection, and reaction',\n  'date': '2023-09-15',\n  'title': 'Empowering Private Tutoring by Chaining Large Language Models',\n  'id': '2309.08112'},\n {'from': 'LLM-powered tools',\n  'to': 'core processes',\n  'name': 'implement',\n  'research': 'method',\n  'summary': 'Each core process is implemented by chaining LLM-powered tools along with dynamically updated memory modules',\n  'date': '2023-09-15',\n  'title': 'Empowering Private Tutoring by Chaining Large Language Models',\n  'id': '2309.08112'},\n {'from': 'memory modules',\n  'to': 'education process',\n  'name': 'updated during',\n  'research': 'method',\n  'summary': 'Memory modules are data storage that gets updated during the education process',\n  'date': '2023-09-15',\n  'title': 'Empowering Private Tutoring by Chaining Large Language Models',\n  'id': '2309.08112'},\n {'from': 'statistical results from learning logs',\n  'to': 'tool effectiveness',\n  'name': 'demonstrate',\n  'research': 'finding',\n  'summary': 'Statistical results from learning logs demonstrate the effectiveness and mechanism of each tool usage',\n  'date': '2023-09-15',\n  'title': 'Empowering Private Tutoring by Chaining Large Language Models',\n  'id': '2309.08112'},\n {'from': 'subjective feedback from human users',\n  'to': 'usability',\n  'name': 'reveal',\n  'research': 'finding',\n  'summary': 'Subjective feedback from human users reveal the usability of each function',\n  'date': '2023-09-15',\n  'title': 'Empowering Private Tutoring by Chaining Large Language Models',\n  'id': '2309.08112'},\n {'from': 'intelligent tutoring system',\n  'to': 'ablation systems',\n  'name': 'compared with',\n  'research': 'evaluation',\n  'summary': 'Comparison with ablation systems further testify the benefits of the designed processes in long-term interaction',\n  'date': '2023-09-15',\n  'title': 'Empowering Private Tutoring by Chaining Large Language Models',\n  'id': '2309.08112'},\n {'from': 'software quality and security',\n  'to': 'software systems',\n  'name': 'is important for',\n  'research': 'context',\n  'summary': 'The quality and security of software systems are crucial due to their role in daily lives',\n  'date': '2023-09-15',\n  'title': 'REEF: A Framework for Collecting Real-World Vulnerabilities and Fixes',\n  'id': '2309.08115'},\n {'from': 'software vulnerabilities',\n  'to': 'software security',\n  'name': 'pose threat to',\n  'research': 'context',\n  'summary': 'Vulnerabilities in software pose a significant threat with serious consequences',\n  'date': '2023-09-15',\n  'title': 'REEF: A Framework for Collecting Real-World Vulnerabilities and Fixes',\n  'id': '2309.08115'},\n {'from': 'automated program repair',\n  'to': 'bug detection and fixing',\n  'name': 'aims to',\n  'research': 'context',\n  'summary': 'Automated program repair aims to automatically detect and fix bugs using data-driven techniques',\n  'date': '2023-09-15',\n  'title': 'REEF: A Framework for Collecting Real-World Vulnerabilities and Fixes',\n  'id': '2309.08115'},\n {'from': 'deep learning',\n  'to': 'automated program repair',\n  'name': 'applied to',\n  'research': 'context',\n  'summary': 'Sophisticated deep learning methods have been applied to automated program repair',\n  'date': '2023-09-15',\n  'title': 'REEF: A Framework for Collecting Real-World Vulnerabilities and Fixes',\n  'id': '2309.08115'},\n {'from': 'limited benchmarks',\n  'to': 'automated program repair',\n  'name': 'hinder',\n  'research': 'problem',\n  'summary': 'Existing benchmarks for training and evaluating automated program repair techniques are limited, outdated, and lack diversity',\n  'date': '2023-09-15',\n  'title': 'REEF: A Framework for Collecting Real-World Vulnerabilities and Fixes',\n  'id': '2309.08115'},\n {'from': 'Reef',\n  'to': 'collecting vulnerabilities and fixes',\n  'name': 'proposed to',\n  'research': 'contribution',\n  'summary': 'Reef is an automated collecting framework proposed to collect real-world vulnerabilities and fixes from open-source repositories',\n  'date': '2023-09-15',\n  'title': 'REEF: A Framework for Collecting Real-World Vulnerabilities and Fixes',\n  'id': '2309.08115'},\n {'from': 'multi-language crawler',\n  'to': 'collecting vulnerabilities and fixes',\n  'name': 'developed for',\n  'research': 'method',\n  'summary': 'A multi-language crawler is developed to collect vulnerabilities and their fixes',\n  'date': '2023-09-15',\n  'title': 'REEF: A Framework for Collecting Real-World Vulnerabilities and Fixes',\n  'id': '2309.08115'},\n {'from': 'neural language model',\n  'to': 'generating vulnerability explanations',\n  'name': 'used to generate',\n  'research': 'method',\n  'summary': 'A neural language model-based approach is proposed to generate high-quality vulnerability explanations',\n  'date': '2023-09-15',\n  'title': 'REEF: A Framework for Collecting Real-World Vulnerabilities and Fixes',\n  'id': '2309.08115'},\n {'from': 'vulnerability-fix pairs dataset',\n  'to': 'CVEs, patches, and CWEs',\n  'name': 'contains',\n  'research': 'contribution',\n  'summary': 'The collected dataset contains 4,466 CVEs with 30,987 patches across 7 programming languages, surpassing existing benchmarks in scale, coverage, and quality',\n  'date': '2023-09-15',\n  'title': 'REEF: A Framework for Collecting Real-World Vulnerabilities and Fixes',\n  'id': '2309.08115'},\n {'from': 'human expert evaluation',\n  'to': 'Reef',\n  'name': 'confirms',\n  'research': 'validation',\n  'summary': 'Evaluations by human experts confirm that the framework produces high-quality vulnerability explanations',\n  'date': '2023-09-15',\n  'title': 'REEF: A Framework for Collecting Real-World Vulnerabilities and Fixes',\n  'id': '2309.08115'},\n {'from': 'token-level serialized output training',\n  'to': 'streaming multi-talker ASR',\n  'name': 'addresses',\n  'research': 'context',\n  'summary': 'Token-level serialized output training (T-SOT) addresses the challenge of streaming multi-talker automatic speech recognition (ASR)',\n  'date': '2023-09-15',\n  'title': 't-SOT FNT: Streaming Multi-talker ASR with Text-only Domain Adaptation\\n  Capability',\n  'id': '2309.08131'},\n {'from': 'token-level serialized output training',\n  'to': 'overlapped speech',\n  'name': 'handles',\n  'research': 'context',\n  'summary': 'T-SOT effectively handles overlapped speech by representing multi-talker transcriptions as a single token stream with special symbols',\n  'date': '2023-09-15',\n  'title': 't-SOT FNT: Streaming Multi-talker ASR with Text-only Domain Adaptation\\n  Capability',\n  'id': '2309.08131'},\n {'from': 'naive neural transducer',\n  'to': 'text-only adaptation',\n  'name': 'constrains',\n  'research': 'problem',\n  'summary': \"The use of a naive neural transducer architecture significantly constrained T-SOT's applicability for text-only adaptation\",\n  'date': '2023-09-15',\n  'title': 't-SOT FNT: Streaming Multi-talker ASR with Text-only Domain Adaptation\\n  Capability',\n  'id': '2309.08131'},\n {'from': 'T-SOT with factorized neural transducer',\n  'to': 'naive neural transducer limitation',\n  'name': 'proposed to overcome',\n  'research': 'contribution',\n  'summary': 'A novel T-SOT model structure incorporating the idea of factorized neural transducers (FNT) is proposed to overcome limitations of the naive neural transducer',\n  'date': '2023-09-15',\n  'title': 't-SOT FNT: Streaming Multi-talker ASR with Text-only Domain Adaptation\\n  Capability',\n  'id': '2309.08131'},\n {'from': 'T-SOT with factorized neural transducer',\n  'to': 'language model',\n  'name': 'separates',\n  'research': 'method',\n  'summary': \"The proposed method separates a language model (LM) from the transducer's predictor\",\n  'date': '2023-09-15',\n  'title': 't-SOT FNT: Streaming Multi-talker ASR with Text-only Domain Adaptation\\n  Capability',\n  'id': '2309.08131'},\n {'from': 'T-SOT with factorized neural transducer',\n  'to': 'unnatural token order',\n  'name': 'handles',\n  'research': 'method',\n  'summary': 'The proposed T-SOT FNT model handles the unnatural token order resulting from the use of special symbols in T-SOT',\n  'date': '2023-09-15',\n  'title': 't-SOT FNT: Streaming Multi-talker ASR with Text-only Domain Adaptation\\n  Capability',\n  'id': '2309.08131'},\n {'from': 'T-SOT with factorized neural transducer',\n  'to': 'special token handling',\n  'name': 'introduces',\n  'research': 'method',\n  'summary': 'Special handling of the special tokens within the LM is introduced, maintaining multiple hidden states',\n  'date': '2023-09-15',\n  'title': 't-SOT FNT: Streaming Multi-talker ASR with Text-only Domain Adaptation\\n  Capability',\n  'id': '2309.08131'},\n {'from': 'T-SOT with factorized neural transducer',\n  'to': 'original T-SOT performance',\n  'name': 'achieves',\n  'research': 'result',\n  'summary': 'The proposed T-SOT FNT model achieves comparable performance to the original T-SOT model',\n  'date': '2023-09-15',\n  'title': 't-SOT FNT: Streaming Multi-talker ASR with Text-only Domain Adaptation\\n  Capability',\n  'id': '2309.08131'},\n {'from': 'T-SOT with factorized neural transducer',\n  'to': 'word error rate reduction',\n  'name': 'retains ability',\n  'research': 'result',\n  'summary': 'The proposed T-SOT FNT model retains the ability to reduce word error rate (WER) on both single and multi-talker datasets through text-only adaptation',\n  'date': '2023-09-15',\n  'title': 't-SOT FNT: Streaming Multi-talker ASR with Text-only Domain Adaptation\\n  Capability',\n  'id': '2309.08131'},\n {'from': 'visual object navigation',\n  'to': 'object presence and identification',\n  'name': 'requires',\n  'research': 'context',\n  'summary': \"The task of visual object navigation involves an agent's ability to locate a particular object within a given scene\",\n  'date': '2023-09-15',\n  'title': 'Find What You Want: Learning Demand-conditioned Object Attribute Space\\n  for Demand-driven Navigation',\n  'id': '2309.08138'},\n {'from': 'object presence and identification',\n  'to': 'object name and presence knowledge',\n  'name': 'includes',\n  'research': 'context',\n  'summary': \"Two essential conditions for VON: 1) knowing the name of the desired object, and 2) the object's presence in the scene\",\n  'date': '2023-09-15',\n  'title': 'Find What You Want: Learning Demand-conditioned Object Attribute Space\\n  for Demand-driven Navigation',\n  'id': '2309.08138'},\n {'from': 'scene metadata',\n  'to': 'pre-defined object names and positions',\n  'name': 'incorporates',\n  'research': 'context',\n  'summary': 'A simulator can incorporate pre-defined object names and positions into the scene metadata',\n  'date': '2023-09-15',\n  'title': 'Find What You Want: Learning Demand-conditioned Object Attribute Space\\n  for Demand-driven Navigation',\n  'id': '2309.08138'},\n {'from': 'real-world scenarios',\n  'to': 'visual object navigation',\n  'name': 'complicate',\n  'research': 'context',\n  'summary': 'Real-world scenarios often challenge the fulfillment of VON conditions',\n  'date': '2023-09-15',\n  'title': 'Find What You Want: Learning Demand-conditioned Object Attribute Space\\n  for Demand-driven Navigation',\n  'id': '2309.08138'},\n {'from': 'demand-driven navigation',\n  'to': 'visual object navigation',\n  'name': 'proposed for',\n  'research': 'contribution',\n  'summary': \"Demand-driven navigation is proposed to relax the stringent conditions of VON by focusing on fulfilling the user's demand\",\n  'date': '2023-09-15',\n  'title': 'Find What You Want: Learning Demand-conditioned Object Attribute Space\\n  for Demand-driven Navigation',\n  'id': '2309.08138'},\n {'from': 'textual attribute features extraction',\n  'to': 'large language model',\n  'name': 'uses',\n  'research': 'method',\n  'summary': 'Textual attribute features of objects are acquired by extracting common knowledge from a large language model',\n  'date': '2023-09-15',\n  'title': 'Find What You Want: Learning Demand-conditioned Object Attribute Space\\n  for Demand-driven Navigation',\n  'id': '2309.08138'},\n {'from': 'contrastive language-image pre-training',\n  'to': 'visual attribute features',\n  'name': 'aligns',\n  'research': 'method',\n  'summary': 'Textual attribute features are aligned with visual attribute features using CLIP',\n  'date': '2023-09-15',\n  'title': 'Find What You Want: Learning Demand-conditioned Object Attribute Space\\n  for Demand-driven Navigation',\n  'id': '2309.08138'},\n {'from': 'visual attribute features',\n  'to': 'navigation',\n  'name': 'serve as',\n  'research': 'method',\n  'summary': 'Visual attribute features serve as prior knowledge to enhance the navigation process',\n  'date': '2023-09-15',\n  'title': 'Find What You Want: Learning Demand-conditioned Object Attribute Space\\n  for Demand-driven Navigation',\n  'id': '2309.08138'},\n {'from': 'AI2-THOR with the ProcTHOR dataset',\n  'to': 'navigation performance',\n  'name': 'demonstrates',\n  'research': 'finding',\n  'summary': \"Experiments demonstrate that visual attribute features improve the agent's navigation performance and outperform baseline methods in VON\",\n  'date': '2023-09-15',\n  'title': 'Find What You Want: Learning Demand-conditioned Object Attribute Space\\n  for Demand-driven Navigation',\n  'id': '2309.08138'},\n {'from': 'large language model',\n  'to': 'psychological tools',\n  'name': 'subject of study',\n  'research': 'context',\n  'summary': 'Recent studies have tried to quantify the behavior of LLMs using psychological tools created for human behavior',\n  'date': '2023-09-15',\n  'title': 'Investigating the Applicability of Self-Assessment Tests for Personality\\n  Measurement of Large Language Models',\n  'id': '2309.08163'},\n {'from': 'personality',\n  'to': 'large language model',\n  'name': 'measured in',\n  'research': 'context',\n  'summary': 'Personality of LLMs is measured using personality self-assessment tests',\n  'date': '2023-09-15',\n  'title': 'Investigating the Applicability of Self-Assessment Tests for Personality\\n  Measurement of Large Language Models',\n  'id': '2309.08163'},\n {'from': 'personality self-assessment tests',\n  'to': 'large language model',\n  'name': 'used for',\n  'research': 'context',\n  'summary': 'Personality self-assessment tests created for human behavior are used to measure personality in LLMs',\n  'date': '2023-09-15',\n  'title': 'Investigating the Applicability of Self-Assessment Tests for Personality\\n  Measurement of Large Language Models',\n  'id': '2309.08163'},\n {'from': 'ChatGPT',\n  'to': 'personality',\n  'name': 'tested for',\n  'research': 'experiment',\n  'summary': \"ChatGPT's personality was measured using prompts from three different studies\",\n  'date': '2023-09-15',\n  'title': 'Investigating the Applicability of Self-Assessment Tests for Personality\\n  Measurement of Large Language Models',\n  'id': '2309.08163'},\n {'from': 'LLaMA2',\n  'to': 'personality',\n  'name': 'tested for',\n  'research': 'experiment',\n  'summary': \"LLaMA2's personality was measured using prompts from three different studies\",\n  'date': '2023-09-15',\n  'title': 'Investigating the Applicability of Self-Assessment Tests for Personality\\n  Measurement of Large Language Models',\n  'id': '2309.08163'},\n {'from': 'personality scores variability',\n  'to': 'prompting',\n  'name': 'result of',\n  'research': 'finding',\n  'summary': 'Personality self-assessment scores in LLMs depend on the subjective choice of the prompter',\n  'date': '2023-09-15',\n  'title': 'Investigating the Applicability of Self-Assessment Tests for Personality\\n  Measurement of Large Language Models',\n  'id': '2309.08163'},\n {'from': 'option order symmetry',\n  'to': 'large language model',\n  'name': 'introduced for',\n  'research': 'proposal',\n  'summary': 'Option order symmetry is introduced for personality measurement of LLMs to ensure robustness to option order in MCQs',\n  'date': '2023-09-15',\n  'title': 'Investigating the Applicability of Self-Assessment Tests for Personality\\n  Measurement of Large Language Models',\n  'id': '2309.08163'},\n {'from': 'self-assessment personality tests',\n  'to': 'large language model',\n  'name': 'found inappropriate for',\n  'research': 'conclusion',\n  'summary': 'Self-assessment personality tests created for humans are not appropriate for measuring personality in LLMs',\n  'date': '2023-09-15',\n  'title': 'Investigating the Applicability of Self-Assessment Tests for Personality\\n  Measurement of Large Language Models',\n  'id': '2309.08163'},\n {'from': 'self-speculative decoding',\n  'to': 'large language model',\n  'name': 'accelerates',\n  'research': 'contribution',\n  'summary': 'Self-speculative decoding accelerates large language models without needing an auxiliary model',\n  'date': '2023-09-15',\n  'title': 'Draft & Verify: Lossless Large Language Model Acceleration via\\n  Self-Speculative Decoding',\n  'id': '2309.08168'},\n {'from': 'self-speculative decoding',\n  'to': 'two-stage process',\n  'name': 'characterized by',\n  'research': 'contribution',\n  'summary': 'The approach is characterized by a two-stage process: drafting and verification',\n  'date': '2023-09-15',\n  'title': 'Draft & Verify: Lossless Large Language Model Acceleration via\\n  Self-Speculative Decoding',\n  'id': '2309.08168'},\n {'from': 'drafting',\n  'to': 'draft tokens',\n  'name': 'generates',\n  'research': 'contribution',\n  'summary': 'The drafting stage generates draft tokens quickly by selectively skipping certain intermediate layers',\n  'date': '2023-09-15',\n  'title': 'Draft & Verify: Lossless Large Language Model Acceleration via\\n  Self-Speculative Decoding',\n  'id': '2309.08168'},\n {'from': 'verification',\n  'to': 'large language model',\n  'name': 'employs',\n  'research': 'contribution',\n  'summary': 'The verification stage uses the original LLM to validate draft output tokens in one forward pass',\n  'date': '2023-09-15',\n  'title': 'Draft & Verify: Lossless Large Language Model Acceleration via\\n  Self-Speculative Decoding',\n  'id': '2309.08168'},\n {'from': 'final output',\n  'to': 'unmodified LLM output',\n  'name': 'remains identical to',\n  'research': 'contribution',\n  'summary': 'The final output remains identical to that produced by the unaltered LLM, maintaining output quality',\n  'date': '2023-09-15',\n  'title': 'Draft & Verify: Lossless Large Language Model Acceleration via\\n  Self-Speculative Decoding',\n  'id': '2309.08168'},\n {'from': 'self-speculative decoding',\n  'to': 'additional training or memory',\n  'name': 'requires no',\n  'research': 'advantage',\n  'summary': 'The proposed method requires no additional neural network training and no extra memory footprint',\n  'date': '2023-09-15',\n  'title': 'Draft & Verify: Lossless Large Language Model Acceleration via\\n  Self-Speculative Decoding',\n  'id': '2309.08168'},\n {'from': 'self-speculative decoding',\n  'to': 'plug-and-play and cost-effective',\n  'name': 'is',\n  'research': 'advantage',\n  'summary': 'Self-speculative decoding is a plug-and-play and cost-effective solution for inference acceleration',\n  'date': '2023-09-15',\n  'title': 'Draft & Verify: Lossless Large Language Model Acceleration via\\n  Self-Speculative Decoding',\n  'id': '2309.08168'},\n {'from': 'with LLaMA-2 and fine-tuned models',\n  'to': 'speedup',\n  'name': 'demonstrated',\n  'research': 'result',\n  'summary': 'Benchmarks with LLaMA-2 and its fine-tuned models demonstrated a speedup up to 1.73 times',\n  'date': '2023-09-15',\n  'title': 'Draft & Verify: Lossless Large Language Model Acceleration via\\n  Self-Speculative Decoding',\n  'id': '2309.08168'},\n {'from': 'large language model',\n  'to': 'interactive decision-making',\n  'name': 'adapted for',\n  'research': 'context',\n  'summary': 'LLMs have been successfully adapted for interactive decision-making tasks like web navigation',\n  'date': '2023-09-15',\n  'title': 'LASER: LLM Agent with State-Space Exploration for Web Navigation',\n  'id': '2309.08172'},\n {'from': 'oracle trajectories',\n  'to': 'large language model',\n  'name': 'used to teach',\n  'research': 'context',\n  'summary': 'Previous methods use oracle trajectories as in-context examples to teach the model how to reason in interactive environments',\n  'date': '2023-09-15',\n  'title': 'LASER: LLM Agent with State-Space Exploration for Web Navigation',\n  'id': '2309.08172'},\n {'from': 'large language model',\n  'to': 'forward-only',\n  'name': 'assumed to have',\n  'research': 'limitation',\n  'summary': 'Previous methods implicitly assume a forward-only execution mode for the model',\n  'date': '2023-09-15',\n  'title': 'LASER: LLM Agent with State-Space Exploration for Web Navigation',\n  'id': '2309.08172'},\n {'from': 'large language model',\n  'to': 'challenging scenarios',\n  'name': 'struggles with',\n  'research': 'problem',\n  'summary': 'The model could not handle challenging scenarios not covered in the in-context examples, such as mistakes',\n  'date': '2023-09-15',\n  'title': 'LASER: LLM Agent with State-Space Exploration for Web Navigation',\n  'id': '2309.08172'},\n {'from': 'state space exploration',\n  'to': 'interactive decision-making',\n  'name': 'proposed to model',\n  'research': 'contribution',\n  'summary': 'State space exploration is proposed to model the interactive task, enabling the LLM agent to transition among states by performing actions',\n  'date': '2023-09-15',\n  'title': 'LASER: LLM Agent with State-Space Exploration for Web Navigation',\n  'id': '2309.08172'},\n {'from': 'flexible back-tracking',\n  'to': 'state space exploration',\n  'name': 'enabled by',\n  'research': 'contribution',\n  'summary': 'The state space exploration formulation enables flexible back-tracking, allowing the model to recover from errors',\n  'date': '2023-09-15',\n  'title': 'LASER: LLM Agent with State-Space Exploration for Web Navigation',\n  'id': '2309.08172'},\n {'from': 'LASER',\n  'to': 'WebShop',\n  'name': 'evaluated on',\n  'research': 'method',\n  'summary': 'The LASER agent with state-space exploration is evaluated on the WebShop task',\n  'date': '2023-09-15',\n  'title': 'LASER: LLM Agent with State-Space Exploration for Web Navigation',\n  'id': '2309.08172'},\n {'from': 'LASER',\n  'to': 'previous methods',\n  'name': 'outperforms',\n  'research': 'finding',\n  'summary': 'The LASER agent significantly outperforms previous methods and closes the gap with human performance on the web navigation task',\n  'date': '2023-09-15',\n  'title': 'LASER: LLM Agent with State-Space Exploration for Web Navigation',\n  'id': '2309.08172'},\n {'from': 'LASER',\n  'to': 'human performance',\n  'name': 'closes gap with',\n  'research': 'finding',\n  'summary': 'The LASER agent significantly outperforms previous methods and closes the gap with human performance on the web navigation task',\n  'date': '2023-09-15',\n  'title': 'LASER: LLM Agent with State-Space Exploration for Web Navigation',\n  'id': '2309.08172'},\n {'from': 'legal intelligence',\n  'to': 'large language model',\n  'name': 'utilizes',\n  'research': 'context',\n  'summary': 'Large language models have gained prominence in legal intelligence, offering potential applications in assisting legal professionals and laymen',\n  'date': '2023-09-15',\n  'title': 'FedJudge: Federated Legal Large Language Model',\n  'id': '2309.08173'},\n {'from': 'large language model',\n  'to': 'data privacy',\n  'name': 'raises concerns in',\n  'research': 'context',\n  'summary': 'The centralized training of legal LLMs raises data privacy concerns due to sensitive individual information in legal data',\n  'date': '2023-09-15',\n  'title': 'FedJudge: Federated Legal Large Language Model',\n  'id': '2309.08173'},\n {'from': 'federated learning',\n  'to': 'large language model',\n  'name': 'integrated with',\n  'research': 'contribution',\n  'summary': 'The paper explores the integration of legal LLMs with federated learning methodologies',\n  'date': '2023-09-15',\n  'title': 'FedJudge: Federated Legal Large Language Model',\n  'id': '2309.08173'},\n {'from': 'federated learning',\n  'to': 'fine-tuning',\n  'name': 'enables',\n  'research': 'method',\n  'summary': 'Federated learning enables fine-tuning of legal LLMs locally on devices or clients, ensuring data privacy without directly sharing raw data',\n  'date': '2023-09-15',\n  'title': 'FedJudge: Federated Legal Large Language Model',\n  'id': '2309.08173'},\n {'from': 'computation and communication overheads',\n  'to': 'fine-tuning',\n  'name': 'hinder',\n  'research': 'context',\n  'summary': 'Computation and communication overheads hinder the full fine-tuning of LLMs under the federated learning setting',\n  'date': '2023-09-15',\n  'title': 'FedJudge: Federated Legal Large Language Model',\n  'id': '2309.08173'},\n {'from': 'distribution shift of legal data',\n  'to': 'federated learning',\n  'name': 'reduces effectiveness of',\n  'research': 'context',\n  'summary': 'The distribution shift of legal data reduces the effectiveness of federated learning methods',\n  'date': '2023-09-15',\n  'title': 'FedJudge: Federated Legal Large Language Model',\n  'id': '2309.08173'},\n {'from': 'FedJudge',\n  'to': 'large language model',\n  'name': 'proposed for',\n  'research': 'contribution',\n  'summary': 'FedJudge is proposed to fine-tune legal LLMs efficiently and effectively under federated learning',\n  'date': '2023-09-15',\n  'title': 'FedJudge: Federated Legal Large Language Model',\n  'id': '2309.08173'},\n {'from': 'FedJudge',\n  'to': 'parameter-efficient fine-tuning',\n  'name': 'utilizes',\n  'research': 'method',\n  'summary': 'FedJudge utilizes parameter-efficient fine-tuning methods to update only a few additional parameters during federated learning training',\n  'date': '2023-09-15',\n  'title': 'FedJudge: Federated Legal Large Language Model',\n  'id': '2309.08173'},\n {'from': 'continual learning',\n  'to': 'distribution shift of legal data',\n  'name': 'explored to',\n  'research': 'method',\n  'summary': \"Continual learning methods are explored to preserve the global model's important parameters when training local clients to mitigate the problem of data shifts\",\n  'date': '2023-09-15',\n  'title': 'FedJudge: Federated Legal Large Language Model',\n  'id': '2309.08173'},\n {'from': 'FedJudge',\n  'to': 'experimental results',\n  'name': 'validated by',\n  'research': 'validation',\n  'summary': 'Extensive experimental results on three real-world datasets validate the effectiveness of FedJudge',\n  'date': '2023-09-15',\n  'title': 'FedJudge: Federated Legal Large Language Model',\n  'id': '2309.08173'},\n {'from': 'FedJudge code',\n  'to': 'GitHub',\n  'name': 'available at',\n  'research': 'resource',\n  'summary': 'Code for FedJudge is released on GitHub',\n  'date': '2023-09-15',\n  'title': 'FedJudge: Federated Legal Large Language Model',\n  'id': '2309.08173'},\n {'from': 'large language model',\n  'to': 'Failure Mode Classification (FMC)',\n  'name': 'investigated for',\n  'research': 'contribution',\n  'summary': 'This paper presents the first investigation into the effectiveness of LLMs for Failure Mode Classification (FMC)',\n  'date': '2023-09-15',\n  'title': 'Large Language Models for Failure Mode Classification: An Investigation',\n  'id': '2309.08181'},\n {'from': 'Failure Mode Classification (FMC)',\n  'to': 'maintenance',\n  'name': 'critical in',\n  'research': 'context',\n  'summary': 'FMC is a critical task in the maintenance domain as it reduces the need for manual analysis of work orders by reliability engineers',\n  'date': '2023-09-15',\n  'title': 'Large Language Models for Failure Mode Classification: An Investigation',\n  'id': '2309.08181'},\n {'from': 'prompt engineering',\n  'to': 'Failure Mode Classification (FMC)',\n  'name': 'enables',\n  'research': 'method',\n  'summary': 'Prompt engineering enables an LLM to predict the failure mode of a given observation using a restricted code list',\n  'date': '2023-09-15',\n  'title': 'Large Language Models for Failure Mode Classification: An Investigation',\n  'id': '2309.08181'},\n {'from': 'GPT-3.5',\n  'to': 'Failure Mode Classification (FMC)',\n  'name': 'fine-tuned for',\n  'research': 'contribution',\n  'summary': 'A fine-tuned GPT-3.5 model significantly improves performance over a currently available text classification model for FMC',\n  'date': '2023-09-15',\n  'title': 'Large Language Models for Failure Mode Classification: An Investigation',\n  'id': '2309.08181'},\n {'from': 'GPT-3.5',\n  'to': 'text classification model',\n  'name': 'compared to',\n  'research': 'finding',\n  'summary': 'The fine-tuned GPT-3.5 model (F1=0.80) outperforms the out-of-the-box GPT-3.5 (F1=0.46) and a text classification model (F1=0.60)',\n  'date': '2023-09-15',\n  'title': 'Large Language Models for Failure Mode Classification: An Investigation',\n  'id': '2309.08181'},\n {'from': 'GPT-3.5',\n  'to': 'out-of-the-box GPT-3.5',\n  'name': 'outperforms',\n  'research': 'finding',\n  'summary': 'The fine-tuned GPT-3.5 model outperforms the out-of-the-box GPT-3.5 in FMC',\n  'date': '2023-09-15',\n  'title': 'Large Language Models for Failure Mode Classification: An Investigation',\n  'id': '2309.08181'},\n {'from': 'high quality fine-tuning data sets',\n  'to': 'domain-specific tasks',\n  'name': 'needed for',\n  'research': 'reflection',\n  'summary': 'High quality fine-tuning data sets are needed for domain-specific tasks using LLMs',\n  'date': '2023-09-15',\n  'title': 'Large Language Models for Failure Mode Classification: An Investigation',\n  'id': '2309.08181'},\n {'from': 'large language model',\n  'to': 'physics word problems',\n  'name': 'capable of solving',\n  'research': 'finding',\n  'summary': 'LLMs pre-trained on texts can solve math and physics word problems',\n  'date': '2023-09-15',\n  'title': 'Using Large Language Model to Solve and Explain Physics Word Problems\\n  Approaching Human Level',\n  'id': '2309.08182'},\n {'from': 'PhysQA',\n  'to': 'physics word problems',\n  'name': 'created for',\n  'research': 'contribution',\n  'summary': 'PhysQA, the first physics word problem dataset, is collected and annotated for LLM testing',\n  'date': '2023-09-15',\n  'title': 'Using Large Language Model to Solve and Explain Physics Word Problems\\n  Approaching Human Level',\n  'id': '2309.08182'},\n {'from': 'PhysQA',\n  'to': 'physics topics',\n  'name': 'contains',\n  'research': 'context',\n  'summary': 'PhysQA contains over 1000 junior high school physics word problems covering various topics',\n  'date': '2023-09-15',\n  'title': 'Using Large Language Model to Solve and Explain Physics Word Problems\\n  Approaching Human Level',\n  'id': '2309.08182'},\n {'from': 'GPT-3.5',\n  'to': 'PhysQA',\n  'name': 'used to solve',\n  'research': 'method',\n  'summary': 'GPT-3.5 is used to generate answers for physics word problems in PhysQA',\n  'date': '2023-09-15',\n  'title': 'Using Large Language Model to Solve and Explain Physics Word Problems\\n  Approaching Human Level',\n  'id': '2309.08182'},\n {'from': 'GPT-3.5',\n  'to': 'problem-solving accuracy',\n  'name': 'achieves accuracy',\n  'research': 'finding',\n  'summary': 'GPT-3.5 solves 49.3% of problems through zero-shot learning and 73.2% through few-shot learning',\n  'date': '2023-09-15',\n  'title': 'Using Large Language Model to Solve and Explain Physics Word Problems\\n  Approaching Human Level',\n  'id': '2309.08182'},\n {'from': 'few-shot learning',\n  'to': 'physics word problems',\n  'name': 'improves',\n  'research': 'finding',\n  'summary': \"Few-shot learning improves LLM's ability to solve physics word problems\",\n  'date': '2023-09-15',\n  'title': 'Using Large Language Model to Solve and Explain Physics Word Problems\\n  Approaching Human Level',\n  'id': '2309.08182'},\n {'from': 'GPT-3.5',\n  'to': 'knowledge summarization and problem generation',\n  'name': 'can perform',\n  'research': 'finding',\n  'summary': 'GPT-3.5 can summarize knowledge, provide explanations, and generate new physics word problems',\n  'date': '2023-09-15',\n  'title': 'Using Large Language Model to Solve and Explain Physics Word Problems\\n  Approaching Human Level',\n  'id': '2309.08182'},\n {'from': 'this work',\n  'to': 'physics word problems',\n  'name': 'is first to',\n  'research': 'contribution',\n  'summary': 'This work is the first to focus on automatic solving, explanation, and generation of physics word problems',\n  'date': '2023-09-15',\n  'title': 'Using Large Language Model to Solve and Explain Physics Word Problems\\n  Approaching Human Level',\n  'id': '2309.08182'},\n {'from': 'large language model',\n  'to': 'secondary education',\n  'name': 'has potential in',\n  'research': 'reflection',\n  'summary': 'LLMs have potential for further applications in secondary education',\n  'date': '2023-09-15',\n  'title': 'Using Large Language Model to Solve and Explain Physics Word Problems\\n  Approaching Human Level',\n  'id': '2309.08182'},\n {'from': 'code review',\n  'to': 'software quality and maintainability',\n  'name': 'is for',\n  'research': 'context',\n  'summary': 'Code review is essential for ensuring the quality and maintainability of software projects',\n  'date': '2023-09-15',\n  'title': 'Exploring the Potential of ChatGPT in Automated Code Refinement: An\\n  Empirical Study',\n  'id': '2309.08221'},\n {'from': 'code review',\n  'to': 'time-consuming and error-prone',\n  'name': 'characterized by',\n  'research': 'context',\n  'summary': 'Code review is time-consuming and often error-prone',\n  'date': '2023-09-15',\n  'title': 'Exploring the Potential of ChatGPT in Automated Code Refinement: An\\n  Empirical Study',\n  'id': '2309.08221'},\n {'from': 'ChatGPT',\n  'to': 'automating code review',\n  'name': 'potential for',\n  'research': 'context',\n  'summary': 'ChatGPT has demonstrated potential to automate code review processes',\n  'date': '2023-09-15',\n  'title': 'Exploring the Potential of ChatGPT in Automated Code Refinement: An\\n  Empirical Study',\n  'id': '2309.08221'},\n {'from': 'empirical study on ChatGPT in code review',\n  'to': 'ChatGPT in code review',\n  'name': 'conducted to understand',\n  'research': 'contribution',\n  'summary': 'The first empirical study to understand the capabilities of ChatGPT in code review tasks',\n  'date': '2023-09-15',\n  'title': 'Exploring the Potential of ChatGPT in Automated Code Refinement: An\\n  Empirical Study',\n  'id': '2309.08221'},\n {'from': 'automated code refinement',\n  'to': 'code reviews',\n  'name': 'based on',\n  'research': 'focus',\n  'summary': 'The study focuses on automated code refinement based on given code reviews',\n  'date': '2023-09-15',\n  'title': 'Exploring the Potential of ChatGPT in Automated Code Refinement: An\\n  Empirical Study',\n  'id': '2309.08221'},\n {'from': 'CodeReview',\n  'to': 'empirical study on ChatGPT in code review',\n  'name': 'selected for',\n  'research': 'method',\n  'summary': 'CodeReview benchmark is selected for the study',\n  'date': '2023-09-15',\n  'title': 'Exploring the Potential of ChatGPT in Automated Code Refinement: An\\n  Empirical Study',\n  'id': '2309.08221'},\n {'from': 'new high-quality code review dataset',\n  'to': 'empirical study on ChatGPT in code review',\n  'name': 'constructed for',\n  'research': 'method',\n  'summary': 'A new high-quality code review dataset is constructed for the study',\n  'date': '2023-09-15',\n  'title': 'Exploring the Potential of ChatGPT in Automated Code Refinement: An\\n  Empirical Study',\n  'id': '2309.08221'},\n {'from': 'CodeReviewer',\n  'to': 'ChatGPT',\n  'name': 'used as',\n  'research': 'method',\n  'summary': 'CodeReviewer is used as a baseline for comparison with ChatGPT',\n  'date': '2023-09-15',\n  'title': 'Exploring the Potential of ChatGPT in Automated Code Refinement: An\\n  Empirical Study',\n  'id': '2309.08221'},\n {'from': 'ChatGPT',\n  'to': 'CodeReviewer',\n  'name': 'outperforms',\n  'research': 'finding',\n  'summary': 'ChatGPT outperforms CodeReviewer in code refinement tasks',\n  'date': '2023-09-15',\n  'title': 'Exploring the Potential of ChatGPT in Automated Code Refinement: An\\n  Empirical Study',\n  'id': '2309.08221'},\n {'from': 'ChatGPT',\n  'to': 'EM and BLEU scores',\n  'name': 'achieves scores',\n  'research': 'finding',\n  'summary': 'ChatGPT achieves higher EM and BLEU scores of 22.78 and 76.44 respectively on a high-quality code review dataset',\n  'date': '2023-09-15',\n  'title': 'Exploring the Potential of ChatGPT in Automated Code Refinement: An\\n  Empirical Study',\n  'id': '2309.08221'},\n {'from': 'ChatGPTs underperformance',\n  'to': 'ChatGPT',\n  'name': 'root causes identified for',\n  'research': 'finding',\n  'summary': \"The study identifies the root causes for ChatGPT's underperformance in code review tasks\",\n  'date': '2023-09-15',\n  'title': 'Exploring the Potential of ChatGPT in Automated Code Refinement: An\\n  Empirical Study',\n  'id': '2309.08221'},\n {'from': 'mitigation strategies for ChatGPTs challenges',\n  'to': 'ChatGPTs underperformance',\n  'name': 'proposed for',\n  'research': 'contribution',\n  'summary': 'Several strategies are proposed to mitigate the challenges faced by ChatGPT in code review tasks',\n  'date': '2023-09-15',\n  'title': 'Exploring the Potential of ChatGPT in Automated Code Refinement: An\\n  Empirical Study',\n  'id': '2309.08221'},\n {'from': 'potential of ChatGPT in automating code review',\n  'to': 'automating code review',\n  'name': 'provided by',\n  'research': 'contribution',\n  'summary': 'The study provides insights into the potential of ChatGPT in automating the code review process',\n  'date': '2023-09-15',\n  'title': 'Exploring the Potential of ChatGPT in Automated Code Refinement: An\\n  Empirical Study',\n  'id': '2309.08221'},\n {'from': 'potential research directions highlighted',\n  'to': 'automating code review',\n  'name': 'highlighted by',\n  'research': 'reflection',\n  'summary': 'The study highlights potential research directions for automating code review with ChatGPT',\n  'date': '2023-09-15',\n  'title': 'Exploring the Potential of ChatGPT in Automated Code Refinement: An\\n  Empirical Study',\n  'id': '2309.08221'},\n {'from': 'pre-training',\n  'to': 'unsupervised raw data',\n  'name': 'improved by',\n  'research': 'contribution',\n  'summary': 'The thesis focuses on improving the pre-training of natural language models using unsupervised raw data',\n  'date': '2023-09-15',\n  'title': 'Structural Self-Supervised Objectives for Transformers',\n  'id': '2309.08272'},\n {'from': 'natural language model',\n  'to': 'downstream applications',\n  'name': 'aligned with',\n  'research': 'goal',\n  'summary': 'Pre-training improvements aim to align natural language models with downstream applications',\n  'date': '2023-09-15',\n  'title': 'Structural Self-Supervised Objectives for Transformers',\n  'id': '2309.08272'},\n {'from': 'masked language modeling (MLM)',\n  'to': 'random token substitution (RTS)',\n  'name': 'alternative to',\n  'research': 'contribution',\n  'summary': \"Three alternative pre-training objectives to BERT's MLM are introduced: RTS, C-RTS, and SLM\",\n  'date': '2023-09-15',\n  'title': 'Structural Self-Supervised Objectives for Transformers',\n  'id': '2309.08272'},\n {'from': 'random token substitution (RTS)',\n  'to': 'token originality prediction',\n  'name': 'aims to predict',\n  'research': 'method',\n  'summary': 'RTS aims to predict token originality',\n  'date': '2023-09-15',\n  'title': 'Structural Self-Supervised Objectives for Transformers',\n  'id': '2309.08272'},\n {'from': 'cluster-based random token substitution (C-RTS)',\n  'to': 'token originality prediction',\n  'name': 'aims to predict',\n  'research': 'method',\n  'summary': 'C-RTS aims to predict token originality',\n  'date': '2023-09-15',\n  'title': 'Structural Self-Supervised Objectives for Transformers',\n  'id': '2309.08272'},\n {'from': 'swapped language modeling (SLM)',\n  'to': 'original token value prediction',\n  'name': 'aims to predict',\n  'research': 'method',\n  'summary': 'SLM predicts the original token values',\n  'date': '2023-09-15',\n  'title': 'Structural Self-Supervised Objectives for Transformers',\n  'id': '2309.08272'},\n {'from': 'swapped language modeling (SLM)',\n  'to': 'masked language modeling (MLM)',\n  'name': 'outperforms',\n  'research': 'finding',\n  'summary': 'SLM outperforms MLM on certain tasks using the same computational budget',\n  'date': '2023-09-15',\n  'title': 'Structural Self-Supervised Objectives for Transformers',\n  'id': '2309.08272'},\n {'from': 'self-supervised pre-training',\n  'to': 'downstream applications',\n  'name': 'aligns with',\n  'research': 'contribution',\n  'summary': 'Self-supervised pre-training tasks are proposed that structurally align with downstream applications',\n  'date': '2023-09-15',\n  'title': 'Structural Self-Supervised Objectives for Transformers',\n  'id': '2309.08272'},\n {'from': 'Wikipedia',\n  'to': 'self-supervised pre-training',\n  'name': 'used for',\n  'research': 'method',\n  'summary': 'Large corpora like Wikipedia are used to train models on self-supervised pre-training tasks',\n  'date': '2023-09-15',\n  'title': 'Structural Self-Supervised Objectives for Transformers',\n  'id': '2309.08272'},\n {'from': 'RoBERTa',\n  'to': 'continuous pre-training',\n  'name': 'used for',\n  'research': 'method',\n  'summary': 'Continuous pre-training starting from existing models like RoBERTa shows significant performance improvements',\n  'date': '2023-09-15',\n  'title': 'Structural Self-Supervised Objectives for Transformers',\n  'id': '2309.08272'},\n {'from': 'fact verification',\n  'to': 'continuous pre-training',\n  'name': 'improved by',\n  'research': 'finding',\n  'summary': 'Continuous pre-training leads to performance improvements in tasks like fact verification',\n  'date': '2023-09-15',\n  'title': 'Structural Self-Supervised Objectives for Transformers',\n  'id': '2309.08272'},\n {'from': 'answer sentence selection',\n  'to': 'continuous pre-training',\n  'name': 'improved by',\n  'research': 'finding',\n  'summary': 'Continuous pre-training leads to performance improvements in tasks like answer sentence selection',\n  'date': '2023-09-15',\n  'title': 'Structural Self-Supervised Objectives for Transformers',\n  'id': '2309.08272'},\n {'from': 'summarization',\n  'to': 'continuous pre-training',\n  'name': 'improved by',\n  'research': 'finding',\n  'summary': 'Continuous pre-training leads to performance improvements in tasks like summarization',\n  'date': '2023-09-15',\n  'title': 'Structural Self-Supervised Objectives for Transformers',\n  'id': '2309.08272'},\n {'from': 'FEVER (dev set)',\n  'to': 'proposed objectives',\n  'name': 'achieves state-of-the-art results on',\n  'research': 'finding',\n  'summary': 'The proposed objectives achieve state-of-the-art results on various benchmark datasets, including FEVER (dev set)',\n  'date': '2023-09-15',\n  'title': 'Structural Self-Supervised Objectives for Transformers',\n  'id': '2309.08272'},\n {'from': 'integration with other methods',\n  'to': 'Transformer',\n  'name': 'possible without altering',\n  'research': 'advantage',\n  'summary': 'The techniques can be integrated with other methods without altering the internal structure of transformer models',\n  'date': '2023-09-15',\n  'title': 'Structural Self-Supervised Objectives for Transformers',\n  'id': '2309.08272'},\n {'from': 'abduction',\n  'to': 'narrative comprehension',\n  'name': 'is crucial for',\n  'research': 'context',\n  'summary': 'Abduction has long been seen as crucial for narrative comprehension and reasoning about everyday situations',\n  'date': '2023-09-15',\n  'title': 'Self-Consistent Narrative Prompts on Abductive Natural Language\\n  Inference',\n  'id': '2309.08303'},\n {'from': 'abductive natural language inference',\n  'to': 'plausible hypothesis inference',\n  'name': 'aims to',\n  'research': 'context',\n  'summary': 'The abductive natural language inference task aims to infer the most plausible hypothesis from the candidates given two observations',\n  'date': '2023-09-15',\n  'title': 'Self-Consistent Narrative Prompts on Abductive Natural Language\\n  Inference',\n  'id': '2309.08303'},\n {'from': 'inter-sentential coherence',\n  'to': 'abductive natural language inference',\n  'name': 'not well exploited in',\n  'research': 'problem',\n  'summary': 'Inter-sentential coherence has not been well exploited in previous works on the abductive natural language inference task',\n  'date': '2023-09-15',\n  'title': 'Self-Consistent Narrative Prompts on Abductive Natural Language\\n  Inference',\n  'id': '2309.08303'},\n {'from': 'model consistency',\n  'to': 'abductive natural language inference',\n  'name': 'not well exploited in',\n  'research': 'problem',\n  'summary': 'Model consistency has not been well exploited in previous works on the abductive natural language inference task',\n  'date': '2023-09-15',\n  'title': 'Self-Consistent Narrative Prompts on Abductive Natural Language\\n  Inference',\n  'id': '2309.08303'},\n {'from': 'Alpha-PACE',\n  'to': 'self-consistency and inter-sentential coherence',\n  'name': 'considers',\n  'research': 'contribution',\n  'summary': 'Alpha-PACE is a prompt tuning model that takes self-consistency and inter-sentential coherence into consideration',\n  'date': '2023-09-15',\n  'title': 'Self-Consistent Narrative Prompts on Abductive Natural Language\\n  Inference',\n  'id': '2309.08303'},\n {'from': 'general self-consistent framework',\n  'to': 'pre-trained language model',\n  'name': 'proposed for',\n  'research': 'contribution',\n  'summary': 'A general self-consistent framework is proposed for guiding pre-trained language models in understanding various narrative sequences',\n  'date': '2023-09-15',\n  'title': 'Self-Consistent Narrative Prompts on Abductive Natural Language\\n  Inference',\n  'id': '2309.08303'},\n {'from': 'linear narrative',\n  'to': 'general self-consistent framework',\n  'name': 'considered in',\n  'research': 'context',\n  'summary': 'Various narrative sequences such as linear narrative and reverse chronology are considered in the general self-consistent framework',\n  'date': '2023-09-15',\n  'title': 'Self-Consistent Narrative Prompts on Abductive Natural Language\\n  Inference',\n  'id': '2309.08303'},\n {'from': 'reverse chronology',\n  'to': 'general self-consistent framework',\n  'name': 'considered in',\n  'research': 'context',\n  'summary': 'Various narrative sequences such as linear narrative and reverse chronology are considered in the general self-consistent framework',\n  'date': '2023-09-15',\n  'title': 'Self-Consistent Narrative Prompts on Abductive Natural Language\\n  Inference',\n  'id': '2309.08303'},\n {'from': 'Alpha-PACE',\n  'to': 'necessity and effectiveness',\n  'name': 'experiments and studies illustrate',\n  'research': 'method',\n  'summary': 'Extensive experiments and thorough ablation studies illustrate the necessity and effectiveness of Alpha-PACE',\n  'date': '2023-09-15',\n  'title': 'Self-Consistent Narrative Prompts on Abductive Natural Language\\n  Inference',\n  'id': '2309.08303'},\n {'from': 'Alpha-PACE performance',\n  'to': 'competitive baselines',\n  'name': 'shows improvement against',\n  'research': 'finding',\n  'summary': 'The performance of Alpha-PACE shows significant improvement against extensive competitive baselines',\n  'date': '2023-09-15',\n  'title': 'Self-Consistent Narrative Prompts on Abductive Natural Language\\n  Inference',\n  'id': '2309.08303'},\n {'from': 'language model',\n  'to': 'in-distribution (ID)',\n  'name': 'perform well in',\n  'research': 'context',\n  'summary': 'Language models excel in in-distribution (ID) scenarios',\n  'date': '2023-09-15',\n  'title': 'Bridging Topic, Domain, and Language Shifts: An Evaluation of\\n  Comprehensive Out-of-Distribution Scenarios',\n  'id': '2309.08316'},\n {'from': 'language model',\n  'to': 'out-of-distribution (OOD)',\n  'name': 'perform poorly in',\n  'research': 'problem',\n  'summary': 'Language models often degrade in performance in out-of-distribution (OOD) scenarios',\n  'date': '2023-09-15',\n  'title': 'Bridging Topic, Domain, and Language Shifts: An Evaluation of\\n  Comprehensive Out-of-Distribution Scenarios',\n  'id': '2309.08316'},\n {'from': 'argument mining',\n  'to': 'out-of-distribution (OOD)',\n  'name': 'exemplifies',\n  'research': 'context',\n  'summary': \"Argument mining exemplifies real-world applications where language models' performance degrades\",\n  'date': '2023-09-15',\n  'title': 'Bridging Topic, Domain, and Language Shifts: An Evaluation of\\n  Comprehensive Out-of-Distribution Scenarios',\n  'id': '2309.08316'},\n {'from': 'simulation of distribution shifts',\n  'to': 'language model',\n  'name': 'used to assess',\n  'research': 'method',\n  'summary': \"Simulation of distribution shifts is used to assess LMs' generalization abilities in OOD scenarios\",\n  'date': '2023-09-15',\n  'title': 'Bridging Topic, Domain, and Language Shifts: An Evaluation of\\n  Comprehensive Out-of-Distribution Scenarios',\n  'id': '2309.08316'},\n {'from': 'social media',\n  'to': 'out-of-distribution (OOD)',\n  'name': 'used in',\n  'research': 'method',\n  'summary': 'Withholding instances from the social media domain for testing to simulate distribution shifts',\n  'date': '2023-09-15',\n  'title': 'Bridging Topic, Domain, and Language Shifts: An Evaluation of\\n  Comprehensive Out-of-Distribution Scenarios',\n  'id': '2309.08316'},\n {'from': 'solar energy',\n  'to': 'out-of-distribution (OOD)',\n  'name': 'used in',\n  'research': 'method',\n  'summary': 'Withholding instances on the topic of solar energy for testing to simulate distribution shifts',\n  'date': '2023-09-15',\n  'title': 'Bridging Topic, Domain, and Language Shifts: An Evaluation of\\n  Comprehensive Out-of-Distribution Scenarios',\n  'id': '2309.08316'},\n {'from': 'OOD generalization analysis',\n  'to': 'language model',\n  'name': 'focuses on',\n  'research': 'contribution',\n  'summary': 'The study comprehensively analyzes OOD generalization, unlike prior studies focusing on specific shifts and metrics in isolation',\n  'date': '2023-09-15',\n  'title': 'Bridging Topic, Domain, and Language Shifts: An Evaluation of\\n  Comprehensive Out-of-Distribution Scenarios',\n  'id': '2309.08316'},\n {'from': 'generalization flaws',\n  'to': 'language model',\n  'name': 'defined to pinpoint',\n  'research': 'method',\n  'summary': 'Three metrics are defined to pinpoint generalization flaws in LMs',\n  'date': '2023-09-15',\n  'title': 'Bridging Topic, Domain, and Language Shifts: An Evaluation of\\n  Comprehensive Out-of-Distribution Scenarios',\n  'id': '2309.08316'},\n {'from': 'prompt-based fine-tuning',\n  'to': 'semantic differences in train and test data',\n  'name': 'performs better',\n  'research': 'finding',\n  'summary': 'Prompt-based fine-tuning shows superior performance when train and test splits primarily differ semantically',\n  'date': '2023-09-15',\n  'title': 'Bridging Topic, Domain, and Language Shifts: An Evaluation of\\n  Comprehensive Out-of-Distribution Scenarios',\n  'id': '2309.08316'},\n {'from': 'in-context learning',\n  'to': 'discrepancies in label distribution',\n  'name': 'more effective than',\n  'research': 'finding',\n  'summary': 'In-context learning is more effective than prompt-based or vanilla fine-tuning for tasks with heavy discrepancies in label distribution between training and testing data',\n  'date': '2023-09-15',\n  'title': 'Bridging Topic, Domain, and Language Shifts: An Evaluation of\\n  Comprehensive Out-of-Distribution Scenarios',\n  'id': '2309.08316'},\n {'from': 'gradient-based learning bias',\n  'to': 'language model',\n  'name': 'is a drawback of',\n  'research': 'finding',\n  'summary': 'Gradient-based learning biases language models, which is a crucial drawback revealed by the study',\n  'date': '2023-09-15',\n  'title': 'Bridging Topic, Domain, and Language Shifts: An Evaluation of\\n  Comprehensive Out-of-Distribution Scenarios',\n  'id': '2309.08316'},\n {'from': 'language model',\n  'to': 'understanding and generating language',\n  'name': 'demonstrated abilities in',\n  'research': 'context',\n  'summary': 'Language models have demonstrated remarkable abilities in understanding and generating both natural and formal language',\n  'date': '2023-09-15',\n  'title': 'Data Distribution Bottlenecks in Grounding Language Models to Knowledge\\n  Bases',\n  'id': '2309.08345'},\n {'from': 'language model with knowledge bases',\n  'to': 'real-world environments',\n  'name': 'is underdeveloped',\n  'research': 'context',\n  'summary': 'The integration of language models with real-world environments such as large-scale knowledge bases remains an underdeveloped area',\n  'date': '2023-09-15',\n  'title': 'Data Distribution Bottlenecks in Grounding Language Models to Knowledge\\n  Bases',\n  'id': '2309.08345'},\n {'from': 'semantic parsing',\n  'to': 'language model with knowledge bases',\n  'name': 'affected by',\n  'research': 'context',\n  'summary': 'Applications such as semantic parsing are affected by the underdeveloped integration of LMs with real-world environments',\n  'date': '2023-09-15',\n  'title': 'Data Distribution Bottlenecks in Grounding Language Models to Knowledge\\n  Bases',\n  'id': '2309.08345'},\n {'from': 'hallucinated information',\n  'to': 'language model with knowledge bases',\n  'name': 'result of',\n  'research': 'context',\n  'summary': \"Underdeveloped integration of LMs with KBS may result in indulging in 'hallucinated' information\",\n  'date': '2023-09-15',\n  'title': 'Data Distribution Bottlenecks in Grounding Language Models to Knowledge\\n  Bases',\n  'id': '2309.08345'},\n {'from': 'knowledge base question answering (KBQA)',\n  'to': 'language model',\n  'name': 'poses robustness challenges for',\n  'research': 'contribution',\n  'summary': 'The paper investigates the robustness challenges that LMs encounter when tasked with KBQA',\n  'date': '2023-09-15',\n  'title': 'Data Distribution Bottlenecks in Grounding Language Models to Knowledge\\n  Bases',\n  'id': '2309.08345'},\n {'from': 'inconsistent data distribution',\n  'to': 'knowledge base question answering (KBQA)',\n  'name': 'affects',\n  'research': 'context',\n  'summary': 'Scenarios with inconsistent data distribution between training and inference affect LM performance in KBQA',\n  'date': '2023-09-15',\n  'title': 'Data Distribution Bottlenecks in Grounding Language Models to Knowledge\\n  Bases',\n  'id': '2309.08345'},\n {'from': 'language model',\n  'to': 'robustness challenges',\n  'name': 'exhibits poor performance',\n  'research': 'finding',\n  'summary': 'Both small and large language models exhibit poor performance in various dimensions, even with data augmentation techniques',\n  'date': '2023-09-15',\n  'title': 'Data Distribution Bottlenecks in Grounding Language Models to Knowledge\\n  Bases',\n  'id': '2309.08345'},\n {'from': 'language model',\n  'to': 'data distribution',\n  'name': 'has limited practicality due to',\n  'research': 'finding',\n  'summary': 'The robustness of LMs in dealing with complex environments is fragile and of limited practicality because of the data distribution issue',\n  'date': '2023-09-15',\n  'title': 'Data Distribution Bottlenecks in Grounding Language Models to Knowledge\\n  Bases',\n  'id': '2309.08345'},\n {'from': 'data collection and LM learning paradigms',\n  'to': 'language model',\n  'name': 'needed for',\n  'research': 'reflection',\n  'summary': 'Future research on data collection and LM learning paradigms is called for to address robustness and practicality issues',\n  'date': '2023-09-15',\n  'title': 'Data Distribution Bottlenecks in Grounding Language Models to Knowledge\\n  Bases',\n  'id': '2309.08345'},\n {'from': 'semi-structured explanation',\n  'to': 'reasoning process',\n  'name': 'depicts',\n  'research': 'context',\n  'summary': 'Semi-structured explanation depicts the implicit process of a reasoner with an explicit representation',\n  'date': '2023-09-15',\n  'title': 'Reward Engineering for Generating Semi-structured Explanation',\n  'id': '2309.08347'},\n {'from': 'semi-structured explanation',\n  'to': 'generating an answer',\n  'name': 'highlights',\n  'research': 'context',\n  'summary': \"This explanation highlights how information from a specific query is supplemented with information produced from a reasoner's internal weights\",\n  'date': '2023-09-15',\n  'title': 'Reward Engineering for Generating Semi-structured Explanation',\n  'id': '2309.08347'},\n {'from': 'language model',\n  'to': 'generative capabilities',\n  'name': 'improved in',\n  'research': 'context',\n  'summary': 'Recent improvements in generative capabilities of language models',\n  'date': '2023-09-15',\n  'title': 'Reward Engineering for Generating Semi-structured Explanation',\n  'id': '2309.08347'},\n {'from': 'producing structured explanations',\n  'to': 'true reasoning capabilities',\n  'name': 'verifies',\n  'research': 'context',\n  'summary': \"Producing structured explanations to verify model's true reasoning capabilities remains a challenge\",\n  'date': '2023-09-15',\n  'title': 'Reward Engineering for Generating Semi-structured Explanation',\n  'id': '2309.08347'},\n {'from': 'not-so-large language model',\n  'to': 'structured explanation',\n  'name': 'expected to couple',\n  'research': 'context',\n  'summary': 'Not-so-large LMs are expected to couple a sequential answer with a structured explanation embodying correct presentation and reasoning',\n  'date': '2023-09-15',\n  'title': 'Reward Engineering for Generating Semi-structured Explanation',\n  'id': '2309.08347'},\n {'from': 'supervised fine-tuning',\n  'to': 'producing structured explanations',\n  'name': 'has limitations in',\n  'research': 'claim',\n  'summary': 'The limitations of Supervised Fine-Tuning (SFT) in tackling the challenge of producing structured explanations are underscored',\n  'date': '2023-09-15',\n  'title': 'Reward Engineering for Generating Semi-structured Explanation',\n  'id': '2309.08347'},\n {'from': 'reward engineering in reinforcement learning',\n  'to': 'producing structured explanations',\n  'name': 'introduced to address',\n  'research': 'contribution',\n  'summary': 'A carefully crafted reward engineering method in Reinforcement Learning (RL) is introduced to better address the problem of producing structured explanations',\n  'date': '2023-09-15',\n  'title': 'Reward Engineering for Generating Semi-structured Explanation',\n  'id': '2309.08347'},\n {'from': 'reinforcement learning',\n  'to': 'reward aggregation methods',\n  'name': 'investigates',\n  'research': 'method',\n  'summary': 'Multiple reward aggregation methods in RL are investigated',\n  'date': '2023-09-15',\n  'title': 'Reward Engineering for Generating Semi-structured Explanation',\n  'id': '2309.08347'},\n {'from': 'reinforcement learning',\n  'to': 'future research',\n  'name': 'has potential for',\n  'research': 'reflection',\n  'summary': 'A detailed discussion sheds light on the promising potential of RL for future research',\n  'date': '2023-09-15',\n  'title': 'Reward Engineering for Generating Semi-structured Explanation',\n  'id': '2309.08347'},\n {'from': 'proposed reward',\n  'to': 'semi-structured explanation generation benchmarks',\n  'name': 'achieves results on',\n  'research': 'result',\n  'summary': 'The proposed reward achieves new state-of-the-art results on two semi-structured explanation generation benchmarks',\n  'date': '2023-09-15',\n  'title': 'Reward Engineering for Generating Semi-structured Explanation',\n  'id': '2309.08347'},\n {'from': 'semi-structured explanation generation benchmarks',\n  'to': 'ExplaGraph and COPA-SSE',\n  'name': 'includes',\n  'research': 'context',\n  'summary': 'Benchmarks include ExplaGraph and COPA-SSE',\n  'date': '2023-09-15',\n  'title': 'Reward Engineering for Generating Semi-structured Explanation',\n  'id': '2309.08347'},\n {'from': 'self-supervised pre-training',\n  'to': 'probability prediction',\n  'name': 'involves',\n  'research': 'context',\n  'summary': 'Self-supervised pre-training of language models usually involves predicting probability distributions over token vocabularies',\n  'date': '2023-09-15',\n  'title': 'Headless Language Models: Learning without Predicting with Contrastive\\n  Weight Tying',\n  'id': '2309.08351'},\n {'from': 'current',\n  'to': 'Contrastive Weight Tying (CWT)',\n  'name': 'proposes',\n  'research': 'contribution',\n  'summary': 'This study proposes a method focusing on reconstructing input embeddings in a contrastive fashion via Contrastive Weight Tying (CWT)',\n  'date': '2023-09-15',\n  'title': 'Headless Language Models: Learning without Predicting with Contrastive\\n  Weight Tying',\n  'id': '2309.08351'},\n {'from': 'Contrastive Weight Tying (CWT)',\n  'to': 'headless language model',\n  'name': 'applied to',\n  'research': 'application',\n  'summary': 'CWT is applied to pretrain headless language models in monolingual and multilingual contexts',\n  'date': '2023-09-15',\n  'title': 'Headless Language Models: Learning without Predicting with Contrastive\\n  Weight Tying',\n  'id': '2309.08351'},\n {'from': 'Contrastive Weight Tying (CWT)',\n  'to': 'computational requirements',\n  'name': 'reduces',\n  'research': 'advantage',\n  'summary': 'CWT substantially reduces training computational requirements by up to 20 times',\n  'date': '2023-09-15',\n  'title': 'Headless Language Models: Learning without Predicting with Contrastive\\n  Weight Tying',\n  'id': '2309.08351'},\n {'from': 'Contrastive Weight Tying (CWT)',\n  'to': 'downstream performance',\n  'name': 'enhances',\n  'research': 'advantage',\n  'summary': 'CWT enhances downstream performance and data efficiency',\n  'date': '2023-09-15',\n  'title': 'Headless Language Models: Learning without Predicting with Contrastive\\n  Weight Tying',\n  'id': '2309.08351'},\n {'from': 'GLUE score',\n  'to': 'Contrastive Weight Tying (CWT)',\n  'name': 'increase observed',\n  'research': 'result',\n  'summary': 'A significant +1.6 GLUE score increase is observed with CWT compared to classical LMs',\n  'date': '2023-09-15',\n  'title': 'Headless Language Models: Learning without Predicting with Contrastive\\n  Weight Tying',\n  'id': '2309.08351'},\n {'from': 'LAMBADA accuracy',\n  'to': 'Contrastive Weight Tying (CWT)',\n  'name': 'improvement observed',\n  'research': 'result',\n  'summary': 'A notable +2.7 LAMBADA accuracy improvement is observed with CWT compared to classical LMs',\n  'date': '2023-09-15',\n  'title': 'Headless Language Models: Learning without Predicting with Contrastive\\n  Weight Tying',\n  'id': '2309.08351'},\n {'from': 'headless language model',\n  'to': 'monolingual and multilingual',\n  'name': 'context of use',\n  'research': 'context',\n  'summary': 'Headless language models are pretrained using CWT in both monolingual and multilingual contexts',\n  'date': '2023-09-15',\n  'title': 'Headless Language Models: Learning without Predicting with Contrastive\\n  Weight Tying',\n  'id': '2309.08351'},\n {'from': 'classical language model',\n  'to': 'headless language model',\n  'name': 'compared with',\n  'research': 'comparison',\n  'summary': 'Classical LMs are compared with headless language models pretrained with CWT in terms of GLUE score and LAMBADA accuracy',\n  'date': '2023-09-15',\n  'title': 'Headless Language Models: Learning without Predicting with Contrastive\\n  Weight Tying',\n  'id': '2309.08351'},\n {'from': 'knowledge incorporation into dialogue generation',\n  'to': 'correctness of response',\n  'name': 'improves',\n  'research': 'context',\n  'summary': 'Incorporating external knowledge into dialogue generation is crucial for improving the correctness of response',\n  'date': '2023-09-15',\n  'title': 'Unleashing Potential of Evidence in Knowledge-Intensive Dialogue\\n  Generation',\n  'id': '2309.08380'},\n {'from': 'evidence fragments',\n  'to': 'supporting factual dialogue replies',\n  'name': 'serve as',\n  'research': 'context',\n  'summary': 'Evidence fragments serve as knowledgeable snippets supporting factual dialogue replies',\n  'date': '2023-09-15',\n  'title': 'Unleashing Potential of Evidence in Knowledge-Intensive Dialogue\\n  Generation',\n  'id': '2309.08380'},\n {'from': 'irrelevant content introduction',\n  'to': 'hallucinated responses',\n  'name': 'leads to',\n  'research': 'problem',\n  'summary': 'Introducing irrelevant content often adversely impacts reply quality and easily leads to hallucinated responses',\n  'date': '2023-09-15',\n  'title': 'Unleashing Potential of Evidence in Knowledge-Intensive Dialogue\\n  Generation',\n  'id': '2309.08380'},\n {'from': 'evidence retrieval and integration in dialogue systems',\n  'to': 'inaccurate evidence location',\n  'name': 'falls short in',\n  'research': 'problem',\n  'summary': 'Prior work falls short of fully leveraging existing evidence due to inaccurate location of useful fragments and overlooking hidden evidence labels within the dataset',\n  'date': '2023-09-15',\n  'title': 'Unleashing Potential of Evidence in Knowledge-Intensive Dialogue\\n  Generation',\n  'id': '2309.08380'},\n {'from': 'U-EIDG',\n  'to': 'knowledge incorporation into dialogue generation',\n  'name': 'proposed for',\n  'research': 'contribution',\n  'summary': 'A framework is proposed to effectively incorporate evidence in knowledge-intensive dialogue generation',\n  'date': '2023-09-15',\n  'title': 'Unleashing Potential of Evidence in Knowledge-Intensive Dialogue\\n  Generation',\n  'id': '2309.08380'},\n {'from': 'automatic evidence generation',\n  'to': 'large language model',\n  'name': 'utilizes',\n  'research': 'method',\n  'summary': 'An automatic evidence generation framework harnesses the power of LLMs to mine reliable evidence veracity labels from unlabeled data',\n  'date': '2023-09-15',\n  'title': 'Unleashing Potential of Evidence in Knowledge-Intensive Dialogue\\n  Generation',\n  'id': '2309.08380'},\n {'from': 'reliable evidence indicator',\n  'to': 'identifying relevant evidence',\n  'name': 'trained to',\n  'research': 'method',\n  'summary': 'A reliable evidence indicator is trained to effectively identify relevant evidence from retrieved passages',\n  'date': '2023-09-15',\n  'title': 'Unleashing Potential of Evidence in Knowledge-Intensive Dialogue\\n  Generation',\n  'id': '2309.08380'},\n {'from': 'evidence-augmented generator',\n  'to': 'evidence-focused attention mechanism',\n  'name': 'proposed with',\n  'research': 'contribution',\n  'summary': 'An evidence-augmented generator with an evidence-focused attention mechanism is proposed',\n  'date': '2023-09-15',\n  'title': 'Unleashing Potential of Evidence in Knowledge-Intensive Dialogue\\n  Generation',\n  'id': '2309.08380'},\n {'from': 'MultiDoc2Dial',\n  'to': 'evidential label augmentation',\n  'name': 'used to demonstrate',\n  'research': 'method',\n  'summary': 'Experimental results on MultiDoc2Dial demonstrate the efficacy of evidential label augmentation and refined attention mechanisms in improving model performance',\n  'date': '2023-09-15',\n  'title': 'Unleashing Potential of Evidence in Knowledge-Intensive Dialogue\\n  Generation',\n  'id': '2309.08380'},\n {'from': 'coherence and factual consistency',\n  'to': 'U-EIDG framework',\n  'name': 'improved by',\n  'research': 'finding',\n  'summary': 'The proposed method outperforms other baselines regarding coherence and factual consistency',\n  'date': '2023-09-15',\n  'title': 'Unleashing Potential of Evidence in Knowledge-Intensive Dialogue\\n  Generation',\n  'id': '2309.08380'},\n {'from': 'language model evaluation',\n  'to': 'language understanding and generation',\n  'name': 'is essential in',\n  'research': 'context',\n  'summary': 'The evaluation of large language models is an essential task in the field of language understanding and generation',\n  'date': '2023-09-15',\n  'title': 'Advancing the Evaluation of Traditional Chinese Language Models: Towards\\n  a Comprehensive Benchmark Suite',\n  'id': '2309.08448'},\n {'from': 'large language model',\n  'to': 'benchmarks',\n  'name': 'requires',\n  'research': 'context',\n  'summary': 'Effective benchmarks are required to assess the performance of advancing language models',\n  'date': '2023-09-15',\n  'title': 'Advancing the Evaluation of Traditional Chinese Language Models: Towards\\n  a Comprehensive Benchmark Suite',\n  'id': '2309.08448'},\n {'from': 'Traditional Chinese',\n  'to': 'comprehensive and diverse benchmarks',\n  'name': 'lacks',\n  'research': 'context',\n  'summary': 'There is a scarcity of comprehensive and diverse benchmarks to evaluate language models in Traditional Chinese',\n  'date': '2023-09-15',\n  'title': 'Advancing the Evaluation of Traditional Chinese Language Models: Towards\\n  a Comprehensive Benchmark Suite',\n  'id': '2309.08448'},\n {'from': 'DRCD',\n  'to': 'Traditional Chinese',\n  'name': 'is a benchmark for',\n  'research': 'context',\n  'summary': 'DRCD is an existing benchmark for Traditional Chinese language models',\n  'date': '2023-09-15',\n  'title': 'Advancing the Evaluation of Traditional Chinese Language Models: Towards\\n  a Comprehensive Benchmark Suite',\n  'id': '2309.08448'},\n {'from': 'TTQA',\n  'to': 'Traditional Chinese',\n  'name': 'is a benchmark for',\n  'research': 'context',\n  'summary': 'TTQA is an existing benchmark for Traditional Chinese language models',\n  'date': '2023-09-15',\n  'title': 'Advancing the Evaluation of Traditional Chinese Language Models: Towards\\n  a Comprehensive Benchmark Suite',\n  'id': '2309.08448'},\n {'from': 'CMQA',\n  'to': 'Traditional Chinese',\n  'name': 'is a benchmark for',\n  'research': 'context',\n  'summary': 'CMQA is an existing benchmark for Traditional Chinese language models',\n  'date': '2023-09-15',\n  'title': 'Advancing the Evaluation of Traditional Chinese Language Models: Towards\\n  a Comprehensive Benchmark Suite',\n  'id': '2309.08448'},\n {'from': 'FGC dataset',\n  'to': 'Traditional Chinese',\n  'name': 'is a benchmark for',\n  'research': 'context',\n  'summary': 'FGC dataset is an existing benchmark for Traditional Chinese language models',\n  'date': '2023-09-15',\n  'title': 'Advancing the Evaluation of Traditional Chinese Language Models: Towards\\n  a Comprehensive Benchmark Suite',\n  'id': '2309.08448'},\n {'from': 'novel set of benchmarks',\n  'to': 'Traditional Chinese',\n  'name': 'proposed for',\n  'research': 'contribution',\n  'summary': 'A novel set of benchmarks is proposed to evaluate language models in Traditional Chinese, leveraging existing English datasets',\n  'date': '2023-09-15',\n  'title': 'Advancing the Evaluation of Traditional Chinese Language Models: Towards\\n  a Comprehensive Benchmark Suite',\n  'id': '2309.08448'},\n {'from': 'GPT-3.5',\n  'to': 'novel set of benchmarks',\n  'name': 'evaluated using',\n  'research': 'method',\n  'summary': 'GPT-3.5 is evaluated using the proposed benchmarks for Traditional Chinese',\n  'date': '2023-09-15',\n  'title': 'Advancing the Evaluation of Traditional Chinese Language Models: Towards\\n  a Comprehensive Benchmark Suite',\n  'id': '2309.08448'},\n {'from': 'Taiwan-LLaMA-v1.0',\n  'to': 'novel set of benchmarks',\n  'name': 'evaluated using',\n  'research': 'method',\n  'summary': 'Taiwan-LLaMA-v1.0 is evaluated using the proposed benchmarks for Traditional Chinese',\n  'date': '2023-09-15',\n  'title': 'Advancing the Evaluation of Traditional Chinese Language Models: Towards\\n  a Comprehensive Benchmark Suite',\n  'id': '2309.08448'},\n {'from': 'Model 7-C',\n  'to': 'novel set of benchmarks',\n  'name': 'evaluated using',\n  'research': 'method',\n  'summary': 'Model 7-C is evaluated using the proposed benchmarks and achieves performance comparable to GPT-3.5 in some capabilities',\n  'date': '2023-09-15',\n  'title': 'Advancing the Evaluation of Traditional Chinese Language Models: Towards\\n  a Comprehensive Benchmark Suite',\n  'id': '2309.08448'},\n {'from': 'Model 7-C',\n  'to': 'research community',\n  'name': 'open-sourced',\n  'research': 'contribution',\n  'summary': 'Model 7-C is open-sourced to advance the evaluation of language models in Traditional Chinese and stimulate further research',\n  'date': '2023-09-15',\n  'title': 'Advancing the Evaluation of Traditional Chinese Language Models: Towards\\n  a Comprehensive Benchmark Suite',\n  'id': '2309.08448'},\n {'from': 'large language model',\n  'to': 'knowledge engineering',\n  'name': 'used for',\n  'research': 'application',\n  'summary': 'LLMs are used for knowledge engineering tasks in the ISWC 2023 LM-KBC Challenge',\n  'date': '2023-09-15',\n  'title': 'Using Large Language Models for Knowledge Engineering (LLMKE): A Case\\n  Study on Wikidata',\n  'id': '2309.08491'},\n {'from': 'ISWC 2023 LM-KBC Challenge',\n  'to': 'knowledge engineering',\n  'name': 'involves',\n  'research': 'context',\n  'summary': 'The challenge involves using LLMs to produce objects from subject and relation pairs and link them to Wikidata QIDs',\n  'date': '2023-09-15',\n  'title': 'Using Large Language Models for Knowledge Engineering (LLMKE): A Case\\n  Study on Wikidata',\n  'id': '2309.08491'},\n {'from': 'Wikidata',\n  'to': 'subject and relation pairs',\n  'name': 'provides',\n  'research': 'context',\n  'summary': 'Subject and relation pairs are sourced from Wikidata for the knowledge engineering task',\n  'date': '2023-09-15',\n  'title': 'Using Large Language Models for Knowledge Engineering (LLMKE): A Case\\n  Study on Wikidata',\n  'id': '2309.08491'},\n {'from': 'LLMKE',\n  'to': 'knowledge probing',\n  'name': 'combines',\n  'research': 'method',\n  'summary': 'LLMKE combines knowledge probing and Wikidata entity mapping',\n  'date': '2023-09-15',\n  'title': 'Using Large Language Models for Knowledge Engineering (LLMKE): A Case\\n  Study on Wikidata',\n  'id': '2309.08491'},\n {'from': 'LLMKE',\n  'to': 'Wikidata entity mapping',\n  'name': 'combines',\n  'research': 'method',\n  'summary': 'LLMKE combines knowledge probing and Wikidata entity mapping',\n  'date': '2023-09-15',\n  'title': 'Using Large Language Models for Knowledge Engineering (LLMKE): A Case\\n  Study on Wikidata',\n  'id': '2309.08491'},\n {'from': 'macro-averaged F1-score',\n  'to': 'LLMKE',\n  'name': 'achieved by',\n  'research': 'result',\n  'summary': 'The method achieved a macro-averaged F1-score of 0.701 across the properties',\n  'date': '2023-09-15',\n  'title': 'Using Large Language Models for Knowledge Engineering (LLMKE): A Case\\n  Study on Wikidata',\n  'id': '2309.08491'},\n {'from': 'large language model',\n  'to': 'various domains',\n  'name': 'knowledge varies by',\n  'research': 'finding',\n  'summary': 'The knowledge of LLMs varies significantly depending on the domain',\n  'date': '2023-09-15',\n  'title': 'Using Large Language Models for Knowledge Engineering (LLMKE): A Case\\n  Study on Wikidata',\n  'id': '2309.08491'},\n {'from': 'large language model',\n  'to': 'knowledge base completion and correction',\n  'name': 'requires further experimentation for',\n  'research': 'reflection',\n  'summary': 'Further experimentation is required to determine the circumstances under which LLMs can be used for automatic knowledge base completion and correction',\n  'date': '2023-09-15',\n  'title': 'Using Large Language Models for Knowledge Engineering (LLMKE): A Case\\n  Study on Wikidata',\n  'id': '2309.08491'},\n {'from': 'Wikidata',\n  'to': 'knowledge base completion and correction',\n  'name': 'subject of',\n  'research': 'context',\n  'summary': 'Wikidata is the knowledge base for which completion and correction tasks are considered using LLMs',\n  'date': '2023-09-15',\n  'title': 'Using Large Language Models for Knowledge Engineering (LLMKE): A Case\\n  Study on Wikidata',\n  'id': '2309.08491'},\n {'from': 'large language model',\n  'to': 'collaborative knowledge engineering',\n  'name': 'contributes to',\n  'research': 'insight',\n  'summary': 'LLMs have a promising contribution in collaborative knowledge engineering',\n  'date': '2023-09-15',\n  'title': 'Using Large Language Models for Knowledge Engineering (LLMKE): A Case\\n  Study on Wikidata',\n  'id': '2309.08491'},\n {'from': 'LLMKE',\n  'to': 'Track 2 of ISWC 2023 LM-KBC Challenge',\n  'name': 'winner of',\n  'research': 'achievement',\n  'summary': 'LLMKE won Track 2 of the ISWC 2023 LM-KBC Challenge',\n  'date': '2023-09-15',\n  'title': 'Using Large Language Models for Knowledge Engineering (LLMKE): A Case\\n  Study on Wikidata',\n  'id': '2309.08491'},\n {'from': 'LLMKE',\n  'to': 'GitHub repository',\n  'name': 'implementation available at',\n  'research': 'resource',\n  'summary': 'The implementation of LLMKE is available on GitHub',\n  'date': '2023-09-15',\n  'title': 'Using Large Language Models for Knowledge Engineering (LLMKE): A Case\\n  Study on Wikidata',\n  'id': '2309.08491'},\n {'from': 'image-to-speech captioning',\n  'to': 'Im2Sp model',\n  'name': 'improved by',\n  'research': 'contribution',\n  'summary': 'Proposed methods to build a powerful and efficient image-to-speech captioning (Im2Sp) model',\n  'date': '2023-09-15',\n  'title': 'Towards Practical and Efficient Image-to-Speech Captioning with\\n  Vision-Language Pre-training and Multi-modal Tokens',\n  'id': '2309.08531'},\n {'from': 'Im2Sp model',\n  'to': 'vision-language model',\n  'name': 'imports knowledge from',\n  'research': 'method',\n  'summary': 'Im2Sp imports knowledge from a large-scale pre-trained vision-language model',\n  'date': '2023-09-15',\n  'title': 'Towards Practical and Efficient Image-to-Speech Captioning with\\n  Vision-Language Pre-training and Multi-modal Tokens',\n  'id': '2309.08531'},\n {'from': 'Im2Sp model',\n  'to': 'discretized speech units',\n  'name': 'outputs',\n  'research': 'method',\n  'summary': 'The output of the proposed Im2Sp is discretized speech units from a self-supervised speech model',\n  'date': '2023-09-15',\n  'title': 'Towards Practical and Efficient Image-to-Speech Captioning with\\n  Vision-Language Pre-training and Multi-modal Tokens',\n  'id': '2309.08531'},\n {'from': 'discretized speech units',\n  'to': 'linguistic information',\n  'name': 'contains',\n  'research': 'context',\n  'summary': 'Speech units mainly contain linguistic information while suppressing other characteristics of speech',\n  'date': '2023-09-15',\n  'title': 'Towards Practical and Efficient Image-to-Speech Captioning with\\n  Vision-Language Pre-training and Multi-modal Tokens',\n  'id': '2309.08531'},\n {'from': 'vision-language pre-training',\n  'to': 'Im2Sp model',\n  'name': 'used for',\n  'research': 'method',\n  'summary': 'Vision-language pre-training strategy is used to set new state-of-the-art Im2Sp performances',\n  'date': '2023-09-15',\n  'title': 'Towards Practical and Efficient Image-to-Speech Captioning with\\n  Vision-Language Pre-training and Multi-modal Tokens',\n  'id': '2309.08531'},\n {'from': 'Im2Sp model',\n  'to': 'COCO and Flickr8k',\n  'name': 'sets record on',\n  'research': 'result',\n  'summary': 'New state-of-the-art Im2Sp performances on COCO and Flickr8k databases',\n  'date': '2023-09-15',\n  'title': 'Towards Practical and Efficient Image-to-Speech Captioning with\\n  Vision-Language Pre-training and Multi-modal Tokens',\n  'id': '2309.08531'},\n {'from': 'Im2Sp model',\n  'to': 'image units',\n  'name': 'improves',\n  'research': 'contribution',\n  'summary': 'Im2Sp model efficiency improved by converting original images into image units',\n  'date': '2023-09-15',\n  'title': 'Towards Practical and Efficient Image-to-Speech Captioning with\\n  Vision-Language Pre-training and Multi-modal Tokens',\n  'id': '2309.08531'},\n {'from': 'image units',\n  'to': 'vector quantization',\n  'name': 'derived from',\n  'research': 'method',\n  'summary': 'Image units are derived through vector quantization of the raw image',\n  'date': '2023-09-15',\n  'title': 'Towards Practical and Efficient Image-to-Speech Captioning with\\n  Vision-Language Pre-training and Multi-modal Tokens',\n  'id': '2309.08531'},\n {'from': 'image units',\n  'to': '0.8% data storage',\n  'name': 'reduces data storage to',\n  'research': 'result',\n  'summary': 'Image units reduce the required data storage for saving image data to just 0.8% compared to original image data',\n  'date': '2023-09-15',\n  'title': 'Towards Practical and Efficient Image-to-Speech Captioning with\\n  Vision-Language Pre-training and Multi-modal Tokens',\n  'id': '2309.08531'},\n {'from': 'demo page',\n  'to': 'Im2Sp model',\n  'name': 'demonstrates',\n  'research': 'resource',\n  'summary': 'Demo page available for the image-to-speech captioning model',\n  'date': '2023-09-15',\n  'title': 'Towards Practical and Efficient Image-to-Speech Captioning with\\n  Vision-Language Pre-training and Multi-modal Tokens',\n  'id': '2309.08531'},\n {'from': 'large language model',\n  'to': 'prompt crafting',\n  'name': 'rely on',\n  'research': 'context',\n  'summary': 'LLMs excel in various tasks but rely on carefully crafted prompts',\n  'date': '2023-09-15',\n  'title': 'Connecting Large Language Models with Evolutionary Algorithms Yields\\n  Powerful Prompt Optimizers',\n  'id': '2309.08532'},\n {'from': 'prompt crafting',\n  'to': 'human effort',\n  'name': 'requires',\n  'research': 'context',\n  'summary': 'Carefully crafted prompts often demand substantial human effort',\n  'date': '2023-09-15',\n  'title': 'Connecting Large Language Models with Evolutionary Algorithms Yields\\n  Powerful Prompt Optimizers',\n  'id': '2309.08532'},\n {'from': 'EvoPrompt',\n  'to': 'discrete prompt optimization',\n  'name': 'proposed for',\n  'research': 'contribution',\n  'summary': 'EvoPrompt is a novel framework proposed for discrete prompt optimization',\n  'date': '2023-09-15',\n  'title': 'Connecting Large Language Models with Evolutionary Algorithms Yields\\n  Powerful Prompt Optimizers',\n  'id': '2309.08532'},\n {'from': 'evolutionary algorithms',\n  'to': 'EvoPrompt',\n  'name': 'inspire',\n  'research': 'method',\n  'summary': 'EvoPrompt borrows the idea of evolutionary algorithms due to their good performance and fast convergence',\n  'date': '2023-09-15',\n  'title': 'Connecting Large Language Models with Evolutionary Algorithms Yields\\n  Powerful Prompt Optimizers',\n  'id': '2309.08532'},\n {'from': 'large language model',\n  'to': 'evolutionary algorithms',\n  'name': 'connected with',\n  'research': 'method',\n  'summary': 'LLMs are connected with evolutionary algorithms to work on discrete prompts',\n  'date': '2023-09-15',\n  'title': 'Connecting Large Language Models with Evolutionary Algorithms Yields\\n  Powerful Prompt Optimizers',\n  'id': '2309.08532'},\n {'from': 'discrete prompt optimization',\n  'to': 'large language model',\n  'name': 'leverages',\n  'research': 'method',\n  'summary': 'Discrete prompt optimization leverages the language processing capabilities of LLMs and the optimization performance of EAs',\n  'date': '2023-09-15',\n  'title': 'Connecting Large Language Models with Evolutionary Algorithms Yields\\n  Powerful Prompt Optimizers',\n  'id': '2309.08532'},\n {'from': 'EvoPrompt',\n  'to': 'gradients or parameters',\n  'name': 'operates without',\n  'research': 'method',\n  'summary': 'EvoPrompt operates without any gradients or parameters',\n  'date': '2023-09-15',\n  'title': 'Connecting Large Language Models with Evolutionary Algorithms Yields\\n  Powerful Prompt Optimizers',\n  'id': '2309.08532'},\n {'from': 'EvoPrompt',\n  'to': 'GPT-3.5',\n  'name': 'optimizes',\n  'research': 'contribution',\n  'summary': 'EvoPrompt optimizes prompts for both closed- and open-source LLMs including GPT-3.5 and Alpaca',\n  'date': '2023-09-15',\n  'title': 'Connecting Large Language Models with Evolutionary Algorithms Yields\\n  Powerful Prompt Optimizers',\n  'id': '2309.08532'},\n {'from': 'EvoPrompt',\n  'to': 'Alpaca',\n  'name': 'optimizes',\n  'research': 'contribution',\n  'summary': 'EvoPrompt optimizes prompts for both closed- and open-source LLMs including GPT-3.5 and Alpaca',\n  'date': '2023-09-15',\n  'title': 'Connecting Large Language Models with Evolutionary Algorithms Yields\\n  Powerful Prompt Optimizers',\n  'id': '2309.08532'},\n {'from': 'EvoPrompt',\n  'to': 'human-engineered prompt generation',\n  'name': 'outperforms',\n  'research': 'finding',\n  'summary': 'EvoPrompt significantly outperforms human-engineered prompts and existing methods for automatic prompt generation',\n  'date': '2023-09-15',\n  'title': 'Connecting Large Language Models with Evolutionary Algorithms Yields\\n  Powerful Prompt Optimizers',\n  'id': '2309.08532'},\n {'from': 'LLMs and EAs combination',\n  'to': 'EvoPrompt',\n  'name': 'demonstrated by',\n  'research': 'finding',\n  'summary': 'EvoPrompt demonstrates that connecting LLMs with EAs creates synergies',\n  'date': '2023-09-15',\n  'title': 'Connecting Large Language Models with Evolutionary Algorithms Yields\\n  Powerful Prompt Optimizers',\n  'id': '2309.08532'},\n {'from': 'language model-based expansion',\n  'to': 'information retrieval',\n  'name': 'used for',\n  'research': 'context',\n  'summary': 'Using LMs for query or document expansion can improve generalization in information retrieval',\n  'date': '2023-09-15',\n  'title': 'When do Generative Query and Document Expansions Fail? A Comprehensive\\n  Study Across Methods, Retrievers, and Datasets',\n  'id': '2309.08541'},\n {'from': 'effectiveness of LM-based expansion',\n  'to': 'information retrieval',\n  'name': 'concerns',\n  'research': 'context',\n  'summary': 'It is unknown whether LM-based expansion techniques are universally beneficial or only effective in specific settings',\n  'date': '2023-09-15',\n  'title': 'When do Generative Query and Document Expansions Fail? A Comprehensive\\n  Study Across Methods, Retrievers, and Datasets',\n  'id': '2309.08541'},\n {'from': 'comprehensive analysis of LM-based expansion',\n  'to': 'effectiveness of LM-based expansion',\n  'name': 'conducted to determine',\n  'research': 'method',\n  'summary': 'The first comprehensive analysis of LM-based expansion is conducted to understand its effectiveness',\n  'date': '2023-09-15',\n  'title': 'When do Generative Query and Document Expansions Fail? A Comprehensive\\n  Study Across Methods, Retrievers, and Datasets',\n  'id': '2309.08541'},\n {'from': 'negative correlation between retriever performance and gains from expansion',\n  'to': 'retriever performance',\n  'name': 'found between',\n  'research': 'finding',\n  'summary': 'A strong negative correlation is found between retriever performance and gains from expansion',\n  'date': '2023-09-15',\n  'title': 'When do Generative Query and Document Expansions Fail? A Comprehensive\\n  Study Across Methods, Retrievers, and Datasets',\n  'id': '2309.08541'},\n {'from': 'expansion improves scores for weaker models',\n  'to': 'retriever performance',\n  'name': 'observed in',\n  'research': 'finding',\n  'summary': 'Expansion improves scores for weaker models but generally harms stronger models',\n  'date': '2023-09-15',\n  'title': 'When do Generative Query and Document Expansions Fail? A Comprehensive\\n  Study Across Methods, Retrievers, and Datasets',\n  'id': '2309.08541'},\n {'from': 'qualitative error analysis',\n  'to': 'retriever performance',\n  'name': 'hypothesizes',\n  'research': 'method',\n  'summary': 'Qualitative error analysis hypothesizes that expansions add noise and introduce false positives',\n  'date': '2023-09-15',\n  'title': 'When do Generative Query and Document Expansions Fail? A Comprehensive\\n  Study Across Methods, Retrievers, and Datasets',\n  'id': '2309.08541'},\n {'from': 'use expansions for weaker models',\n  'to': 'weaker models or significant format difference',\n  'name': 'advised when',\n  'research': 'implication',\n  'summary': 'Use expansions for weaker models or when the target dataset significantly differs from the training corpus in format',\n  'date': '2023-09-15',\n  'title': 'When do Generative Query and Document Expansions Fail? A Comprehensive\\n  Study Across Methods, Retrievers, and Datasets',\n  'id': '2309.08541'},\n {'from': 'avoid expansions to keep relevance signal clear',\n  'to': 'stronger models or minor format difference',\n  'name': 'advised unless',\n  'research': 'implication',\n  'summary': 'Avoid expansions to keep the relevance signal clear unless the model is weak or there is a significant format difference',\n  'date': '2023-09-15',\n  'title': 'When do Generative Query and Document Expansions Fail? A Comprehensive\\n  Study Across Methods, Retrievers, and Datasets',\n  'id': '2309.08541'},\n {'from': 'large language model',\n  'to': 'millions of users',\n  'name': 'used by',\n  'research': 'context',\n  'summary': 'LLMs are now used daily by millions of users',\n  'date': '2023-09-15',\n  'title': 'Casteist but Not Racist? Quantifying Disparities in Large Language Model\\n  Bias between India and the West',\n  'id': '2309.08573'},\n {'from': 'large language model',\n  'to': 'societal biases',\n  'name': 'can encode',\n  'research': 'problem',\n  'summary': 'LLMs can encode societal biases, exposing users to representational harms',\n  'date': '2023-09-15',\n  'title': 'Casteist but Not Racist? Quantifying Disparities in Large Language Model\\n  Bias between India and the West',\n  'id': '2309.08573'},\n {'from': 'LLM bias research',\n  'to': 'western-centric',\n  'name': 'adopts frame',\n  'research': 'context',\n  'summary': 'A large body of scholarship on LLM bias predominantly adopts a western-centric frame',\n  'date': '2023-09-15',\n  'title': 'Casteist but Not Racist? Quantifying Disparities in Large Language Model\\n  Bias between India and the West',\n  'id': '2309.08573'},\n {'from': 'LLM bias research',\n  'to': 'Global South',\n  'name': 'attends less to',\n  'research': 'gap',\n  'summary': 'LLM bias research attends comparatively less to bias levels and potential harms in the Global South',\n  'date': '2023-09-15',\n  'title': 'Casteist but Not Racist? Quantifying Disparities in Large Language Model\\n  Bias between India and the West',\n  'id': '2309.08573'},\n {'from': 'Indian-BHED',\n  'to': 'bias quantification',\n  'name': 'developed for',\n  'research': 'method',\n  'summary': 'Indian-BHED, a novel dataset containing stereotypical and anti-stereotypical examples for caste and religion contexts, is developed to quantify stereotypical bias in LLMs',\n  'date': '2023-09-15',\n  'title': 'Casteist but Not Racist? Quantifying Disparities in Large Language Model\\n  Bias between India and the West',\n  'id': '2309.08573'},\n {'from': 'large language model',\n  'to': 'Indian context',\n  'name': 'found biased in',\n  'research': 'finding',\n  'summary': 'LLMs tested are strongly biased towards stereotypes in the Indian context, especially as compared to the Western context',\n  'date': '2023-09-15',\n  'title': 'Casteist but Not Racist? Quantifying Disparities in Large Language Model\\n  Bias between India and the West',\n  'id': '2309.08573'},\n {'from': 'instruction prompting',\n  'to': 'societal biases',\n  'name': 'investigated as intervention for',\n  'research': 'method',\n  'summary': 'Instruction prompting is investigated as a simple intervention to mitigate bias in LLMs',\n  'date': '2023-09-15',\n  'title': 'Casteist but Not Racist? Quantifying Disparities in Large Language Model\\n  Bias between India and the West',\n  'id': '2309.08573'},\n {'from': 'GPT-3.5',\n  'to': 'societal biases',\n  'name': 'shows reduction in',\n  'research': 'finding',\n  'summary': 'Instruction prompting significantly reduces both stereotypical and anti-stereotypical biases in the majority of cases for GPT-3.5',\n  'date': '2023-09-15',\n  'title': 'Casteist but Not Racist? Quantifying Disparities in Large Language Model\\n  Bias between India and the West',\n  'id': '2309.08573'},\n {'from': 'this paper',\n  'to': 'diversity in evaluation',\n  'name': 'highlights need for',\n  'research': 'implication',\n  'summary': 'The findings highlight the need for including more diverse voices when evaluating LLMs',\n  'date': '2023-09-15',\n  'title': 'Casteist but Not Racist? Quantifying Disparities in Large Language Model\\n  Bias between India and the West',\n  'id': '2309.08573'},\n {'from': 'style transfer',\n  'to': 'language model',\n  'name': 'performed by',\n  'research': 'context',\n  'summary': 'State-of-the-art language models excel at the style transfer task',\n  'date': '2023-09-15',\n  'title': 'ICLEF: In-Context Learning with Expert Feedback for Explainable Style\\n  Transfer',\n  'id': '2309.08583'},\n {'from': 'style transfer',\n  'to': 'explainability',\n  'name': 'lacks',\n  'research': 'problem',\n  'summary': 'Current work does not address explainability of style transfer systems',\n  'date': '2023-09-15',\n  'title': 'ICLEF: In-Context Learning with Expert Feedback for Explainable Style\\n  Transfer',\n  'id': '2309.08583'},\n {'from': 'GPT-3.5',\n  'to': 'explanations',\n  'name': 'could generate',\n  'research': 'context',\n  'summary': 'Explanations for style transfer could be generated using models like GPT-3.5',\n  'date': '2023-09-15',\n  'title': 'ICLEF: In-Context Learning with Expert Feedback for Explainable Style\\n  Transfer',\n  'id': '2309.08583'},\n {'from': 'GPT-4',\n  'to': 'explanations',\n  'name': 'could generate',\n  'research': 'context',\n  'summary': 'Explanations for style transfer could be generated using models like GPT-4',\n  'date': '2023-09-15',\n  'title': 'ICLEF: In-Context Learning with Expert Feedback for Explainable Style\\n  Transfer',\n  'id': '2309.08583'},\n {'from': 'large language model',\n  'to': 'smaller distributed transparent model',\n  'name': 'inefficient compared to',\n  'research': 'claim',\n  'summary': 'Using complex systems like large language models for generating explanations is inefficient compared to smaller, widely distributed, and transparent alternatives',\n  'date': '2023-09-15',\n  'title': 'ICLEF: In-Context Learning with Expert Feedback for Explainable Style\\n  Transfer',\n  'id': '2309.08583'},\n {'from': 'model distillation',\n  'to': 'formality style transfer dataset',\n  'name': 'proposed for',\n  'research': 'contribution',\n  'summary': 'A framework is proposed to augment and improve a formality style transfer dataset with explanations via model distillation from ChatGPT',\n  'date': '2023-09-15',\n  'title': 'ICLEF: In-Context Learning with Expert Feedback for Explainable Style\\n  Transfer',\n  'id': '2309.08583'},\n {'from': 'in-context learning from expert feedback (ICLEF)',\n  'to': 'explanations',\n  'name': 'proposed to refine',\n  'research': 'contribution',\n  'summary': 'ICLEF is proposed to refine generated explanations by prompting ChatGPT to act as a critic to its own outputs',\n  'date': '2023-09-15',\n  'title': 'ICLEF: In-Context Learning with Expert Feedback for Explainable Style\\n  Transfer',\n  'id': '2309.08583'},\n {'from': 'E-GYAFC',\n  'to': 'explainable formality style transfer',\n  'name': 'used to show',\n  'research': 'finding',\n  'summary': 'The E-GYAFC dataset is used to show that current openly distributed instruction-tuned models and, in some settings, ChatGPT perform poorly on the explainable formality style transfer task',\n  'date': '2023-09-15',\n  'title': 'ICLEF: In-Context Learning with Expert Feedback for Explainable Style\\n  Transfer',\n  'id': '2309.08583'},\n {'from': 'E-GYAFC',\n  'to': 'explainable formality style transfer',\n  'name': 'fine-tuning on leads to',\n  'research': 'finding',\n  'summary': 'Fine-tuning on the E-GYAFC dataset leads to significant improvements in the explainable formality style transfer task',\n  'date': '2023-09-15',\n  'title': 'ICLEF: In-Context Learning with Expert Feedback for Explainable Style\\n  Transfer',\n  'id': '2309.08583'},\n {'from': 'smaller model',\n  'to': 'expert preferences',\n  'name': 'aligns better with',\n  'research': 'finding',\n  'summary': 'Models much smaller than ChatGPT fine-tuned on E-GYAFC data align better with expert preferences in human evaluation',\n  'date': '2023-09-15',\n  'title': 'ICLEF: In-Context Learning with Expert Feedback for Explainable Style\\n  Transfer',\n  'id': '2309.08583'},\n {'from': 'interpretable authorship verification',\n  'to': 'explainable style transfer',\n  'name': 'potential application of',\n  'research': 'discussion',\n  'summary': 'Interpretable authorship verification is discussed as a potential application of models fine-tuned on the explainable style transfer task',\n  'date': '2023-09-15',\n  'title': 'ICLEF: In-Context Learning with Expert Feedback for Explainable Style\\n  Transfer',\n  'id': '2309.08583'},\n {'from': 'interpretable adversarial attacks on AI-generated text detectors',\n  'to': 'explainable style transfer',\n  'name': 'potential application of',\n  'research': 'discussion',\n  'summary': 'Interpretable adversarial attacks on AI-generated text detectors is discussed as a potential application of models fine-tuned on the explainable style transfer task',\n  'date': '2023-09-15',\n  'title': 'ICLEF: In-Context Learning with Expert Feedback for Explainable Style\\n  Transfer',\n  'id': '2309.08583'},\n {'from': 'hierarchical reasoning',\n  'to': 'effective decision-making',\n  'name': 'crucial for',\n  'research': 'context',\n  'summary': 'Hierarchical reasoning across spatial and temporal scales is crucial for making effective decisions in novel environments with long-horizon goals',\n  'date': '2023-09-15',\n  'title': 'Compositional Foundation Models for Hierarchical Planning',\n  'id': '2309.08587'},\n {'from': 'effective decision-making',\n  'to': 'hierarchical planning',\n  'name': 'involves',\n  'research': 'context',\n  'summary': 'Effective decision-making involves planning abstract subgoal sequences, visually reasoning about plans, and executing actions through visual-motor control',\n  'date': '2023-09-15',\n  'title': 'Compositional Foundation Models for Hierarchical Planning',\n  'id': '2309.08587'},\n {'from': 'compositional foundation models for hierarchical planning (HiP)',\n  'to': 'long-horizon tasks',\n  'name': 'proposed for',\n  'research': 'contribution',\n  'summary': 'HiP is proposed to leverage multiple expert foundation models trained on language, vision, and action data to solve long-horizon tasks',\n  'date': '2023-09-15',\n  'title': 'Compositional Foundation Models for Hierarchical Planning',\n  'id': '2309.08587'},\n {'from': 'large language model',\n  'to': 'symbolic plans',\n  'name': 'used to construct',\n  'research': 'method',\n  'summary': 'A large language model is used to construct symbolic plans grounded in the environment',\n  'date': '2023-09-15',\n  'title': 'Compositional Foundation Models for Hierarchical Planning',\n  'id': '2309.08587'},\n {'from': 'large video diffusion model',\n  'to': 'video plans',\n  'name': 'grounds',\n  'research': 'method',\n  'summary': 'A large video diffusion model grounds generated video plans in the environment',\n  'date': '2023-09-15',\n  'title': 'Compositional Foundation Models for Hierarchical Planning',\n  'id': '2309.08587'},\n {'from': 'inverse dynamics model',\n  'to': 'visual-motor control',\n  'name': 'infers',\n  'research': 'method',\n  'summary': 'An inverse dynamics model infers actions from generated videos for visual-motor control',\n  'date': '2023-09-15',\n  'title': 'Compositional Foundation Models for Hierarchical Planning',\n  'id': '2309.08587'},\n {'from': 'compositional foundation models for hierarchical planning (HiP)',\n  'to': 'model consistency',\n  'name': 'enforces',\n  'research': 'method',\n  'summary': 'HiP enforces consistency between the models via iterative refinement to enable effective reasoning within the hierarchy',\n  'date': '2023-09-15',\n  'title': 'Compositional Foundation Models for Hierarchical Planning',\n  'id': '2309.08587'},\n {'from': 'compositional foundation models for hierarchical planning (HiP)',\n  'to': 'table-top manipulation tasks',\n  'name': 'illustrated by',\n  'research': 'validation',\n  'summary': 'The efficacy and adaptability of HiP is illustrated in three different long-horizon table-top manipulation tasks',\n  'date': '2023-09-15',\n  'title': 'Compositional Foundation Models for Hierarchical Planning',\n  'id': '2309.08587'},\n {'from': 'large language model',\n  'to': 'self-teaching',\n  'name': 'lack',\n  'research': 'context',\n  'summary': 'Large language models currently lack the ability to teach themselves new skills and rely on human-generated training data',\n  'date': '2023-09-15',\n  'title': 'Chain-of-Thought Reasoning is a Policy Improvement Operator',\n  'id': '2309.08589'},\n {'from': 'SECTOR (Self-Education via Chain-of-Thought Reasoning)',\n  'to': 'self-teaching',\n  'name': 'demonstrates',\n  'research': 'contribution',\n  'summary': 'SECTOR is a proof-of-concept that language models can teach themselves new skills using chain-of-thought reasoning',\n  'date': '2023-09-15',\n  'title': 'Chain-of-Thought Reasoning is a Policy Improvement Operator',\n  'id': '2309.08589'},\n {'from': 'self-learning loop',\n  'to': 'solving addition problems',\n  'name': 'involves',\n  'research': 'method',\n  'summary': 'During the self-learning loop, SECTOR asks models to solve addition problems using chain-of-thought reasoning before training the next version of the model to solve those same problems directly',\n  'date': '2023-09-15',\n  'title': 'Chain-of-Thought Reasoning is a Policy Improvement Operator',\n  'id': '2309.08589'},\n {'from': 'large language model',\n  'to': 'self-learning loop',\n  'name': 'improved by',\n  'research': 'finding',\n  'summary': 'The self-learning loop often results in an improved model which can solve even harder problems than the original model',\n  'date': '2023-09-15',\n  'title': 'Chain-of-Thought Reasoning is a Policy Improvement Operator',\n  'id': '2309.08589'},\n {'from': 'large language model',\n  'to': 'solving addition problems',\n  'name': 'learns autonomously',\n  'research': 'finding',\n  'summary': 'Language models trained via SECTOR autonomously learn to add up to the longest-length-digit numbers without access to ground truth examples beyond an initial supervised fine-tuning phase',\n  'date': '2023-09-15',\n  'title': 'Chain-of-Thought Reasoning is a Policy Improvement Operator',\n  'id': '2309.08589'},\n {'from': 'chain-of-thought reasoning as a policy improvement operator',\n  'to': 'Monte-Carlo Tree Search in AlphaZero',\n  'name': 'compared to',\n  'research': 'theory',\n  'summary': 'The central hypothesis is that chain-of-thought reasoning can act as a policy improvement operator, similarly to how Monte-Carlo Tree Search is used in AlphaZero',\n  'date': '2023-09-15',\n  'title': 'Chain-of-Thought Reasoning is a Policy Improvement Operator',\n  'id': '2309.08589'},\n {'from': 'this research',\n  'to': 'self-teaching',\n  'name': 'aims to lead to',\n  'research': 'reflection',\n  'summary': 'The research hopes to lead to new directions in which language models can learn to teach themselves without the need for human demonstrations',\n  'date': '2023-09-15',\n  'title': 'Chain-of-Thought Reasoning is a Policy Improvement Operator',\n  'id': '2309.08589'},\n {'from': 'in-context learning',\n  'to': 'large language model',\n  'name': 'is an emergent ability of',\n  'research': 'context',\n  'summary': 'In-context learning is an emergent ability of large language models to use a small number of examples to learn to perform in novel domains and tasks',\n  'date': '2023-09-15',\n  'title': 'Neural Machine Translation Models Can Learn to be Few-shot Learners',\n  'id': '2309.08590'},\n {'from': 'smaller model',\n  'to': 'in-context learning',\n  'name': 'can be trained for',\n  'research': 'contribution',\n  'summary': 'A much smaller model can be trained to perform in-context learning by fine-tuning towards a specialized training objective',\n  'date': '2023-09-15',\n  'title': 'Neural Machine Translation Models Can Learn to be Few-shot Learners',\n  'id': '2309.08590'},\n {'from': 'domain adaptation',\n  'to': 'neural machine translation',\n  'name': 'used to exemplify',\n  'research': 'application',\n  'summary': 'Domain adaptation for neural machine translation is used to exemplify the specialized training objective for in-context learning',\n  'date': '2023-09-15',\n  'title': 'Neural Machine Translation Models Can Learn to be Few-shot Learners',\n  'id': '2309.08590'},\n {'from': 'smaller model',\n  'to': 'few-shot learning',\n  'name': 'takes advantage of',\n  'research': 'mechanism',\n  'summary': 'The smaller model can take advantage of relevant few-shot examples to adapt its output towards the domain',\n  'date': '2023-09-15',\n  'title': 'Neural Machine Translation Models Can Learn to be Few-shot Learners',\n  'id': '2309.08590'},\n {'from': 'our approach',\n  'to': 'batch inference',\n  'name': 'allows',\n  'research': 'contribution',\n  'summary': 'Our approach allows efficient batch inference on a mix of domains',\n  'date': '2023-09-15',\n  'title': 'Neural Machine Translation Models Can Learn to be Few-shot Learners',\n  'id': '2309.08590'},\n {'from': 'our approach',\n  'to': 'traditional supervised learning',\n  'name': 'compared with',\n  'research': 'comparison',\n  'summary': 'The quality of domain adaptation is compared to traditional supervised techniques and in-context learning with a 40b-parameter large language model',\n  'date': '2023-09-15',\n  'title': 'Neural Machine Translation Models Can Learn to be Few-shot Learners',\n  'id': '2309.08590'},\n {'from': '40b-parameter large language model',\n  'to': 'our approach',\n  'name': 'compared with',\n  'research': 'comparison',\n  'summary': 'In-context learning with a 40b-parameter large language model is compared to our approach for domain adaptation',\n  'date': '2023-09-15',\n  'title': 'Neural Machine Translation Models Can Learn to be Few-shot Learners',\n  'id': '2309.08590'},\n {'from': 'our approach',\n  'to': 'state-of-the-art baselines',\n  'name': 'outperforms',\n  'research': 'finding',\n  'summary': 'Our approach outperforms state-of-the-art baselines in terms of translation quality and immediate adaptation rate',\n  'date': '2023-09-15',\n  'title': 'Neural Machine Translation Models Can Learn to be Few-shot Learners',\n  'id': '2309.08590'},\n {'from': 'immediate adaptation rate',\n  'to': 'in-context learning',\n  'name': 'is a measure of',\n  'research': 'context',\n  'summary': 'Immediate adaptation rate is the ability to reproduce a specific term after being shown a single example',\n  'date': '2023-09-15',\n  'title': 'Neural Machine Translation Models Can Learn to be Few-shot Learners',\n  'id': '2309.08590'},\n {'from': 'large language model',\n  'to': 'question answering and reasoning',\n  'name': 'perform well in',\n  'research': 'context',\n  'summary': 'LLMs are highly adept at question answering and reasoning tasks',\n  'date': '2023-09-15',\n  'title': 'Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation\\n  into Multicultural Proverbs and Sayings',\n  'id': '2309.08591'},\n {'from': 'reasoning in situational context',\n  'to': 'cultural common ground',\n  'name': 'varies by',\n  'research': 'context',\n  'summary': 'Human expectations for reasoning in situational context vary depending on cultural common ground',\n  'date': '2023-09-15',\n  'title': 'Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation\\n  into Multicultural Proverbs and Sayings',\n  'id': '2309.08591'},\n {'from': 'multilingual large language model',\n  'to': 'culturally-diverse reasoning',\n  'name': 'should be',\n  'research': 'context',\n  'summary': 'Multilingual LLMs should also be culturally-diverse reasoners',\n  'date': '2023-09-15',\n  'title': 'Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation\\n  into Multicultural Proverbs and Sayings',\n  'id': '2309.08591'},\n {'from': 'multilingual large language model',\n  'to': 'proverb reasoning in conversational context',\n  'name': 'studied for',\n  'research': 'contribution',\n  'summary': 'The paper studies the ability of multilingual LLMs to reason with proverbs and sayings in a conversational context',\n  'date': '2023-09-15',\n  'title': 'Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation\\n  into Multicultural Proverbs and Sayings',\n  'id': '2309.08591'},\n {'from': 'multilingual large language model',\n  'to': 'figurative proverb reasoning',\n  'name': 'struggle with',\n  'research': 'finding',\n  'summary': 'MLLMs struggle to reason with figurative proverbs and sayings, especially when asked to select the wrong answer',\n  'date': '2023-09-15',\n  'title': 'Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation\\n  into Multicultural Proverbs and Sayings',\n  'id': '2309.08591'},\n {'from': 'multilingual large language model',\n  'to': 'culture gap',\n  'name': 'exhibit',\n  'research': 'finding',\n  'summary': \"There is a 'culture gap' in MLLMs when reasoning about proverbs and sayings translated from other languages\",\n  'date': '2023-09-15',\n  'title': 'Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation\\n  into Multicultural Proverbs and Sayings',\n  'id': '2309.08591'},\n {'from': 'MAPS (Multicultural Proverbs and Sayings)',\n  'to': 'proverb understanding in conversational context',\n  'name': 'constructed and released for',\n  'research': 'contribution',\n  'summary': 'The evaluation dataset MAPS is constructed and released for proverb understanding with conversational context in six different languages',\n  'date': '2023-09-15',\n  'title': 'Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation\\n  into Multicultural Proverbs and Sayings',\n  'id': '2309.08591'},\n {'from': 'parametric knowledge',\n  'to': 'pre-training',\n  'name': 'acquired during',\n  'research': 'context',\n  'summary': 'LLMs acquire extensive parametric knowledge during pre-training',\n  'date': '2023-09-15',\n  'title': '\"Merge Conflicts!\" Exploring the Impacts of External Distractors to\\n  Parametric Knowledge Graphs',\n  'id': '2309.08594'},\n {'from': 'external knowledge',\n  'to': 'interaction with users',\n  'name': 'required for',\n  'research': 'context',\n  'summary': 'LLMs require external knowledge to remain up-to-date and align with human instructions',\n  'date': '2023-09-15',\n  'title': '\"Merge Conflicts!\" Exploring the Impacts of External Distractors to\\n  Parametric Knowledge Graphs',\n  'id': '2309.08594'},\n {'from': 'large language model',\n  'to': 'external knowledge',\n  'name': 'response to',\n  'research': 'question',\n  'summary': 'Investigating how LLMs respond when external knowledge interferes with their parametric knowledge',\n  'date': '2023-09-15',\n  'title': '\"Merge Conflicts!\" Exploring the Impacts of External Distractors to\\n  Parametric Knowledge Graphs',\n  'id': '2309.08594'},\n {'from': 'systematic elicitation of LLM knowledge',\n  'to': 'large language model',\n  'name': 'proposed to',\n  'research': 'contribution',\n  'summary': 'A framework is proposed to systematically elicit LLM parametric knowledge and introduce external knowledge',\n  'date': '2023-09-15',\n  'title': '\"Merge Conflicts!\" Exploring the Impacts of External Distractors to\\n  Parametric Knowledge Graphs',\n  'id': '2309.08594'},\n {'from': 'parametric knowledge graph',\n  'to': 'LLM knowledge structures',\n  'name': 'constructed to reveal',\n  'research': 'method',\n  'summary': 'A parametric knowledge graph is constructed to reveal the different knowledge structures of LLMs',\n  'date': '2023-09-15',\n  'title': '\"Merge Conflicts!\" Exploring the Impacts of External Distractors to\\n  Parametric Knowledge Graphs',\n  'id': '2309.08594'},\n {'from': 'external knowledge',\n  'to': 'distractors',\n  'name': 'introduced through',\n  'research': 'method',\n  'summary': 'External knowledge is introduced through distractors of varying degrees, methods, positions, and formats',\n  'date': '2023-09-15',\n  'title': '\"Merge Conflicts!\" Exploring the Impacts of External Distractors to\\n  Parametric Knowledge Graphs',\n  'id': '2309.08594'},\n {'from': 'large language model',\n  'to': 'deviation from parametric knowledge',\n  'name': 'tends to produce',\n  'research': 'finding',\n  'summary': 'LLMs tend to produce responses that deviate from their parametric knowledge when encountering direct conflicts or confounding changes of information',\n  'date': '2023-09-15',\n  'title': '\"Merge Conflicts!\" Exploring the Impacts of External Distractors to\\n  Parametric Knowledge Graphs',\n  'id': '2309.08594'},\n {'from': 'large language model',\n  'to': 'external knowledge',\n  'name': 'sensitive to',\n  'research': 'finding',\n  'summary': 'LLMs are sensitive to the veracity of external knowledge but can be distracted by unrelated information',\n  'date': '2023-09-15',\n  'title': '\"Merge Conflicts!\" Exploring the Impacts of External Distractors to\\n  Parametric Knowledge Graphs',\n  'id': '2309.08594'},\n {'from': 'hallucination',\n  'to': 'large language model',\n  'name': 'risk of',\n  'research': 'implication',\n  'summary': 'There is a risk of hallucination when integrating external knowledge during interactions with current LLMs',\n  'date': '2023-09-15',\n  'title': '\"Merge Conflicts!\" Exploring the Impacts of External Distractors to\\n  Parametric Knowledge Graphs',\n  'id': '2309.08594'},\n {'from': 'data and results',\n  'to': 'publicly available',\n  'name': 'availability',\n  'research': 'note',\n  'summary': 'All the data and results from the study are publicly available',\n  'date': '2023-09-15',\n  'title': '\"Merge Conflicts!\" Exploring the Impacts of External Distractors to\\n  Parametric Knowledge Graphs',\n  'id': '2309.08594'},\n {'from': 'polysemanticity',\n  'to': 'understandability',\n  'name': 'prevents',\n  'research': 'problem',\n  'summary': 'Polysemanticity prevents concise, human-understandable explanations of neural network internals',\n  'date': '2023-09-15',\n  'title': 'Sparse Autoencoders Find Highly Interpretable Features in Language\\n  Models',\n  'id': '2309.08600'},\n {'from': 'superposition',\n  'to': 'polysemanticity',\n  'name': 'hypothesised cause of',\n  'research': 'context',\n  'summary': 'Superposition is a hypothesised cause of polysemanticity in neural networks',\n  'date': '2023-09-15',\n  'title': 'Sparse Autoencoders Find Highly Interpretable Features in Language\\n  Models',\n  'id': '2309.08600'},\n {'from': 'sparse autoencoder',\n  'to': 'interpretable directions',\n  'name': 'used to identify',\n  'research': 'method',\n  'summary': 'Sparse autoencoders are used to reconstruct internal activations of a language model to identify interpretable directions',\n  'date': '2023-09-15',\n  'title': 'Sparse Autoencoders Find Highly Interpretable Features in Language\\n  Models',\n  'id': '2309.08600'},\n {'from': 'interpretable directions',\n  'to': 'monosemantic',\n  'name': 'characterized by',\n  'research': 'finding',\n  'summary': 'Interpretable directions are characterized by sparse activation and are more monosemantic than those identified by alternative approaches',\n  'date': '2023-09-15',\n  'title': 'Sparse Autoencoders Find Highly Interpretable Features in Language\\n  Models',\n  'id': '2309.08600'},\n {'from': 'automated interpretability measurement',\n  'to': 'interpretability',\n  'name': 'used to measure',\n  'research': 'method',\n  'summary': 'Automated methods are used to measure the interpretability of features learned by sparse autoencoders',\n  'date': '2023-09-15',\n  'title': 'Sparse Autoencoders Find Highly Interpretable Features in Language\\n  Models',\n  'id': '2309.08600'},\n {'from': 'learned set of features',\n  'to': 'indirect object identification',\n  'name': 'enables pinpointing',\n  'research': 'contribution',\n  'summary': 'The learned set of features enables pinpointing causally responsible features for counterfactual behavior in tasks',\n  'date': '2023-09-15',\n  'title': 'Sparse Autoencoders Find Highly Interpretable Features in Language\\n  Models',\n  'id': '2309.08600'},\n {'from': 'language model',\n  'to': 'superposition',\n  'name': 'subject to resolution of',\n  'research': 'contribution',\n  'summary': 'It is possible to resolve superposition in language models using a scalable, unsupervised method',\n  'date': '2023-09-15',\n  'title': 'Sparse Autoencoders Find Highly Interpretable Features in Language\\n  Models',\n  'id': '2309.08600'},\n {'from': 'scalable unsupervised method',\n  'to': 'superposition',\n  'name': 'proposed for',\n  'research': 'contribution',\n  'summary': 'A scalable, unsupervised method is proposed to resolve superposition in language models',\n  'date': '2023-09-15',\n  'title': 'Sparse Autoencoders Find Highly Interpretable Features in Language\\n  Models',\n  'id': '2309.08600'},\n {'from': 'mechanistic interpretability',\n  'to': 'model transparency',\n  'name': 'foundation for',\n  'research': 'reflection',\n  'summary': 'The method may serve as a foundation for future mechanistic interpretability work',\n  'date': '2023-09-15',\n  'title': 'Sparse Autoencoders Find Highly Interpretable Features in Language\\n  Models',\n  'id': '2309.08600'},\n {'from': 'model transparency',\n  'to': 'steerability',\n  'name': 'associated with',\n  'research': 'reflection',\n  'summary': 'Greater model transparency and steerability are goals of mechanistic interpretability work',\n  'date': '2023-09-15',\n  'title': 'Sparse Autoencoders Find Highly Interpretable Features in Language\\n  Models',\n  'id': '2309.08600'},\n {'from': 'brittleness',\n  'to': 'larger and diverse benchmarks',\n  'name': 'leads to',\n  'research': 'context',\n  'summary': 'Brittle behavior of modern language models leads to the development of larger and more diverse benchmarks',\n  'date': '2023-09-14',\n  'title': 'Anchor Points: Benchmarking Models with Much Fewer Examples',\n  'id': '2309.08638'},\n {'from': 'small evaluation sets',\n  'to': 'assessment',\n  'name': 'can benchmark',\n  'research': 'proposal',\n  'summary': 'Model performance can be benchmarked and elucidated with much smaller evaluation sets',\n  'date': '2023-09-14',\n  'title': 'Anchor Points: Benchmarking Models with Much Fewer Examples',\n  'id': '2309.08638'},\n {'from': 'model confidence correlation',\n  'to': 'language classification benchmarks',\n  'name': 'observed in',\n  'research': 'finding',\n  'summary': 'Model confidence in the correct class on many pairs of points is strongly correlated across models in six popular language classification benchmarks',\n  'date': '2023-09-14',\n  'title': 'Anchor Points: Benchmarking Models with Much Fewer Examples',\n  'id': '2309.08638'},\n {'from': 'anchor point selection',\n  'to': 'small subsets selection',\n  'name': 'proposed for',\n  'research': 'contribution',\n  'summary': 'Anchor point selection is proposed to select small subsets of datasets that capture model behavior across the entire dataset',\n  'date': '2023-09-14',\n  'title': 'Anchor Points: Benchmarking Models with Much Fewer Examples',\n  'id': '2309.08638'},\n {'from': 'anchor point selection',\n  'to': 'model ranking',\n  'name': 'reliably ranks',\n  'research': 'finding',\n  'summary': 'Anchor points reliably rank models across diverse language model-prompt pairs and outperform other baselines at accurately ranking models',\n  'date': '2023-09-14',\n  'title': 'Anchor Points: Benchmarking Models with Much Fewer Examples',\n  'id': '2309.08638'},\n {'from': 'anchor point selection',\n  'to': 'per-class prediction estimation',\n  'name': 'estimates',\n  'research': 'finding',\n  'summary': 'Just several anchor points can be used to estimate model per-class predictions on all other points in a dataset with low mean absolute error',\n  'date': '2023-09-14',\n  'title': 'Anchor Points: Benchmarking Models with Much Fewer Examples',\n  'id': '2309.08638'},\n {'from': 'anchor point maps',\n  'to': 'model performance visualization',\n  'name': 'presented for',\n  'research': 'contribution',\n  'summary': 'Anchor point maps are presented for visualizing insights and facilitating comparisons of the performance of different models on various regions within the dataset distribution',\n  'date': '2023-09-14',\n  'title': 'Anchor Points: Benchmarking Models with Much Fewer Examples',\n  'id': '2309.08638'},\n {'from': 'mobile applications',\n  'to': 'app usage prediction',\n  'name': 'challenge in',\n  'research': 'context',\n  'summary': 'Predicting app usage remains a formidable challenge due to intricate user behaviours and ever-evolving contexts',\n  'date': '2023-09-15',\n  'title': 'MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings',\n  'id': '2309.08648'},\n {'from': 'MAPLE',\n  'to': 'app usage prediction',\n  'name': 'introduced to address',\n  'research': 'contribution',\n  'summary': 'The MAPLE model is introduced to address the challenges in predicting app usage by leveraging LLM embeddings',\n  'date': '2023-09-15',\n  'title': 'MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings',\n  'id': '2309.08648'},\n {'from': 'large language model',\n  'to': 'MAPLE',\n  'name': 'utilized by',\n  'research': 'method',\n  'summary': 'MAPLE utilizes LLMs to predict app usage accurately',\n  'date': '2023-09-15',\n  'title': 'MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings',\n  'id': '2309.08648'},\n {'from': 'MAPLE',\n  'to': 'public datasets',\n  'name': 'tested on',\n  'research': 'method',\n  'summary': 'MAPLE is rigorously tested on two public datasets to highlight its capability',\n  'date': '2023-09-15',\n  'title': 'MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings',\n  'id': '2309.08648'},\n {'from': 'MAPLE',\n  'to': 'understanding intricate patterns and contexts',\n  'name': 'capable of',\n  'research': 'finding',\n  'summary': 'MAPLE is capable of deciphering intricate patterns and comprehending user contexts',\n  'date': '2023-09-15',\n  'title': 'MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings',\n  'id': '2309.08648'},\n {'from': 'MAPLE',\n  'to': 'versatility and resilience',\n  'name': 'confirms',\n  'research': 'finding',\n  'summary': \"The robust results from testing confirm MAPLE's versatility and resilience across various scenarios\",\n  'date': '2023-09-15',\n  'title': 'MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings',\n  'id': '2309.08648'},\n {'from': 'large language model',\n  'to': 'various domains',\n  'name': 'applicability emphasized in',\n  'research': 'reflection',\n  'summary': 'The outcomes emphasize the broader applicability of LLMs in different domains beyond app prediction',\n  'date': '2023-09-15',\n  'title': 'MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings',\n  'id': '2309.08648'},\n {'from': 'large language model',\n  'to': 'modelling human behaviours',\n  'name': 'potential in',\n  'research': 'reflection',\n  'summary': 'The research emphasizes the potential of LLMs in app usage prediction and their transformative capacity in modelling human behaviours',\n  'date': '2023-09-15',\n  'title': 'MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings',\n  'id': '2309.08648'},\n {'from': 'large language model',\n  'to': 'table representation learning',\n  'name': 'applied in',\n  'research': 'context',\n  'summary': 'The capabilities of LLMs have been successfully applied in the context of table representation learning',\n  'date': '2023-09-15',\n  'title': 'Adversarial Attacks on Tables with Entity Swap',\n  'id': '2309.08650'},\n {'from': 'tabular language model',\n  'to': 'state-of-the-art results',\n  'name': 'achieves',\n  'research': 'context',\n  'summary': 'Tabular language models have reported state-of-the-art results across various tasks for table interpretation',\n  'date': '2023-09-15',\n  'title': 'Adversarial Attacks on Tables with Entity Swap',\n  'id': '2309.08650'},\n {'from': 'entity leakage',\n  'to': 'evaluation datasets',\n  'name': 'found in',\n  'research': 'context',\n  'summary': 'A closer look into the datasets commonly used for evaluation reveals an entity leakage from the train set into the test set',\n  'date': '2023-09-15',\n  'title': 'Adversarial Attacks on Tables with Entity Swap',\n  'id': '2309.08650'},\n {'from': 'adversarial attack',\n  'to': 'realistic inference',\n  'name': 'explores',\n  'research': 'contribution',\n  'summary': 'The paper explores adversarial attacks that represent a more realistic inference setup',\n  'date': '2023-09-15',\n  'title': 'Adversarial Attacks on Tables with Entity Swap',\n  'id': '2309.08650'},\n {'from': 'large language model',\n  'to': 'adversarial attack',\n  'name': 'affected by',\n  'research': 'context',\n  'summary': 'Adversarial attacks on text have been shown to greatly affect the performance of LLMs',\n  'date': '2023-09-15',\n  'title': 'Adversarial Attacks on Tables with Entity Swap',\n  'id': '2309.08650'},\n {'from': 'tabular language model',\n  'to': 'adversarial attack',\n  'name': 'target of',\n  'research': 'contribution',\n  'summary': 'Currently, there are no attacks targeting tabular language models',\n  'date': '2023-09-15',\n  'title': 'Adversarial Attacks on Tables with Entity Swap',\n  'id': '2309.08650'},\n {'from': 'evasive entity-swap attack',\n  'to': 'column type annotation',\n  'name': 'proposed for',\n  'research': 'contribution',\n  'summary': 'An evasive entity-swap attack is proposed for the column type annotation (CTA) task',\n  'date': '2023-09-15',\n  'title': 'Adversarial Attacks on Tables with Entity Swap',\n  'id': '2309.08650'},\n {'from': 'evasive entity-swap attack',\n  'to': 'first black-box attack on tables',\n  'name': 'characterized as',\n  'research': 'contribution',\n  'summary': 'The CTA attack is the first black-box attack on tables',\n  'date': '2023-09-15',\n  'title': 'Adversarial Attacks on Tables with Entity Swap',\n  'id': '2309.08650'},\n {'from': 'similarity-based sampling',\n  'to': 'adversarial example generation',\n  'name': 'employed to',\n  'research': 'method',\n  'summary': 'A similarity-based sampling strategy is employed to generate adversarial examples',\n  'date': '2023-09-15',\n  'title': 'Adversarial Attacks on Tables with Entity Swap',\n  'id': '2309.08650'},\n {'from': 'evasive entity-swap attack',\n  'to': 'performance drop',\n  'name': 'results in',\n  'research': 'finding',\n  'summary': 'The proposed attack generates up to a 70% drop in performance',\n  'date': '2023-09-15',\n  'title': 'Adversarial Attacks on Tables with Entity Swap',\n  'id': '2309.08650'},\n {'from': 'fake news',\n  'to': 'trust',\n  'name': 'poses threats to',\n  'research': 'context',\n  'summary': 'The spread of fake news undermines trust and poses threats to society',\n  'date': '2023-09-15',\n  'title': 'Fake News Detectors are Biased against Texts Generated by Large Language\\n  Models',\n  'id': '2309.08674'},\n {'from': 'generating believable fake content',\n  'to': 'large language model',\n  'name': 'intensified by',\n  'research': 'context',\n  'summary': 'The capability to generate believable fake content has been intensified by LLMs',\n  'date': '2023-09-15',\n  'title': 'Fake News Detectors are Biased against Texts Generated by Large Language\\n  Models',\n  'id': '2309.08674'},\n {'from': 'evaluation of fake news detectors',\n  'to': 'human-written and LLM-generated misinformation',\n  'name': 'involves',\n  'research': 'contribution',\n  'summary': 'A novel paradigm is presented to evaluate fake news detectors involving both human-written and LLM-generated misinformation',\n  'date': '2023-09-15',\n  'title': 'Fake News Detectors are Biased against Texts Generated by Large Language\\n  Models',\n  'id': '2309.08674'},\n {'from': 'flagging LLM-generated content as fake',\n  'to': 'fake news detectors',\n  'name': 'found in',\n  'research': 'finding',\n  'summary': 'A significant bias is found in many existing detectors, more prone to flagging LLM-generated content as fake news',\n  'date': '2023-09-15',\n  'title': 'Fake News Detectors are Biased against Texts Generated by Large Language\\n  Models',\n  'id': '2309.08674'},\n {'from': 'misclassifying human-written fake news as genuine',\n  'to': 'fake news detectors',\n  'name': 'found in',\n  'research': 'finding',\n  'summary': 'Many existing detectors often misclassify human-written fake news as genuine',\n  'date': '2023-09-15',\n  'title': 'Fake News Detectors are Biased against Texts Generated by Large Language\\n  Models',\n  'id': '2309.08674'},\n {'from': 'distinct to LLM outputs',\n  'to': 'flagging LLM-generated content as fake',\n  'name': 'contributes to',\n  'research': 'analysis',\n  'summary': 'Distinct linguistic patterns inherent to LLM outputs contribute to the bias in fake news detection',\n  'date': '2023-09-15',\n  'title': 'Fake News Detectors are Biased against Texts Generated by Large Language\\n  Models',\n  'id': '2309.08674'},\n {'from': 'adversarial training with LLM-paraphrased genuine news',\n  'to': 'fake news detection',\n  'name': 'mitigates',\n  'research': 'solution',\n  'summary': 'Adversarial training with LLM-paraphrased genuine news is introduced as a mitigation strategy for the bias in fake news detection',\n  'date': '2023-09-15',\n  'title': 'Fake News Detectors are Biased against Texts Generated by Large Language\\n  Models',\n  'id': '2309.08674'},\n {'from': 'mitigated fake news detector',\n  'to': 'detection accuracy',\n  'name': 'shows improvement in',\n  'research': 'outcome',\n  'summary': 'The mitigated model shows improvements in detection accuracy for both human and LLM-generated news',\n  'date': '2023-09-15',\n  'title': 'Fake News Detectors are Biased against Texts Generated by Large Language\\n  Models',\n  'id': '2309.08674'},\n {'from': 'GossipCop++',\n  'to': 'fake news detection',\n  'name': 'released for',\n  'research': 'resource',\n  'summary': 'GossipCop++ is released, amalgamating human-validated articles with LLM-generated fake and real news',\n  'date': '2023-09-15',\n  'title': 'Fake News Detectors are Biased against Texts Generated by Large Language\\n  Models',\n  'id': '2309.08674'},\n {'from': 'Politifact++',\n  'to': 'fake news detection',\n  'name': 'released for',\n  'research': 'resource',\n  'summary': 'Politifact++ is released, amalgamating human-validated articles with LLM-generated fake and real news',\n  'date': '2023-09-15',\n  'title': 'Fake News Detectors are Biased against Texts Generated by Large Language\\n  Models',\n  'id': '2309.08674'},\n {'from': 'pre-trained language model',\n  'to': 'memory-constrained deployment',\n  'name': 'has challenge',\n  'research': 'context',\n  'summary': 'The extensive memory footprint of PLMs can hinder deployment in memory-constrained settings',\n  'date': '2023-09-15',\n  'title': 'Frustratingly Simple Memory Efficiency for Pre-trained Language Models\\n  via Dynamic Embedding Pruning',\n  'id': '2309.08708'},\n {'from': 'embedding matrix',\n  'to': 'vocabulary',\n  'name': 'represents',\n  'research': 'context',\n  'summary': 'PLMs use embedding matrices to represent extensive vocabularies, forming a large proportion of the model parameters',\n  'date': '2023-09-15',\n  'title': 'Frustratingly Simple Memory Efficiency for Pre-trained Language Models\\n  via Dynamic Embedding Pruning',\n  'id': '2309.08708'},\n {'from': 'parameter-efficient PLM development',\n  'to': 'pruning parameters',\n  'name': 'considers',\n  'research': 'context',\n  'summary': 'Previous work towards parameter-efficient PLM development has considered pruning parameters within the Transformer layers',\n  'date': '2023-09-15',\n  'title': 'Frustratingly Simple Memory Efficiency for Pre-trained Language Models\\n  via Dynamic Embedding Pruning',\n  'id': '2309.08708'},\n {'from': 'embedding matrix',\n  'to': 'fine-tuning',\n  'name': 'pruning unexplored in',\n  'research': 'gap',\n  'summary': 'Pruning the embedding matrix as part of fine-tuning or inference has yet to be explored',\n  'date': '2023-09-15',\n  'title': 'Frustratingly Simple Memory Efficiency for Pre-trained Language Models\\n  via Dynamic Embedding Pruning',\n  'id': '2309.08708'},\n {'from': 'embedding matrix',\n  'to': 'inference',\n  'name': 'pruning unexplored in',\n  'research': 'gap',\n  'summary': 'Pruning the embedding matrix as part of fine-tuning or inference has yet to be explored',\n  'date': '2023-09-15',\n  'title': 'Frustratingly Simple Memory Efficiency for Pre-trained Language Models\\n  via Dynamic Embedding Pruning',\n  'id': '2309.08708'},\n {'from': 'unused vocabulary proportion',\n  'to': 'fine-tuning and inference',\n  'name': 'demonstrated in',\n  'research': 'finding',\n  'summary': 'A significant proportion of the vocabulary remains unused in fine-tuning or inference scenarios',\n  'date': '2023-09-15',\n  'title': 'Frustratingly Simple Memory Efficiency for Pre-trained Language Models\\n  via Dynamic Embedding Pruning',\n  'id': '2309.08708'},\n {'from': 'embedding matrix memory footprint minimization',\n  'to': 'unused vocabulary proportion',\n  'name': 'leverages',\n  'research': 'contribution',\n  'summary': 'A simple yet effective approach is proposed that leverages the finding of unused vocabulary to minimize the memory footprint of the embedding matrix',\n  'date': '2023-09-15',\n  'title': 'Frustratingly Simple Memory Efficiency for Pre-trained Language Models\\n  via Dynamic Embedding Pruning',\n  'id': '2309.08708'},\n {'from': 'embedding matrix memory footprint minimization',\n  'to': 'reduced memory usage',\n  'name': 'provides',\n  'research': 'finding',\n  'summary': 'The approach provides substantial reductions in memory usage across a wide range of models and tasks',\n  'date': '2023-09-15',\n  'title': 'Frustratingly Simple Memory Efficiency for Pre-trained Language Models\\n  via Dynamic Embedding Pruning',\n  'id': '2309.08708'},\n {'from': 'embedding matrix memory footprint minimization',\n  'to': 'downstream task performance',\n  'name': 'maintains',\n  'research': 'finding',\n  'summary': 'The approach maintains equivalent downstream task performance while allowing a more efficient use of compute resources',\n  'date': '2023-09-15',\n  'title': 'Frustratingly Simple Memory Efficiency for Pre-trained Language Models\\n  via Dynamic Embedding Pruning',\n  'id': '2309.08708'},\n {'from': 'byte pair encoding tokenization',\n  'to': 'large language model',\n  'name': 'used in',\n  'research': 'context',\n  'summary': 'Byte pair encoding tokenization is formalized as it is used in large language models and other NLP systems',\n  'date': '2023-09-15',\n  'title': 'Formalizing BPE Tokenization',\n  'id': '2309.08715'},\n {'from': 'byte pair encoding tokenization',\n  'to': 'Natural Language Processing (NLP)',\n  'name': 'used in',\n  'research': 'context',\n  'summary': 'Byte pair encoding tokenization is formalized as it is used in large language models and other NLP systems',\n  'date': '2023-09-15',\n  'title': 'Formalizing BPE Tokenization',\n  'id': '2309.08715'},\n {'from': 'SentencePiece',\n  'to': 'semantics',\n  'name': 'investigated for',\n  'research': 'contribution',\n  'summary': 'The paper formally defines and investigates the semantics of the SentencePiece tokenizer',\n  'date': '2023-09-15',\n  'title': 'Formalizing BPE Tokenization',\n  'id': '2309.08715'},\n {'from': 'HuggingFace',\n  'to': 'semantics',\n  'name': 'investigated for',\n  'research': 'contribution',\n  'summary': 'The paper formally defines and investigates the semantics of the HuggingFace tokenizer',\n  'date': '2023-09-15',\n  'title': 'Formalizing BPE Tokenization',\n  'id': '2309.08715'},\n {'from': 'SentencePiece',\n  'to': 'HuggingFace',\n  'name': 'related to',\n  'research': 'contribution',\n  'summary': 'The paper investigates how the SentencePiece tokenizer relates to the HuggingFace tokenizer',\n  'date': '2023-09-15',\n  'title': 'Formalizing BPE Tokenization',\n  'id': '2309.08715'},\n {'from': 'incremental tokenization',\n  'to': 'byte pair encoding tokenization',\n  'name': 'considered for',\n  'research': 'contribution',\n  'summary': 'The paper considers how tokenization can be performed incrementally',\n  'date': '2023-09-15',\n  'title': 'Formalizing BPE Tokenization',\n  'id': '2309.08715'},\n {'from': 'left-to-right tokenization',\n  'to': 'byte pair encoding tokenization',\n  'name': 'considered for',\n  'research': 'contribution',\n  'summary': 'The paper considers doing tokenization left-to-right using an amount of memory constant in the length of the string',\n  'date': '2023-09-15',\n  'title': 'Formalizing BPE Tokenization',\n  'id': '2309.08715'},\n {'from': 'finite state string-to-string transducer',\n  'to': 'left-to-right tokenization',\n  'name': 'enables',\n  'research': 'contribution',\n  'summary': 'Using a finite state string-to-string transducer enables tokenization with constant memory usage',\n  'date': '2023-09-15',\n  'title': 'Formalizing BPE Tokenization',\n  'id': '2309.08715'},\n {'from': 'large language model',\n  'to': 'multimodal applications',\n  'name': 'potential in',\n  'research': 'context',\n  'summary': 'LLMs have shown immense potential in multimodal applications',\n  'date': '2023-09-15',\n  'title': 'MusiLingo: Bridging Music and Text with Pre-trained Language Models for\\n  Music Captioning and Query Response',\n  'id': '2309.08730'},\n {'from': 'Musilingo',\n  'to': 'music caption generation',\n  'name': 'developed for',\n  'research': 'contribution',\n  'summary': 'Musilingo is a novel system for music caption generation and music-related query responses',\n  'date': '2023-09-15',\n  'title': 'MusiLingo: Bridging Music and Text with Pre-trained Language Models for\\n  Music Captioning and Query Response',\n  'id': '2309.08730'},\n {'from': 'Musilingo',\n  'to': 'music-related query responses',\n  'name': 'developed for',\n  'research': 'contribution',\n  'summary': 'Musilingo is a novel system for music caption generation and music-related query responses',\n  'date': '2023-09-15',\n  'title': 'MusiLingo: Bridging Music and Text with Pre-trained Language Models for\\n  Music Captioning and Query Response',\n  'id': '2309.08730'},\n {'from': 'MERT',\n  'to': 'Vicuna-7B',\n  'name': 'aligned with',\n  'research': 'method',\n  'summary': 'Music representations from the pre-trained frozen music audio model MERT are aligned with the frozen Vicuna-7B language model',\n  'date': '2023-09-15',\n  'title': 'MusiLingo: Bridging Music and Text with Pre-trained Language Models for\\n  Music Captioning and Query Response',\n  'id': '2309.08730'},\n {'from': 'Vicuna-7B',\n  'to': 'LLaMA',\n  'name': 'adaptation of',\n  'research': 'context',\n  'summary': 'Vicuna-7B language model is an adaptation of LLaMA',\n  'date': '2023-09-15',\n  'title': 'MusiLingo: Bridging Music and Text with Pre-trained Language Models for\\n  Music Captioning and Query Response',\n  'id': '2309.08730'},\n {'from': 'Music Instruct (MI)',\n  'to': 'MusicCaps',\n  'name': 'created from',\n  'research': 'method',\n  'summary': 'The Music Instruct (MI) dataset is created from captions in the MusicCaps datasets, tailored for open-ended music inquiries',\n  'date': '2023-09-15',\n  'title': 'MusiLingo: Bridging Music and Text with Pre-trained Language Models for\\n  Music Captioning and Query Response',\n  'id': '2309.08730'},\n {'from': 'Musilingo',\n  'to': 'music caption dataset',\n  'name': 'trained on',\n  'research': 'method',\n  'summary': 'Musilingo is trained on an extensive music caption dataset and fine-tuned with instructional data',\n  'date': '2023-09-15',\n  'title': 'MusiLingo: Bridging Music and Text with Pre-trained Language Models for\\n  Music Captioning and Query Response',\n  'id': '2309.08730'},\n {'from': 'Musilingo',\n  'to': 'generating music captions',\n  'name': 'performance evaluated by',\n  'research': 'evaluation',\n  'summary': \"Empirical evaluations demonstrate Musilingo's competitive performance in generating music captions and composing music-related Q&A pairs\",\n  'date': '2023-09-15',\n  'title': 'MusiLingo: Bridging Music and Text with Pre-trained Language Models for\\n  Music Captioning and Query Response',\n  'id': '2309.08730'},\n {'from': 'Musilingo',\n  'to': 'composing music-related Q&A pairs',\n  'name': 'performance evaluated by',\n  'research': 'evaluation',\n  'summary': \"Empirical evaluations demonstrate Musilingo's competitive performance in generating music captions and composing music-related Q&A pairs\",\n  'date': '2023-09-15',\n  'title': 'MusiLingo: Bridging Music and Text with Pre-trained Language Models for\\n  Music Captioning and Query Response',\n  'id': '2309.08730'},\n {'from': 'predicting chemical function from structure',\n  'to': 'chemical sciences',\n  'name': 'is a major goal in',\n  'research': 'context',\n  'summary': 'Predicting chemical function from structure is a major goal of the chemical sciences',\n  'date': '2023-09-15',\n  'title': 'Mining Patents with Large Language Models Demonstrates Congruence of\\n  Functional Labels and Chemical Structures',\n  'id': '2309.08765'},\n {'from': 'machine learning',\n  'to': 'general predictive models',\n  'name': 'enables',\n  'research': 'context',\n  'summary': 'New machine learning algorithms enable the possibility of general predictive models for different chemical functions',\n  'date': '2023-09-15',\n  'title': 'Mining Patents with Large Language Models Demonstrates Congruence of\\n  Functional Labels and Chemical Structures',\n  'id': '2309.08765'},\n {'from': 'large language model',\n  'to': 'chemical patents',\n  'name': 'applied to',\n  'research': 'contribution',\n  'summary': 'Large language models are applied to chemical patents to leverage information about chemical functionality',\n  'date': '2023-09-15',\n  'title': 'Mining Patents with Large Language Models Demonstrates Congruence of\\n  Functional Labels and Chemical Structures',\n  'id': '2309.08765'},\n {'from': 'chemical patents',\n  'to': 'chemical function',\n  'name': 'contain',\n  'research': 'context',\n  'summary': 'Chemical patents contain vast knowledge on chemical function',\n  'date': '2023-09-15',\n  'title': 'Mining Patents with Large Language Models Demonstrates Congruence of\\n  Functional Labels and Chemical Structures',\n  'id': '2309.08765'},\n {'from': 'extracting high-quality functional labels',\n  'to': 'chemical patents',\n  'name': 'historically neglected due to',\n  'research': 'context',\n  'summary': 'The usefulness of chemical patents as a dataset has been neglected due to the impracticality of extracting high-quality functional labels',\n  'date': '2023-09-15',\n  'title': 'Mining Patents with Large Language Models Demonstrates Congruence of\\n  Functional Labels and Chemical Structures',\n  'id': '2309.08765'},\n {'from': 'ChatGPT-assisted patent summarization and word-embedding label cleaning',\n  'to': 'CHEF dataset',\n  'name': 'used to create',\n  'research': 'method',\n  'summary': 'A scalable ChatGPT-assisted patent summarization and word-embedding label cleaning pipeline is used to derive the CHEF dataset',\n  'date': '2023-09-15',\n  'title': 'Mining Patents with Large Language Models Demonstrates Congruence of\\n  Functional Labels and Chemical Structures',\n  'id': '2309.08765'},\n {'from': 'CHEF dataset',\n  'to': '100k molecules with functional labels',\n  'name': 'contains',\n  'research': 'contribution',\n  'summary': 'The CHEF dataset contains 100k molecules and their patent-derived functional labels',\n  'date': '2023-09-15',\n  'title': 'Mining Patents with Large Language Models Demonstrates Congruence of\\n  Functional Labels and Chemical Structures',\n  'id': '2309.08765'},\n {'from': 'functional label prediction model',\n  'to': 'CHEF dataset',\n  'name': 'trained on',\n  'research': 'method',\n  'summary': 'A model is trained on the CHEF dataset to assign new functional labels to compounds',\n  'date': '2023-09-15',\n  'title': 'Mining Patents with Large Language Models Demonstrates Congruence of\\n  Functional Labels and Chemical Structures',\n  'id': '2309.08765'},\n {'from': 'functional label prediction model',\n  'to': 'drug discovery and repurposing',\n  'name': 'used for',\n  'research': 'finding',\n  'summary': 'The model was used to retrodict approved hepatitis C antivirals, uncover an undisclosed antiviral mechanism, and identify plausible serotonin-related drugs',\n  'date': '2023-09-15',\n  'title': 'Mining Patents with Large Language Models Demonstrates Congruence of\\n  Functional Labels and Chemical Structures',\n  'id': '2309.08765'},\n {'from': 'co-occurrence graph of functional labels',\n  'to': 'semantic structure',\n  'name': 'reveals',\n  'research': 'finding',\n  'summary': 'The co-occurrence graph of the functional labels contains a robust semantic structure, allowing examination of functional relatedness among compounds',\n  'date': '2023-09-15',\n  'title': 'Mining Patents with Large Language Models Demonstrates Congruence of\\n  Functional Labels and Chemical Structures',\n  'id': '2309.08765'},\n {'from': 'audio and text representation alignment',\n  'to': 'audio generation',\n  'name': 'enhances',\n  'research': 'contribution',\n  'summary': 'The approach enhances control over audio generation by emphasizing the alignment between audio and text representations during model training',\n  'date': '2023-09-15',\n  'title': 'Enhance audio generation controllability through representation\\n  similarity regularization',\n  'id': '2309.08773'},\n {'from': 'language model-based audio generator',\n  'to': 'textual and audio token representations',\n  'name': 'leverages',\n  'research': 'context',\n  'summary': 'The model leverages input from both textual and audio token representations to predict subsequent audio tokens',\n  'date': '2023-09-15',\n  'title': 'Enhance audio generation controllability through representation\\n  similarity regularization',\n  'id': '2309.08773'},\n {'from': 'lack of explicit regularization',\n  'to': 'language model-based audio generator',\n  'name': 'exists in',\n  'research': 'problem',\n  'summary': \"The current configuration lacks explicit regularization to ensure the alignment between the chosen text representation and the language model's predictions\",\n  'date': '2023-09-15',\n  'title': 'Enhance audio generation controllability through representation\\n  similarity regularization',\n  'id': '2309.08773'},\n {'from': 'classifier-free guidance (CFG)',\n  'to': 'text condition',\n  'name': 'excludes',\n  'research': 'context',\n  'summary': 'During the CFG phase, the text condition is excluded from cross attention during language model training',\n  'date': '2023-09-15',\n  'title': 'Enhance audio generation controllability through representation\\n  similarity regularization',\n  'id': '2309.08773'},\n {'from': 'audio and text representation regularization',\n  'to': 'alignment between audio and text representations',\n  'name': 'aims to',\n  'research': 'contribution',\n  'summary': 'The proposal aims to minimize discrepancies in audio and text similarity compared to other samples within the same training batch',\n  'date': '2023-09-15',\n  'title': 'Enhance audio generation controllability through representation\\n  similarity regularization',\n  'id': '2309.08773'},\n {'from': 'music generation',\n  'to': 'audio and text representation regularization',\n  'name': 'benefits from',\n  'research': 'finding',\n  'summary': 'Music generation tasks demonstrate improvements in objective metrics from the proposed methods',\n  'date': '2023-09-15',\n  'title': 'Enhance audio generation controllability through representation\\n  similarity regularization',\n  'id': '2309.08773'},\n {'from': 'audio generation',\n  'to': 'audio and text representation regularization',\n  'name': 'benefits from',\n  'research': 'finding',\n  'summary': 'Audio generation tasks demonstrate improvements in objective metrics and human perception from the proposed methods',\n  'date': '2023-09-15',\n  'title': 'Enhance audio generation controllability through representation\\n  similarity regularization',\n  'id': '2309.08773'},\n {'from': 'objective metrics',\n  'to': 'audio and text representation regularization',\n  'name': 'improved by',\n  'research': 'finding',\n  'summary': 'Objective metrics for both audio and music generation are improved by the proposed methods',\n  'date': '2023-09-15',\n  'title': 'Enhance audio generation controllability through representation\\n  similarity regularization',\n  'id': '2309.08773'},\n {'from': 'human perception for audio generation',\n  'to': 'audio and text representation regularization',\n  'name': 'enhanced by',\n  'research': 'finding',\n  'summary': 'Human perception for audio generation is enhanced by the proposed methods',\n  'date': '2023-09-15',\n  'title': 'Enhance audio generation controllability through representation\\n  similarity regularization',\n  'id': '2309.08773'},\n {'from': 'bio-inspired materials science',\n  'to': 'engineering solutions',\n  'name': 'lacks',\n  'research': 'context',\n  'summary': 'Surprisingly little knowledge from bio-inspired materials science has been systematically translated to engineering solutions',\n  'date': '2023-09-15',\n  'title': 'BioinspiredLLM: Conversational Large Language Model for the Mechanics of\\n  Biological and Bio-inspired Materials',\n  'id': '2309.08788'},\n {'from': 'BioInspiredLLM',\n  'to': 'large language model',\n  'name': 'is a',\n  'research': 'contribution',\n  'summary': 'BioInspiredLLM is an open-source autoregressive transformer large language model reported to accelerate discovery and guide insights in bio-inspired materials science',\n  'date': '2023-09-15',\n  'title': 'BioinspiredLLM: Conversational Large Language Model for the Mechanics of\\n  Biological and Bio-inspired Materials',\n  'id': '2309.08788'},\n {'from': 'BioInspiredLLM',\n  'to': 'peer-reviewed articles corpus',\n  'name': 'finetuned with',\n  'research': 'method',\n  'summary': 'BioInspiredLLM was finetuned with a corpus of over a thousand peer-reviewed articles in the field of structural biological and bio-inspired materials',\n  'date': '2023-09-15',\n  'title': 'BioinspiredLLM: Conversational Large Language Model for the Mechanics of\\n  Biological and Bio-inspired Materials',\n  'id': '2309.08788'},\n {'from': 'BioInspiredLLM',\n  'to': 'research tasks',\n  'name': 'can assist with',\n  'research': 'capability',\n  'summary': 'BioInspiredLLM can be prompted to recall information, assist with research tasks, and function as an engine for creativity',\n  'date': '2023-09-15',\n  'title': 'BioinspiredLLM: Conversational Large Language Model for the Mechanics of\\n  Biological and Bio-inspired Materials',\n  'id': '2309.08788'},\n {'from': 'BioInspiredLLM',\n  'to': 'information recall and question formulation',\n  'name': 'able to',\n  'research': 'finding',\n  'summary': 'BioInspiredLLM is able to accurately recall information about biological materials and formulate biomaterials questions and answers',\n  'date': '2023-09-15',\n  'title': 'BioinspiredLLM: Conversational Large Language Model for the Mechanics of\\n  Biological and Bio-inspired Materials',\n  'id': '2309.08788'},\n {'from': 'BioInspiredLLM',\n  'to': 'hypothesis development',\n  'name': 'develops',\n  'research': 'finding',\n  'summary': 'BioInspiredLLM develops sound hypotheses regarding biological materials design, even for materials never explicitly studied before',\n  'date': '2023-09-15',\n  'title': 'BioinspiredLLM: Conversational Large Language Model for the Mechanics of\\n  Biological and Bio-inspired Materials',\n  'id': '2309.08788'},\n {'from': 'BioInspiredLLM',\n  'to': 'generative artificial intelligence models',\n  'name': 'collaborates with',\n  'research': 'finding',\n  'summary': 'BioInspiredLLM collaborates with other generative AI models in a workflow that can reshape traditional materials design process',\n  'date': '2023-09-15',\n  'title': 'BioinspiredLLM: Conversational Large Language Model for the Mechanics of\\n  Biological and Bio-inspired Materials',\n  'id': '2309.08788'},\n {'from': 'collaborative generative artificial intelligence',\n  'to': 'bio-inspired materials design',\n  'name': 'enhances',\n  'research': 'implication',\n  'summary': 'Collaborative generative artificial intelligence method can stimulate and enhance bio-inspired materials design workflows',\n  'date': '2023-09-15',\n  'title': 'BioinspiredLLM: Conversational Large Language Model for the Mechanics of\\n  Biological and Bio-inspired Materials',\n  'id': '2309.08788'},\n {'from': 'biological materials',\n  'to': 'interdisciplinary science',\n  'name': 'at intersection of',\n  'research': 'context',\n  'summary': 'Biological materials is at a critical intersection of multiple scientific fields',\n  'date': '2023-09-15',\n  'title': 'BioinspiredLLM: Conversational Large Language Model for the Mechanics of\\n  Biological and Bio-inspired Materials',\n  'id': '2309.08788'},\n {'from': 'BioInspiredLLM',\n  'to': 'knowledge domain integration',\n  'name': 'connects',\n  'research': 'implication',\n  'summary': 'Models like BioInspiredLLM help to connect knowledge domains',\n  'date': '2023-09-15',\n  'title': 'BioinspiredLLM: Conversational Large Language Model for the Mechanics of\\n  Biological and Bio-inspired Materials',\n  'id': '2309.08788'},\n {'from': 'music generation',\n  'to': 'hierarchical token stacks',\n  'name': 'uses',\n  'research': 'context',\n  'summary': 'In language modeling based music generation, a generated waveform is represented by a sequence of hierarchical token stacks',\n  'date': '2023-09-15',\n  'title': 'Stack-and-Delay: a new codebook pattern for music generation',\n  'id': '2309.08804'},\n {'from': 'hierarchical token stacks',\n  'to': 'decoding strategies',\n  'name': 'can be decoded',\n  'research': 'context',\n  'summary': 'Hierarchical token stacks can be decoded either in an auto-regressive manner or in parallel',\n  'date': '2023-09-15',\n  'title': 'Stack-and-Delay: a new codebook pattern for music generation',\n  'id': '2309.08804'},\n {'from': 'flattening codebooks',\n  'to': 'high-quality decoding',\n  'name': 'represents',\n  'research': 'context',\n  'summary': 'Flattening the codebooks represents the highest quality decoding strategy',\n  'date': '2023-09-15',\n  'title': 'Stack-and-Delay: a new codebook pattern for music generation',\n  'id': '2309.08804'},\n {'from': 'flattening codebooks',\n  'to': 'slow decoding speed',\n  'name': 'is',\n  'research': 'context',\n  'summary': 'Flattening the codebooks is notoriously slow',\n  'date': '2023-09-15',\n  'title': 'Stack-and-Delay: a new codebook pattern for music generation',\n  'id': '2309.08804'},\n {'from': 'stack-and-delay decoding',\n  'to': 'flattening codebooks',\n  'name': 'proposed to improve',\n  'research': 'contribution',\n  'summary': 'A novel stack-and-delay style of decoding strategy is proposed to improve upon the flat pattern decoding',\n  'date': '2023-09-15',\n  'title': 'Stack-and-Delay: a new codebook pattern for music generation',\n  'id': '2309.08804'},\n {'from': 'stack-and-delay decoding',\n  'to': 'fast decoding speed',\n  'name': 'increases',\n  'research': 'contribution',\n  'summary': 'Generation speed is four times faster as opposed to vanilla flat decoding',\n  'date': '2023-09-15',\n  'title': 'Stack-and-Delay: a new codebook pattern for music generation',\n  'id': '2309.08804'},\n {'from': 'stack-and-delay decoding',\n  'to': 'delay decoding strategy',\n  'name': 'brings inference time close to',\n  'research': 'contribution',\n  'summary': 'The inference time with stack-and-delay decoding is close to that of the delay decoding strategy',\n  'date': '2023-09-15',\n  'title': 'Stack-and-Delay: a new codebook pattern for music generation',\n  'id': '2309.08804'},\n {'from': 'stack-and-delay decoding',\n  'to': 'GPU efficiency',\n  'name': 'allows for',\n  'research': 'contribution',\n  'summary': 'Allows for faster inference on GPU for small batch sizes',\n  'date': '2023-09-15',\n  'title': 'Stack-and-Delay: a new codebook pattern for music generation',\n  'id': '2309.08804'},\n {'from': 'stack-and-delay decoding',\n  'to': 'objective evaluations',\n  'name': 'performs better in',\n  'research': 'finding',\n  'summary': 'For the same inference efficiency budget as the delay pattern, the proposed approach performs better in objective evaluations',\n  'date': '2023-09-15',\n  'title': 'Stack-and-Delay: a new codebook pattern for music generation',\n  'id': '2309.08804'},\n {'from': 'stack-and-delay decoding',\n  'to': 'flattening codebooks',\n  'name': 'closes the gap with',\n  'research': 'finding',\n  'summary': 'Almost closes the gap with the flat pattern in terms of quality',\n  'date': '2023-09-15',\n  'title': 'Stack-and-Delay: a new codebook pattern for music generation',\n  'id': '2309.08804'},\n {'from': 'subjective evaluations',\n  'to': 'stack-and-delay decoding',\n  'name': 'corroborate',\n  'research': 'finding',\n  'summary': 'Subjective evaluations show that samples generated by the new model are slightly more often preferred',\n  'date': '2023-09-15',\n  'title': 'Stack-and-Delay: a new codebook pattern for music generation',\n  'id': '2309.08804'},\n {'from': 'subjective evaluations',\n  'to': 'competing model',\n  'name': 'compared with',\n  'research': 'finding',\n  'summary': 'Samples generated by the new model are compared with samples generated by the competing model given the same text prompts',\n  'date': '2023-09-15',\n  'title': 'Stack-and-Delay: a new codebook pattern for music generation',\n  'id': '2309.08804'},\n {'from': 'dialogue state tracking',\n  'to': 'tracking user preferences and intents',\n  'name': 'aims to',\n  'research': 'context',\n  'summary': 'The traditional DST problem aims to track user preferences and intents in user-agent conversations',\n  'date': '2023-09-16',\n  'title': 'S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking\\n  in the Era of LLMs',\n  'id': '2309.08827'},\n {'from': 'task-oriented dialogue system',\n  'to': 'narrow domain applications',\n  'name': 'supports',\n  'research': 'context',\n  'summary': 'Task-oriented dialogue systems support narrow domain applications',\n  'date': '2023-09-16',\n  'title': 'S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking\\n  in the Era of LLMs',\n  'id': '2309.08827'},\n {'from': 'large language model',\n  'to': 'real-world intricacies in open-domain dialogues',\n  'name': 'introduces',\n  'research': 'context',\n  'summary': 'LLM-based chat systems have introduced many real-world intricacies in open-domain dialogues',\n  'date': '2023-09-16',\n  'title': 'S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking\\n  in the Era of LLMs',\n  'id': '2309.08827'},\n {'from': 'real-world intricacies in open-domain dialogues',\n  'to': 'complex contextual interactions and shifts',\n  'name': 'manifest as',\n  'research': 'context',\n  'summary': 'These intricacies manifest in the form of increased complexity in contextual interactions, extended dialogue sessions encompassing a diverse array of topics, and more frequent contextual shifts',\n  'date': '2023-09-16',\n  'title': 'S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking\\n  in the Era of LLMs',\n  'id': '2309.08827'},\n {'from': 'joint dialogue segmentation and state tracking per segment',\n  'to': 'real-world intricacies in open-domain dialogues',\n  'name': 'proposed to handle',\n  'research': 'contribution',\n  'summary': 'To handle intricacies arising from evolving LLM-based chat systems, joint dialogue segmentation and state tracking per segment is proposed',\n  'date': '2023-09-16',\n  'title': 'S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking\\n  in the Era of LLMs',\n  'id': '2309.08827'},\n {'from': 'zero-shot',\n  'to': 'open-domain dialogue system',\n  'name': 'appropriate for',\n  'research': 'context',\n  'summary': 'Assuming a zero-shot setting appropriate to a true open-domain dialogue system',\n  'date': '2023-09-16',\n  'title': 'S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking\\n  in the Era of LLMs',\n  'id': '2309.08827'},\n {'from': 'S3-DST',\n  'to': 'pre-analytical recollection',\n  'name': 'employs',\n  'research': 'contribution',\n  'summary': 'S3-DST employs structured prompting and pre-analytical recollection for improving long context tracking',\n  'date': '2023-09-16',\n  'title': 'S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking\\n  in the Era of LLMs',\n  'id': '2309.08827'},\n {'from': 'pre-analytical recollection',\n  'to': 'improving long context tracking',\n  'name': 'designed for',\n  'research': 'contribution',\n  'summary': 'Pre-analytical recollection is a novel grounding mechanism designed for improving long context tracking',\n  'date': '2023-09-16',\n  'title': 'S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking\\n  in the Era of LLMs',\n  'id': '2309.08827'},\n {'from': 'S3-DST',\n  'to': 'open-domain dialogue dataset',\n  'name': 'evaluated on',\n  'research': 'method',\n  'summary': 'S3-DST is evaluated on a proprietary anonymized open-domain dialogue dataset, as well as publicly available DST and segmentation datasets',\n  'date': '2023-09-16',\n  'title': 'S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking\\n  in the Era of LLMs',\n  'id': '2309.08827'},\n {'from': 'S3-DST',\n  'to': 'state-of-the-art',\n  'name': 'outperforms',\n  'research': 'finding',\n  'summary': 'S3-DST consistently outperforms the state-of-the-art across all datasets and settings',\n  'date': '2023-09-16',\n  'title': 'S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking\\n  in the Era of LLMs',\n  'id': '2309.08827'},\n {'from': 'chatbot',\n  'to': 'over half a century',\n  'name': 'studied for',\n  'research': 'context',\n  'summary': 'Chatbots have been studied for more than half a century',\n  'date': '2023-09-16',\n  'title': 'Bias and Fairness in Chatbots: An Overview',\n  'id': '2309.08836'},\n {'from': 'chatbot',\n  'to': 'large language model',\n  'name': 'utilizes',\n  'research': 'context',\n  'summary': 'Modern chatbots utilize large language models (LLMs) and have gained much attention',\n  'date': '2023-09-16',\n  'title': 'Bias and Fairness in Chatbots: An Overview',\n  'id': '2309.08836'},\n {'from': 'chatbot',\n  'to': 'traditional chatbot',\n  'name': 'compared to',\n  'research': 'context',\n  'summary': 'Modern chatbots are more powerful compared to traditional ones and are used in real-world applications',\n  'date': '2023-09-16',\n  'title': 'Bias and Fairness in Chatbots: An Overview',\n  'id': '2309.08836'},\n {'from': 'bias and fairness',\n  'to': 'modern chatbot',\n  'name': 'concern in',\n  'research': 'context',\n  'summary': 'There are bias and fairness concerns in modern chatbot design',\n  'date': '2023-09-16',\n  'title': 'Bias and Fairness in Chatbots: An Overview',\n  'id': '2309.08836'},\n {'from': 'bias mitigation and fairness preservation',\n  'to': 'large training data, model size, interpretability',\n  'name': 'is challenging due to',\n  'research': 'context',\n  'summary': 'Bias mitigation and fairness preservation of modern chatbots are challenging due to huge amounts of training data, large model sizes, and lack of interpretability',\n  'date': '2023-09-16',\n  'title': 'Bias and Fairness in Chatbots: An Overview',\n  'id': '2309.08836'},\n {'from': 'this paper',\n  'to': 'bias and fairness in chatbot systems',\n  'name': 'provides',\n  'research': 'contribution',\n  'summary': 'This paper provides a comprehensive overview on bias and fairness in chatbot systems',\n  'date': '2023-09-16',\n  'title': 'Bias and Fairness in Chatbots: An Overview',\n  'id': '2309.08836'},\n {'from': 'this paper',\n  'to': 'history and categories of chatbots',\n  'name': 'reviews',\n  'research': 'method',\n  'summary': 'The history of chatbots and their categories are reviewed',\n  'date': '2023-09-16',\n  'title': 'Bias and Fairness in Chatbots: An Overview',\n  'id': '2309.08836'},\n {'from': 'this paper',\n  'to': 'bias sources and potential harms',\n  'name': 'analyzes',\n  'research': 'method',\n  'summary': 'Bias sources and potential harms in applications are analyzed',\n  'date': '2023-09-16',\n  'title': 'Bias and Fairness in Chatbots: An Overview',\n  'id': '2309.08836'},\n {'from': 'this paper',\n  'to': 'fair and unbiased chatbot systems',\n  'name': 'examines',\n  'research': 'method',\n  'summary': 'Considerations in designing fair and unbiased chatbot systems are examined',\n  'date': '2023-09-16',\n  'title': 'Bias and Fairness in Chatbots: An Overview',\n  'id': '2309.08836'},\n {'from': 'this paper',\n  'to': 'future research directions',\n  'name': 'discusses',\n  'research': 'reflection',\n  'summary': 'Future research directions are discussed',\n  'date': '2023-09-16',\n  'title': 'Bias and Fairness in Chatbots: An Overview',\n  'id': '2309.08836'},\n {'from': 'large language model',\n  'to': 'human-like predictive performance',\n  'name': 'represent',\n  'research': 'context',\n  'summary': 'LLMs represent the recent success of deep learning with human-like predictive performance',\n  'date': '2023-09-16',\n  'title': 'Rethinking Learning Rate Tuning in the Era of Large Language Models',\n  'id': '2309.08859'},\n {'from': 'fine-tuning',\n  'to': 'large language model',\n  'name': 'used to adapt',\n  'research': 'context',\n  'summary': 'Fine-tuning is used to adapt LLMs for various real-world applications',\n  'date': '2023-09-16',\n  'title': 'Rethinking Learning Rate Tuning in the Era of Large Language Models',\n  'id': '2309.08859'},\n {'from': 'learning rate',\n  'to': 'LLM fine-tuning',\n  'name': 'impacts',\n  'research': 'context',\n  'summary': 'The learning rate is a critical hyperparameter in LLM fine-tuning affecting efficiency and quality',\n  'date': '2023-09-16',\n  'title': 'Rethinking Learning Rate Tuning in the Era of Large Language Models',\n  'id': '2309.08859'},\n {'from': 'existing learning rate policies',\n  'to': 'traditional deep neural networks',\n  'name': 'designed for',\n  'research': 'context',\n  'summary': 'Existing learning rate policies are primarily designed for traditional DNN training',\n  'date': '2023-09-16',\n  'title': 'Rethinking Learning Rate Tuning in the Era of Large Language Models',\n  'id': '2309.08859'},\n {'from': 'learning rate tuning',\n  'to': 'era of large language models',\n  'name': 'reassessed for',\n  'research': 'contribution',\n  'summary': 'The paper reassesses the challenges and opportunities of learning rate tuning for LLMs',\n  'date': '2023-09-16',\n  'title': 'Rethinking Learning Rate Tuning in the Era of Large Language Models',\n  'id': '2309.08859'},\n {'from': 'LRBench++',\n  'to': 'learning rate tuning',\n  'name': 'presented to',\n  'research': 'contribution',\n  'summary': 'LRBench++ is presented to benchmark learning rate policies and facilitate tuning for DNNs and LLMs',\n  'date': '2023-09-16',\n  'title': 'Rethinking Learning Rate Tuning in the Era of Large Language Models',\n  'id': '2309.08859'},\n {'from': 'experimental analysis with LRBench++',\n  'to': 'LLM fine-tuning vs traditional DNN training',\n  'name': 'demonstrates',\n  'research': 'finding',\n  'summary': 'Experimental analysis with LRBench++ shows key differences between LLM fine-tuning and traditional DNN training',\n  'date': '2023-09-16',\n  'title': 'Rethinking Learning Rate Tuning in the Era of Large Language Models',\n  'id': '2309.08859'},\n {'from': 'ICD coding',\n  'to': 'assigning ICD diagnosis codes',\n  'name': 'is',\n  'research': 'context',\n  'summary': 'ICD coding is the task of assigning ICD diagnosis codes to clinical notes',\n  'date': '2023-09-16',\n  'title': 'MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding',\n  'id': '2309.08868'},\n {'from': 'ICD coding',\n  'to': 'large quantity of labels and lengthy texts',\n  'name': 'challenged by',\n  'research': 'context',\n  'summary': 'ICD coding is challenging given the large quantity of labels and lengthy texts',\n  'date': '2023-09-16',\n  'title': 'MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding',\n  'id': '2309.08868'},\n {'from': 'large quantity of labels and lengthy texts',\n  'to': '9,000 labels and 8,000 tokens',\n  'name': 'consists of',\n  'research': 'context',\n  'summary': 'ICD coding involves nearly 9,000 labels and texts up to 8,000 tokens',\n  'date': '2023-09-16',\n  'title': 'MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding',\n  'id': '2309.08868'},\n {'from': 'single-pass reading',\n  'to': 'multi-pass reading',\n  'name': 'contrasted with',\n  'research': 'context',\n  'summary': 'Humans tend to read the text and label definitions again unlike the single-pass reading process in previous works',\n  'date': '2023-09-16',\n  'title': 'MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding',\n  'id': '2309.08868'},\n {'from': 'pretrained language model',\n  'to': 'huge memory usage',\n  'name': 'suffers from',\n  'research': 'problem',\n  'summary': 'Pretrained language models suffer from huge memory usage',\n  'date': '2023-09-16',\n  'title': 'MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding',\n  'id': '2309.08868'},\n {'from': 'Multi-Hop Label-Wise Attention (MHLAT)',\n  'to': 'ICD coding',\n  'name': 'proposed to address',\n  'research': 'contribution',\n  'summary': 'MHLAT is proposed to address the problems of precision and memory usage in ICD coding',\n  'date': '2023-09-16',\n  'title': 'MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding',\n  'id': '2309.08868'},\n {'from': 'multi-hop label-wise attention',\n  'to': 'Multi-Hop Label-Wise Attention (MHLAT)',\n  'name': 'used in',\n  'research': 'method',\n  'summary': 'Multi-hop label-wise attention is deployed in MHLAT to get more precise and informative representations',\n  'date': '2023-09-16',\n  'title': 'MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding',\n  'id': '2309.08868'},\n {'from': 'Multi-Hop Label-Wise Attention (MHLAT)',\n  'to': 'MIMIC datasets',\n  'name': 'performs on',\n  'research': 'finding',\n  'summary': 'MHLAT achieves significantly better or competitive performance on all seven metrics on three benchmark MIMIC datasets',\n  'date': '2023-09-16',\n  'title': 'MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding',\n  'id': '2309.08868'},\n {'from': 'Multi-Hop Label-Wise Attention (MHLAT)',\n  'to': 'fewer parameters',\n  'name': 'has advantage of',\n  'research': 'finding',\n  'summary': 'MHLAT has much fewer parameters to optimize compared to other models',\n  'date': '2023-09-16',\n  'title': 'MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding',\n  'id': '2309.08868'},\n {'from': 'context length limitation',\n  'to': 'document question answering',\n  'name': 'affects',\n  'research': 'context',\n  'summary': 'LLMs have issues with document QA when the document cannot fit in the small context length of an LLM',\n  'date': '2023-09-16',\n  'title': 'PDFTriage: Question Answering over Long, Structured Documents',\n  'id': '2309.08872'},\n {'from': 'context retrieval',\n  'to': 'document question answering',\n  'name': 'focus of',\n  'research': 'context',\n  'summary': 'Most existing works focus on retrieving relevant context from the document as plain text',\n  'date': '2023-09-16',\n  'title': 'PDFTriage: Question Answering over Long, Structured Documents',\n  'id': '2309.08872'},\n {'from': 'structured document',\n  'to': 'plain text',\n  'name': 'represented as',\n  'research': 'problem',\n  'summary': \"Structured documents are incongruously represented as plain text, which is at odds with the user's mental model of these documents\",\n  'date': '2023-09-16',\n  'title': 'PDFTriage: Question Answering over Long, Structured Documents',\n  'id': '2309.08872'},\n {'from': 'incongruity',\n  'to': 'document question answering',\n  'name': 'caused by',\n  'research': 'problem',\n  'summary': 'The incongruity of representing structured documents as plain text can cause issues for QA systems when querying for context',\n  'date': '2023-09-16',\n  'title': 'PDFTriage: Question Answering over Long, Structured Documents',\n  'id': '2309.08872'},\n {'from': 'PDFtriage',\n  'to': 'context retrieval',\n  'name': 'proposed to',\n  'research': 'contribution',\n  'summary': 'PDFtriage is proposed to enable models to retrieve context based on either structure or content',\n  'date': '2023-09-16',\n  'title': 'PDFTriage: Question Answering over Long, Structured Documents',\n  'id': '2309.08872'},\n {'from': 'PDFtriage-augmented large language model',\n  'to': 'document question answering',\n  'name': 'demonstrates effectiveness',\n  'research': 'finding',\n  'summary': 'PDFtriage-augmented models demonstrate effectiveness across several classes of questions where existing retrieval-augmented LLMs fail',\n  'date': '2023-09-16',\n  'title': 'PDFTriage: Question Answering over Long, Structured Documents',\n  'id': '2309.08872'},\n {'from': 'benchmark dataset',\n  'to': 'document question answering',\n  'name': 'released for',\n  'research': 'contribution',\n  'summary': 'A benchmark dataset consisting of 900+ human-generated questions over 80 structured documents from 10 different categories is released for further research on document QA',\n  'date': '2023-09-16',\n  'title': 'PDFTriage: Question Answering over Long, Structured Documents',\n  'id': '2309.08872'},\n {'from': 'code and datasets',\n  'to': 'GitHub',\n  'name': 'will be released on',\n  'research': 'contribution',\n  'summary': 'The code and datasets for the PDFtriage approach will be released soon on GitHub',\n  'date': '2023-09-16',\n  'title': 'PDFTriage: Question Answering over Long, Structured Documents',\n  'id': '2309.08872'},\n {'from': 'textual information equivalence',\n  'to': 'Natural Language Processing (NLP)',\n  'name': 'is a goal in',\n  'research': 'context',\n  'summary': 'Understanding when two pieces of text convey the same information is a goal in NLP',\n  'date': '2023-09-16',\n  'title': 'X-PARADE: Cross-Lingual Textual Entailment and Information Divergence\\n  across Paragraphs',\n  'id': '2309.08873'},\n {'from': 'textual entailment',\n  'to': 'textual information equivalence',\n  'name': 'is a subproblem of',\n  'research': 'context',\n  'summary': 'Textual entailment is one of the subproblems in understanding textual information equivalence',\n  'date': '2023-09-16',\n  'title': 'X-PARADE: Cross-Lingual Textual Entailment and Information Divergence\\n  across Paragraphs',\n  'id': '2309.08873'},\n {'from': 'fact-checking',\n  'to': 'textual information equivalence',\n  'name': 'is a subproblem of',\n  'research': 'context',\n  'summary': 'Fact-checking is one of the subproblems in understanding textual information equivalence',\n  'date': '2023-09-16',\n  'title': 'X-PARADE: Cross-Lingual Textual Entailment and Information Divergence\\n  across Paragraphs',\n  'id': '2309.08873'},\n {'from': 'X-Parade',\n  'to': 'cross-lingual information divergence analysis',\n  'name': 'introduced for',\n  'research': 'contribution',\n  'summary': 'X-Parade is introduced as the first cross-lingual dataset for paragraph-level information divergences',\n  'date': '2023-09-16',\n  'title': 'X-PARADE: Cross-Lingual Textual Entailment and Information Divergence\\n  across Paragraphs',\n  'id': '2309.08873'},\n {'from': 'Wikipedia',\n  'to': 'X-Parade',\n  'name': 'provides data for',\n  'research': 'method',\n  'summary': 'Aligned paragraphs for X-Parade are sourced from Wikipedia pages in different languages',\n  'date': '2023-09-16',\n  'title': 'X-PARADE: Cross-Lingual Textual Entailment and Information Divergence\\n  across Paragraphs',\n  'id': '2309.08873'},\n {'from': 'token alignment from machine translation',\n  'to': 'cross-lingual information divergence analysis',\n  'name': 'investigated for',\n  'research': 'method',\n  'summary': 'Token alignment from machine translation is investigated for handling cross-lingual information divergence',\n  'date': '2023-09-16',\n  'title': 'X-PARADE: Cross-Lingual Textual Entailment and Information Divergence\\n  across Paragraphs',\n  'id': '2309.08873'},\n {'from': 'textual entailment methods',\n  'to': 'cross-lingual information divergence analysis',\n  'name': 'investigated for',\n  'research': 'method',\n  'summary': 'Textual entailment methods that localize their decisions are investigated for handling cross-lingual information divergence',\n  'date': '2023-09-16',\n  'title': 'X-PARADE: Cross-Lingual Textual Entailment and Information Divergence\\n  across Paragraphs',\n  'id': '2309.08873'},\n {'from': 'prompting of large language models',\n  'to': 'cross-lingual information divergence analysis',\n  'name': 'investigated for',\n  'research': 'method',\n  'summary': 'Prompting of large language models is investigated for handling cross-lingual information divergence',\n  'date': '2023-09-16',\n  'title': 'X-PARADE: Cross-Lingual Textual Entailment and Information Divergence\\n  across Paragraphs',\n  'id': '2309.08873'},\n {'from': 'method performance comparison',\n  'to': 'cross-lingual information divergence analysis',\n  'name': 'shows',\n  'research': 'finding',\n  'summary': 'Results show that methods vary in handling inferable information but fall short of human performance',\n  'date': '2023-09-16',\n  'title': 'X-PARADE: Cross-Lingual Textual Entailment and Information Divergence\\n  across Paragraphs',\n  'id': '2309.08873'},\n {'from': 'human performance',\n  'to': 'cross-lingual information divergence analysis',\n  'name': 'compared to',\n  'research': 'finding',\n  'summary': 'All investigated methods fall short of human performance in handling cross-lingual information divergence',\n  'date': '2023-09-16',\n  'title': 'X-PARADE: Cross-Lingual Textual Entailment and Information Divergence\\n  across Paragraphs',\n  'id': '2309.08873'},\n {'from': 'audio-text pair',\n  'to': 'data collection',\n  'name': 'is expensive to collect',\n  'research': 'context',\n  'summary': 'Collecting audio-text pairs is expensive',\n  'date': '2023-09-16',\n  'title': 'Decoder-only Architecture for Speech Recognition with CTC Prompts and\\n  Text Data Augmentation',\n  'id': '2309.08876'},\n {'from': 'text-only data',\n  'to': 'data collection',\n  'name': 'is easier to access',\n  'research': 'context',\n  'summary': 'It is much easier to access text-only data',\n  'date': '2023-09-16',\n  'title': 'Decoder-only Architecture for Speech Recognition with CTC Prompts and\\n  Text Data Augmentation',\n  'id': '2309.08876'},\n {'from': 'end-to-end automatic speech recognition',\n  'to': 'text-only data',\n  'name': 'requires modifications for',\n  'research': 'context',\n  'summary': 'End-to-end ASR models require architecture modifications or additional training schemes to use text-only data unless using shallow fusion',\n  'date': '2023-09-16',\n  'title': 'Decoder-only Architecture for Speech Recognition with CTC Prompts and\\n  Text Data Augmentation',\n  'id': '2309.08876'},\n {'from': 'decoder-only language model',\n  'to': 'speech-processing',\n  'name': 'inspired by',\n  'research': 'context',\n  'summary': 'Recent advances in decoder-only LMs such as GPT-3 and PaLM are adopted for speech-processing tasks',\n  'date': '2023-09-16',\n  'title': 'Decoder-only Architecture for Speech Recognition with CTC Prompts and\\n  Text Data Augmentation',\n  'id': '2309.08876'},\n {'from': 'GPT-3',\n  'to': 'decoder-only language model',\n  'name': 'used as inspiration for',\n  'research': 'context',\n  'summary': \"GPT-3's decoder-only architecture inspired the proposed ASR model\",\n  'date': '2023-09-16',\n  'title': 'Decoder-only Architecture for Speech Recognition with CTC Prompts and\\n  Text Data Augmentation',\n  'id': '2309.08876'},\n {'from': 'PaLM',\n  'to': 'decoder-only language model',\n  'name': 'used as inspiration for',\n  'research': 'context',\n  'summary': \"PaLM's decoder-only architecture inspired the proposed ASR model\",\n  'date': '2023-09-16',\n  'title': 'Decoder-only Architecture for Speech Recognition with CTC Prompts and\\n  Text Data Augmentation',\n  'id': '2309.08876'},\n {'from': 'decoder-only language model',\n  'to': 'automatic speech recognition',\n  'name': 'proposed for',\n  'research': 'contribution',\n  'summary': 'A decoder-only architecture for ASR with simple text augmentation is proposed',\n  'date': '2023-09-16',\n  'title': 'Decoder-only Architecture for Speech Recognition with CTC Prompts and\\n  Text Data Augmentation',\n  'id': '2309.08876'},\n {'from': 'CTC prediction compression',\n  'to': 'decoder',\n  'name': 'used as',\n  'research': 'method',\n  'summary': 'Encoder features compressed by CTC prediction are used as prompts for the decoder',\n  'date': '2023-09-16',\n  'title': 'Decoder-only Architecture for Speech Recognition with CTC Prompts and\\n  Text Data Augmentation',\n  'id': '2309.08876'},\n {'from': 'decoder-only language model',\n  'to': 'external text data',\n  'name': 'enhanced by',\n  'research': 'method',\n  'summary': 'The decoder-only architecture is enhanced by leveraging external text data with LM training',\n  'date': '2023-09-16',\n  'title': 'Decoder-only Architecture for Speech Recognition with CTC Prompts and\\n  Text Data Augmentation',\n  'id': '2309.08876'},\n {'from': 'LibriSpeech',\n  'to': 'automatic speech recognition',\n  'name': 'used in',\n  'research': 'experiment',\n  'summary': 'An experimental comparison using LibriSpeech shows that the proposed models with text augmentation training reduced word error rates',\n  'date': '2023-09-16',\n  'title': 'Decoder-only Architecture for Speech Recognition with CTC Prompts and\\n  Text Data Augmentation',\n  'id': '2309.08876'},\n {'from': 'Switchboard',\n  'to': 'automatic speech recognition',\n  'name': 'used in',\n  'research': 'experiment',\n  'summary': 'An experimental comparison using Switchboard shows that the proposed models with text augmentation training reduced word error rates',\n  'date': '2023-09-16',\n  'title': 'Decoder-only Architecture for Speech Recognition with CTC Prompts and\\n  Text Data Augmentation',\n  'id': '2309.08876'},\n {'from': 'automatic speech recognition',\n  'to': 'encoder-decoder automatic speech recognition',\n  'name': 'outperformed by',\n  'research': 'finding',\n  'summary': 'The proposed model outperformed conventional encoder-decoder ASR models on the LibriSpeech 100h and Switchboard training scenarios',\n  'date': '2023-09-16',\n  'title': 'Decoder-only Architecture for Speech Recognition with CTC Prompts and\\n  Text Data Augmentation',\n  'id': '2309.08876'},\n {'from': 'automatic speech recognition',\n  'to': 'computational efficiency',\n  'name': 'has advantage in',\n  'research': 'finding',\n  'summary': 'The proposed model had advantage on computational efficiency compared with conventional encoder-decoder ASR models',\n  'date': '2023-09-16',\n  'title': 'Decoder-only Architecture for Speech Recognition with CTC Prompts and\\n  Text Data Augmentation',\n  'id': '2309.08876'},\n {'from': 'large language model',\n  'to': 'user assistance',\n  'name': 'used in',\n  'research': 'context',\n  'summary': 'LLMs are increasingly powerful and widely used to assist users in a variety of tasks',\n  'date': '2023-09-16',\n  'title': 'Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and\\n  Nationality Bias in Generative Models',\n  'id': '2309.08902'},\n {'from': 'introduction of bias',\n  'to': 'large language model',\n  'name': 'associated with',\n  'research': 'context',\n  'summary': 'The use of LLMs risks the introduction of biases to consequential decisions',\n  'date': '2023-09-16',\n  'title': 'Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and\\n  Nationality Bias in Generative Models',\n  'id': '2309.08902'},\n {'from': 'LLM bias',\n  'to': 'consequential decisions',\n  'name': 'affects',\n  'research': 'context',\n  'summary': 'LLM biases can affect consequential decisions such as job hiring, human performance evaluation, and criminal sentencing',\n  'date': '2023-09-16',\n  'title': 'Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and\\n  Nationality Bias in Generative Models',\n  'id': '2309.08902'},\n {'from': 'gender and ethnicity bias',\n  'to': 'Natural Language Processing (NLP)',\n  'name': 'studied in',\n  'research': 'context',\n  'summary': 'Bias in NLP systems along the lines of gender and ethnicity has been widely studied',\n  'date': '2023-09-16',\n  'title': 'Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and\\n  Nationality Bias in Generative Models',\n  'id': '2309.08902'},\n {'from': 'age and beauty bias',\n  'to': 'large language model',\n  'name': 'investigated in',\n  'research': 'contribution',\n  'summary': 'This paper investigates bias along less studied dimensions, such as age and beauty',\n  'date': '2023-09-16',\n  'title': 'Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and\\n  Nationality Bias in Generative Models',\n  'id': '2309.08902'},\n {'from': 'autoregressive language model',\n  'to': 'social group bias',\n  'name': 'analyzed for',\n  'research': 'method',\n  'summary': 'Autoregressive language models are analyzed for biases between social groups and unrelated attributes',\n  'date': '2023-09-16',\n  'title': 'Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and\\n  Nationality Bias in Generative Models',\n  'id': '2309.08902'},\n {'from': 'what is beautiful is good',\n  'to': 'large language model',\n  'name': 'compared to',\n  'research': 'context',\n  'summary': \"LLMs are questioned for biases similar to the 'what is beautiful is good' bias found in experimental psychology\",\n  'date': '2023-09-16',\n  'title': 'Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and\\n  Nationality Bias in Generative Models',\n  'id': '2309.08902'},\n {'from': 'template-generated dataset',\n  'to': 'bias measurement',\n  'name': 'created for',\n  'research': 'method',\n  'summary': 'A template-generated dataset of sentence completion tasks is introduced to measure LLM biases',\n  'date': '2023-09-16',\n  'title': 'Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and\\n  Nationality Bias in Generative Models',\n  'id': '2309.08902'},\n {'from': 'templating',\n  'to': 'template-generated dataset',\n  'name': 'used to',\n  'research': 'method',\n  'summary': 'The templating technique is used to expand the benchmark with minimal additional human annotation',\n  'date': '2023-09-16',\n  'title': 'Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and\\n  Nationality Bias in Generative Models',\n  'id': '2309.08902'},\n {'from': 'cutting-edge large language model',\n  'to': 'template-generated dataset',\n  'name': 'evaluated with',\n  'research': 'method',\n  'summary': 'Multiple cutting-edge LLMs are evaluated using the dataset for generalized biases',\n  'date': '2023-09-16',\n  'title': 'Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and\\n  Nationality Bias in Generative Models',\n  'id': '2309.08902'},\n {'from': 'fine-grained visual classification (FGVC)',\n  'to': 'subtle inter-class discrepancies and large intra-class variations',\n  'name': 'challenges',\n  'research': 'context',\n  'summary': 'FGVC involves categorizing fine subdivisions within a broader category, which poses challenges due to subtle inter-class discrepancies and large intra-class variations',\n  'date': '2023-09-16',\n  'title': 'Delving into Multimodal Prompting for Fine-grained Visual Classification',\n  'id': '2309.08912'},\n {'from': 'prevailing FGVC approaches',\n  'to': 'uni-modal visual',\n  'name': 'focus on',\n  'research': 'context',\n  'summary': 'Prevailing approaches in FGVC primarily focus on uni-modal visual concepts',\n  'date': '2023-09-16',\n  'title': 'Delving into Multimodal Prompting for Fine-grained Visual Classification',\n  'id': '2309.08912'},\n {'from': 'pre-trained vision-language model',\n  'to': 'high-level vision tasks',\n  'name': 'demonstrates performance in',\n  'research': 'context',\n  'summary': 'Recent advancements in pre-trained vision-language models have demonstrated remarkable performance in various high-level vision tasks',\n  'date': '2023-09-16',\n  'title': 'Delving into Multimodal Prompting for Fine-grained Visual Classification',\n  'id': '2309.08912'},\n {'from': 'pre-trained vision-language model',\n  'to': 'FGVC tasks',\n  'name': 'applicability uncertain for',\n  'research': 'context',\n  'summary': 'The applicability of pre-trained vision-language models to FGVC tasks remains uncertain',\n  'date': '2023-09-16',\n  'title': 'Delving into Multimodal Prompting for Fine-grained Visual Classification',\n  'id': '2309.08912'},\n {'from': 'multimodal prompting solution',\n  'to': 'FGVC tasks',\n  'name': 'proposed for',\n  'research': 'contribution',\n  'summary': 'A novel multimodal prompting solution, denoted as MP-FGVC, is proposed to tackle FGVC tasks',\n  'date': '2023-09-16',\n  'title': 'Delving into Multimodal Prompting for Fine-grained Visual Classification',\n  'id': '2309.08912'},\n {'from': 'MP-FGVC',\n  'to': 'CLIP',\n  'name': 'based on',\n  'research': 'contribution',\n  'summary': 'MP-FGVC is based on the Contrastive Language-Image Pertaining (CLIP) model',\n  'date': '2023-09-16',\n  'title': 'Delving into Multimodal Prompting for Fine-grained Visual Classification',\n  'id': '2309.08912'},\n {'from': 'multimodal prompts scheme',\n  'to': 'SSVP and DATP',\n  'name': 'includes',\n  'research': 'contribution',\n  'summary': 'The multimodal prompts scheme includes Subcategory-Specific Vision Prompt (SSVP) and Discrepancy-Aware Text Prompt (DATP)',\n  'date': '2023-09-16',\n  'title': 'Delving into Multimodal Prompting for Fine-grained Visual Classification',\n  'id': '2309.08912'},\n {'from': 'multimodal adaptation scheme',\n  'to': 'common semantic space',\n  'name': 'aligns',\n  'research': 'contribution',\n  'summary': 'The multimodal adaptation scheme aligns the vision and text prompting elements in a common semantic space',\n  'date': '2023-09-16',\n  'title': 'Delving into Multimodal Prompting for Fine-grained Visual Classification',\n  'id': '2309.08912'},\n {'from': 'vision-language fusion module (VLFM)',\n  'to': 'FGVC',\n  'name': 'facilitates',\n  'research': 'contribution',\n  'summary': 'VLFM facilitates cross-modal collaborative reasoning for further improvement on FGVC',\n  'date': '2023-09-16',\n  'title': 'Delving into Multimodal Prompting for Fine-grained Visual Classification',\n  'id': '2309.08912'},\n {'from': 'two-stage optimization strategy',\n  'to': 'MP-FGVC',\n  'name': 'tailored for',\n  'research': 'method',\n  'summary': 'A two-stage optimization strategy is tailored for MP-FGVC to leverage the pre-trained CLIP model and expedite efficient adaptation for FGVC',\n  'date': '2023-09-16',\n  'title': 'Delving into Multimodal Prompting for Fine-grained Visual Classification',\n  'id': '2309.08912'},\n {'from': 'extensive experiments',\n  'to': 'FGVC datasets',\n  'name': 'conducted on',\n  'research': 'validation',\n  'summary': 'Extensive experiments conducted on four FGVC datasets demonstrate the effectiveness of MP-FGVC',\n  'date': '2023-09-16',\n  'title': 'Delving into Multimodal Prompting for Fine-grained Visual Classification',\n  'id': '2309.08912'},\n {'from': 'tool-interacting divide-and-conquer',\n  'to': 'answering complex multimodal multi-hop questions',\n  'name': 'enables',\n  'research': 'contribution',\n  'summary': 'A tool-interacting divide-and-conquer strategy enables LLMs to answer complex multimodal multi-hop questions',\n  'date': '2023-09-16',\n  'title': 'Multimodal Multi-Hop Question Answering Through a Conversation Between\\n  Tools and Efficiently Finetuned Large Language Models',\n  'id': '2309.08922'},\n {'from': 'large language model',\n  'to': 'dividing multimodal multi-hop questions',\n  'name': 'utilizes',\n  'research': 'method',\n  'summary': 'LLMs utilize the power to divide a given multimodal multi-hop question into unimodal single-hop sub-questions',\n  'date': '2023-09-16',\n  'title': 'Multimodal Multi-Hop Question Answering Through a Conversation Between\\n  Tools and Efficiently Finetuned Large Language Models',\n  'id': '2309.08922'},\n {'from': 'dividing multimodal multi-hop questions',\n  'to': 'answering unimodal single-hop sub-questions',\n  'name': 'results in',\n  'research': 'method',\n  'summary': 'Dividing multimodal multi-hop questions results in unimodal single-hop sub-questions',\n  'date': '2023-09-16',\n  'title': 'Multimodal Multi-Hop Question Answering Through a Conversation Between\\n  Tools and Efficiently Finetuned Large Language Models',\n  'id': '2309.08922'},\n {'from': 'predefined tools',\n  'to': 'answering unimodal single-hop sub-questions',\n  'name': 'used to answer',\n  'research': 'method',\n  'summary': 'A predefined set of tools is used to answer the unimodal single-hop sub-questions',\n  'date': '2023-09-16',\n  'title': 'Multimodal Multi-Hop Question Answering Through a Conversation Between\\n  Tools and Efficiently Finetuned Large Language Models',\n  'id': '2309.08922'},\n {'from': 'ChatGPT',\n  'to': 'tool-interacting divide-and-conquer dataset',\n  'name': 'prompted to generate',\n  'research': 'method',\n  'summary': 'ChatGPT is prompted to generate a tool-interacting divide-and-conquer dataset',\n  'date': '2023-09-16',\n  'title': 'Multimodal Multi-Hop Question Answering Through a Conversation Between\\n  Tools and Efficiently Finetuned Large Language Models',\n  'id': '2309.08922'},\n {'from': 'tool-interacting divide-and-conquer dataset',\n  'to': 'finetuning large language models',\n  'name': 'used for',\n  'research': 'method',\n  'summary': 'The tool-interacting divide-and-conquer dataset is used to efficiently finetune the corresponding LLM',\n  'date': '2023-09-16',\n  'title': 'Multimodal Multi-Hop Question Answering Through a Conversation Between\\n  Tools and Efficiently Finetuned Large Language Models',\n  'id': '2309.08922'},\n {'from': 'effectiveness assessment',\n  'to': 'complex question-answering datasets',\n  'name': 'conducted on',\n  'research': 'method',\n  'summary': 'An evaluation is conducted on two recently introduced complex question-answering datasets',\n  'date': '2023-09-16',\n  'title': 'Multimodal Multi-Hop Question Answering Through a Conversation Between\\n  Tools and Efficiently Finetuned Large Language Models',\n  'id': '2309.08922'},\n {'from': 'experimental analysis',\n  'to': 'improvements over state-of-the-art',\n  'name': 'demonstrates',\n  'research': 'finding',\n  'summary': 'Experimental analysis demonstrates substantial improvements over existing state-of-the-art solutions',\n  'date': '2023-09-16',\n  'title': 'Multimodal Multi-Hop Question Answering Through a Conversation Between\\n  Tools and Efficiently Finetuned Large Language Models',\n  'id': '2309.08922'},\n {'from': 'tool-interacting divide-and-conquer',\n  'to': 'efficacy and generality',\n  'name': 'indicates',\n  'research': 'finding',\n  'summary': 'The efficacy and generality of the tool-interacting divide-and-conquer strategy is indicated by substantial improvements',\n  'date': '2023-09-16',\n  'title': 'Multimodal Multi-Hop Question Answering Through a Conversation Between\\n  Tools and Efficiently Finetuned Large Language Models',\n  'id': '2309.08922'},\n {'from': 'noisy web image-text dataset',\n  'to': 'vision-language model',\n  'name': 'efficient for',\n  'research': 'context',\n  'summary': 'Noisy web image-text datasets are efficient for learning robust vision-language models',\n  'date': '2023-09-16',\n  'title': 'In-Style: Bridging Text and Uncurated Videos with Style Transfer for\\n  Text-Video Retrieval',\n  'id': '2309.08928'},\n {'from': 'video retrieval',\n  'to': 'hand-curated paired text-video data',\n  'name': 'requires fine-tuning on',\n  'research': 'challenge',\n  'summary': 'Models need to be fine-tuned on hand-curated paired text-video data for video retrieval',\n  'date': '2023-09-16',\n  'title': 'In-Style: Bridging Text and Uncurated Videos with Style Transfer for\\n  Text-Video Retrieval',\n  'id': '2309.08928'},\n {'from': 'text-video retrieval with uncurated & unpaired data',\n  'to': 'video retrieval',\n  'name': 'proposed for',\n  'research': 'contribution',\n  'summary': 'A new setting is proposed that utilizes only text queries with uncurated web videos without any paired text-video data during training',\n  'date': '2023-09-16',\n  'title': 'In-Style: Bridging Text and Uncurated Videos with Style Transfer for\\n  Text-Video Retrieval',\n  'id': '2309.08928'},\n {'from': 'In-Style',\n  'to': 'uncurated web videos',\n  'name': 'learns and transfers',\n  'research': 'contribution',\n  'summary': 'In-Style learns the style of text queries and transfers it to uncurated web videos',\n  'date': '2023-09-16',\n  'title': 'In-Style: Bridging Text and Uncurated Videos with Style Transfer for\\n  Text-Video Retrieval',\n  'id': '2309.08928'},\n {'from': 'In-Style',\n  'to': 'multiple text styles',\n  'name': 'allows training with',\n  'research': 'contribution',\n  'summary': 'In-Style allows one model to be trained with multiple text styles',\n  'date': '2023-09-16',\n  'title': 'In-Style: Bridging Text and Uncurated Videos with Style Transfer for\\n  Text-Video Retrieval',\n  'id': '2309.08928'},\n {'from': 'multi-style contrastive training',\n  'to': 'model generalizability',\n  'name': 'introduced to improve',\n  'research': 'contribution',\n  'summary': 'A multi-style contrastive training procedure is introduced to improve generalizability over several datasets',\n  'date': '2023-09-16',\n  'title': 'In-Style: Bridging Text and Uncurated Videos with Style Transfer for\\n  Text-Video Retrieval',\n  'id': '2309.08928'},\n {'from': 'In-Style',\n  'to': 'text-video retrieval',\n  'name': 'evaluated for',\n  'research': 'evaluation',\n  'summary': 'The In-Style model is evaluated on retrieval performance over multiple datasets',\n  'date': '2023-09-16',\n  'title': 'In-Style: Bridging Text and Uncurated Videos with Style Transfer for\\n  Text-Video Retrieval',\n  'id': '2309.08928'},\n {'from': 'In-Style',\n  'to': 'zero-shot text-video retrieval',\n  'name': 'improves',\n  'research': 'finding',\n  'summary': 'In-Style improves state-of-the-art performance on zero-shot text-video retrieval',\n  'date': '2023-09-16',\n  'title': 'In-Style: Bridging Text and Uncurated Videos with Style Transfer for\\n  Text-Video Retrieval',\n  'id': '2309.08928'},\n {'from': 'cross-lingual transfer',\n  'to': 'translating training data',\n  'name': 'benefits from',\n  'research': 'context',\n  'summary': 'Translating training data into target languages has proven beneficial for cross-lingual transfer',\n  'date': '2023-09-16',\n  'title': 'Contextual Label Projection for Cross-Lingual Structure Extraction',\n  'id': '2309.08943'},\n {'from': 'structure extraction',\n  'to': 'label projection',\n  'name': 'requires',\n  'research': 'context',\n  'summary': 'For structure extraction tasks, translating data requires a label projection step',\n  'date': '2023-09-16',\n  'title': 'Contextual Label Projection for Cross-Lingual Structure Extraction',\n  'id': '2309.08943'},\n {'from': 'label projection',\n  'to': 'translating training data',\n  'name': 'involves',\n  'research': 'context',\n  'summary': 'Label projection translates input text and obtains translated labels in the translated text jointly',\n  'date': '2023-09-16',\n  'title': 'Contextual Label Projection for Cross-Lingual Structure Extraction',\n  'id': '2309.08943'},\n {'from': 'previous research in label projection',\n  'to': 'translation quality',\n  'name': 'compromises',\n  'research': 'problem',\n  'summary': 'Previous research in label projection compromises translation quality for label identification or uses word-level alignment',\n  'date': '2023-09-16',\n  'title': 'Contextual Label Projection for Cross-Lingual Structure Extraction',\n  'id': '2309.08943'},\n {'from': 'CLAP',\n  'to': 'contextual translation',\n  'name': 'introduced in',\n  'research': 'contribution',\n  'summary': 'CLAP is introduced, which translates text and performs contextual translation on labels using the translated text as context',\n  'date': '2023-09-16',\n  'title': 'Contextual Label Projection for Cross-Lingual Structure Extraction',\n  'id': '2309.08943'},\n {'from': 'instruction-tuned language model',\n  'to': 'contextual translation',\n  'name': 'used as',\n  'research': 'method',\n  'summary': 'Instruction-tuned language models with multilingual capabilities are used as contextual translators in CLAP',\n  'date': '2023-09-16',\n  'title': 'Contextual Label Projection for Cross-Lingual Structure Extraction',\n  'id': '2309.08943'},\n {'from': 'CLAP',\n  'to': 'translated labels presence',\n  'name': 'imposes constraint',\n  'research': 'method',\n  'summary': 'CLAP imposes the constraint of the presence of translated labels in the translated text via instructions',\n  'date': '2023-09-16',\n  'title': 'Contextual Label Projection for Cross-Lingual Structure Extraction',\n  'id': '2309.08943'},\n {'from': 'CLAP',\n  'to': 'creating pseudo-training data',\n  'name': 'compared with',\n  'research': 'evaluation',\n  'summary': 'CLAP is compared with other label projection techniques for creating pseudo-training data in target languages',\n  'date': '2023-09-16',\n  'title': 'Contextual Label Projection for Cross-Lingual Structure Extraction',\n  'id': '2309.08943'},\n {'from': 'event argument extraction',\n  'to': 'CLAP',\n  'name': 'used for comparison',\n  'research': 'evaluation',\n  'summary': 'Event argument extraction is used as a representative structure extraction task to compare CLAP with other methods',\n  'date': '2023-09-16',\n  'title': 'Contextual Label Projection for Cross-Lingual Structure Extraction',\n  'id': '2309.08943'},\n {'from': 'CLAP',\n  'to': 'F1-score',\n  'name': 'improves over',\n  'research': 'finding',\n  'summary': 'CLAP improves by 2-2.5 F1-score over other methods on the Chinese and Arabic ACE05 datasets',\n  'date': '2023-09-16',\n  'title': 'Contextual Label Projection for Cross-Lingual Structure Extraction',\n  'id': '2309.08943'},\n {'from': 'large language model',\n  'to': 'question answering',\n  'name': 'enhances',\n  'research': 'context',\n  'summary': 'The development of LLMs has significantly enhanced question answering and dialogue generation',\n  'date': '2023-09-16',\n  'title': 'Enhancing Large Language Model Induced Task-Oriented Dialogue Systems\\n  Through Look-Forward Motivated Goals',\n  'id': '2309.08949'},\n {'from': 'large language model',\n  'to': 'dialogue generation',\n  'name': 'enhances',\n  'research': 'context',\n  'summary': 'The development of LLMs has significantly enhanced question answering and dialogue generation',\n  'date': '2023-09-16',\n  'title': 'Enhancing Large Language Model Induced Task-Oriented Dialogue Systems\\n  Through Look-Forward Motivated Goals',\n  'id': '2309.08949'},\n {'from': 'task-oriented dialogue system',\n  'to': 'efficient successful dialogue',\n  'name': 'aims to',\n  'research': 'context',\n  'summary': 'Task-oriented dialogue systems aim to achieve the dialogue goal efficiently and successfully in multiple turns',\n  'date': '2023-09-16',\n  'title': 'Enhancing Large Language Model Induced Task-Oriented Dialogue Systems\\n  Through Look-Forward Motivated Goals',\n  'id': '2309.08949'},\n {'from': 'LLM-induced task-oriented dialogue system',\n  'to': 'direct reward',\n  'name': 'lacks',\n  'research': 'problem',\n  'summary': 'Existing LLM-induced TOD systems lack the direct reward toward the final goal and do not take account of dialogue proactivity',\n  'date': '2023-09-16',\n  'title': 'Enhancing Large Language Model Induced Task-Oriented Dialogue Systems\\n  Through Look-Forward Motivated Goals',\n  'id': '2309.08949'},\n {'from': 'ProTOD',\n  'to': 'task-oriented dialogue system',\n  'name': 'introduces',\n  'research': 'contribution',\n  'summary': 'ProTOD approach anticipates future dialogue actions and incorporates the goal-oriented reward signal to enhance TOD systems',\n  'date': '2023-09-16',\n  'title': 'Enhancing Large Language Model Induced Task-Oriented Dialogue Systems\\n  Through Look-Forward Motivated Goals',\n  'id': '2309.08949'},\n {'from': 'goal-driven dialogue simulation',\n  'to': 'task-oriented dialogue system',\n  'name': 'assesses',\n  'research': 'contribution',\n  'summary': 'A novel evaluation method assesses TOD systems based on goal-driven dialogue simulations',\n  'date': '2023-09-16',\n  'title': 'Enhancing Large Language Model Induced Task-Oriented Dialogue Systems\\n  Through Look-Forward Motivated Goals',\n  'id': '2309.08949'},\n {'from': 'goal-driven dialogue simulation',\n  'to': 'user satisfaction',\n  'name': 'gauges',\n  'research': 'contribution',\n  'summary': 'This method allows us to gauge user satisfaction, system efficiency and successful rate',\n  'date': '2023-09-16',\n  'title': 'Enhancing Large Language Model Induced Task-Oriented Dialogue Systems\\n  Through Look-Forward Motivated Goals',\n  'id': '2309.08949'},\n {'from': 'goal-driven dialogue simulation',\n  'to': 'current metrics limitations',\n  'name': 'overcomes',\n  'research': 'contribution',\n  'summary': 'Overcomes the limitations of current information and success metrics',\n  'date': '2023-09-16',\n  'title': 'Enhancing Large Language Model Induced Task-Oriented Dialogue Systems\\n  Through Look-Forward Motivated Goals',\n  'id': '2309.08949'},\n {'from': 'ProTOD',\n  'to': 'MultiWOZ 2.1',\n  'name': 'performs on',\n  'research': 'finding',\n  'summary': 'Empirical experiments on the MultiWOZ 2.1 dataset show that ProTOD achieves superior performance with only 10% of the data compared to previous models',\n  'date': '2023-09-16',\n  'title': 'Enhancing Large Language Model Induced Task-Oriented Dialogue Systems\\n  Through Look-Forward Motivated Goals',\n  'id': '2309.08949'},\n {'from': 'ProTOD',\n  'to': 'user satisfaction',\n  'name': 'enhances',\n  'research': 'finding',\n  'summary': 'The improvement is accompanied by enhanced user satisfaction and efficiency',\n  'date': '2023-09-16',\n  'title': 'Enhancing Large Language Model Induced Task-Oriented Dialogue Systems\\n  Through Look-Forward Motivated Goals',\n  'id': '2309.08949'},\n {'from': 'knowledge editing',\n  'to': 'language model performance',\n  'name': 'aims to change',\n  'research': 'context',\n  'summary': \"Knowledge editing aims to change language models' performance on special cases by infusing expected knowledge\",\n  'date': '2023-09-16',\n  'title': 'Cross-Lingual Knowledge Editing in Large Language Models',\n  'id': '2309.08952'},\n {'from': 'knowledge editing',\n  'to': 'large language model',\n  'name': 'is a technique for',\n  'research': 'context',\n  'summary': 'Knowledge editing is a promising technique to adapt LLMs to new knowledge without retraining from scratch',\n  'date': '2023-09-16',\n  'title': 'Cross-Lingual Knowledge Editing in Large Language Models',\n  'id': '2309.08952'},\n {'from': 'large language model',\n  'to': 'multi-lingual nature',\n  'name': 'neglects aspect',\n  'research': 'problem',\n  'summary': 'Most previous studies neglect the multi-lingual nature of mainstream LLMs and focus on monolingual scenarios',\n  'date': '2023-09-16',\n  'title': 'Cross-Lingual Knowledge Editing in Large Language Models',\n  'id': '2309.08952'},\n {'from': 'LaMDA',\n  'to': 'multi-lingual large language model',\n  'name': 'is an example of',\n  'research': 'context',\n  'summary': 'LaMDA is an example of a mainstream LLM with multi-lingual nature',\n  'date': '2023-09-16',\n  'title': 'Cross-Lingual Knowledge Editing in Large Language Models',\n  'id': '2309.08952'},\n {'from': 'ChatGPT',\n  'to': 'multi-lingual large language model',\n  'name': 'is an example of',\n  'research': 'context',\n  'summary': 'ChatGPT is an example of a mainstream LLM with multi-lingual nature',\n  'date': '2023-09-16',\n  'title': 'Cross-Lingual Knowledge Editing in Large Language Models',\n  'id': '2309.08952'},\n {'from': 'GPT-4',\n  'to': 'multi-lingual large language model',\n  'name': 'is an example of',\n  'research': 'context',\n  'summary': 'GPT-4 is an example of a mainstream LLM with multi-lingual nature',\n  'date': '2023-09-16',\n  'title': 'Cross-Lingual Knowledge Editing in Large Language Models',\n  'id': '2309.08952'},\n {'from': 'cross-lingual effect in knowledge editing',\n  'to': 'knowledge editing',\n  'name': 'is investigated',\n  'research': 'goal',\n  'summary': 'The paper aims to figure out the cross-lingual effect in knowledge editing',\n  'date': '2023-09-16',\n  'title': 'Cross-Lingual Knowledge Editing in Large Language Models',\n  'id': '2309.08952'},\n {'from': 'cross-lingual synthetic dataset',\n  'to': 'ZSRE',\n  'name': 'created by translating',\n  'research': 'method',\n  'summary': 'A large-scale cross-lingual synthetic dataset is created by translating ZSRE from English to Chinese',\n  'date': '2023-09-16',\n  'title': 'Cross-Lingual Knowledge Editing in Large Language Models',\n  'id': '2309.08952'},\n {'from': 'English',\n  'to': 'knowledge editing',\n  'name': 'editing performed in',\n  'research': 'method',\n  'summary': 'English editing is conducted on various knowledge editing methods and evaluated in Chinese',\n  'date': '2023-09-16',\n  'title': 'Cross-Lingual Knowledge Editing in Large Language Models',\n  'id': '2309.08952'},\n {'from': 'Chinese',\n  'to': 'knowledge editing',\n  'name': 'evaluation performed in',\n  'research': 'method',\n  'summary': 'The performance of knowledge editing methods edited in English is evaluated in Chinese, and vice versa',\n  'date': '2023-09-16',\n  'title': 'Cross-Lingual Knowledge Editing in Large Language Models',\n  'id': '2309.08952'},\n {'from': 'cross-lingual knowledge editing evaluation',\n  'to': 'reliability, generality, locality, portability',\n  'name': 'includes aspects',\n  'research': 'method',\n  'summary': 'The evaluation of cross-lingual knowledge editing includes aspects of reliability, generality, locality, and portability',\n  'date': '2023-09-16',\n  'title': 'Cross-Lingual Knowledge Editing in Large Language Models',\n  'id': '2309.08952'},\n {'from': 'inconsistent behaviors of edited models',\n  'to': 'edited language model',\n  'name': 'analyzed for',\n  'research': 'contribution',\n  'summary': 'The paper analyzes the inconsistent behaviors of the edited models and discusses their specific challenges',\n  'date': '2023-09-16',\n  'title': 'Cross-Lingual Knowledge Editing in Large Language Models',\n  'id': '2309.08952'},\n {'from': 'large language model',\n  'to': 'open-ended question-answering',\n  'name': 'can be',\n  'research': 'context',\n  'summary': 'Foundational LLMs can be instruction-tuned to develop open-ended question-answering capability',\n  'date': '2023-09-16',\n  'title': 'Monolingual or Multilingual Instruction Tuning: Which Makes a Better\\n  Alpaca',\n  'id': '2309.08958'},\n {'from': 'open-ended question-answering',\n  'to': 'AI assistants',\n  'name': 'facilitates',\n  'research': 'context',\n  'summary': 'Open-ended question-answering capability facilitates the creation of AI assistants',\n  'date': '2023-09-16',\n  'title': 'Monolingual or Multilingual Instruction Tuning: Which Makes a Better\\n  Alpaca',\n  'id': '2309.08958'},\n {'from': 'monolingual tuning',\n  'to': 'monolingual',\n  'name': 'analyzed for',\n  'research': 'contribution',\n  'summary': 'The study empirically analyzes cost-efficient approaches of monolingual tuning',\n  'date': '2023-09-16',\n  'title': 'Monolingual or Multilingual Instruction Tuning: Which Makes a Better\\n  Alpaca',\n  'id': '2309.08958'},\n {'from': 'multilingual tuning',\n  'to': 'multilingual',\n  'name': 'analyzed for',\n  'research': 'contribution',\n  'summary': 'The study empirically analyzes cost-efficient approaches of multilingual tuning',\n  'date': '2023-09-16',\n  'title': 'Monolingual or Multilingual Instruction Tuning: Which Makes a Better\\n  Alpaca',\n  'id': '2309.08958'},\n {'from': 'ALPaCA',\n  'to': 'multilingual training data',\n  'name': 'used to form',\n  'research': 'method',\n  'summary': 'The ALPaCA dataset and its machine translations are used to form multilingual training data',\n  'date': '2023-09-16',\n  'title': 'Monolingual or Multilingual Instruction Tuning: Which Makes a Better\\n  Alpaca',\n  'id': '2309.08958'},\n {'from': 'low-rank adaptation',\n  'to': 'large language model',\n  'name': 'used to tune',\n  'research': 'method',\n  'summary': 'LLMs are tuned through low-rank adaptation',\n  'date': '2023-09-16',\n  'title': 'Monolingual or Multilingual Instruction Tuning: Which Makes a Better\\n  Alpaca',\n  'id': '2309.08958'},\n {'from': 'full-parameter training',\n  'to': 'large language model',\n  'name': 'used to tune',\n  'research': 'method',\n  'summary': 'LLMs are tuned through full-parameter training',\n  'date': '2023-09-16',\n  'title': 'Monolingual or Multilingual Instruction Tuning: Which Makes a Better\\n  Alpaca',\n  'id': '2309.08958'},\n {'from': 'multilingual tuning not crucial for English performance',\n  'to': 'English',\n  'name': 'revealed by',\n  'research': 'finding',\n  'summary': \"Comparisons reveal that multilingual tuning is not crucial for an LLM's English performance\",\n  'date': '2023-09-16',\n  'title': 'Monolingual or Multilingual Instruction Tuning: Which Makes a Better\\n  Alpaca',\n  'id': '2309.08958'},\n {'from': 'multilingual tuning key for multilingual robustness',\n  'to': 'multilingual environment',\n  'name': 'revealed by',\n  'research': 'finding',\n  'summary': \"Multilingual tuning is key to LLM's robustness in a multilingual environment\",\n  'date': '2023-09-16',\n  'title': 'Monolingual or Multilingual Instruction Tuning: Which Makes a Better\\n  Alpaca',\n  'id': '2309.08958'},\n {'from': 'multilingual instruction-tuned model',\n  'to': 'monolingual models',\n  'name': 'compared with',\n  'research': 'finding',\n  'summary': 'A multilingual instruction-tuned model trained on downsampled data can be as powerful as training monolingual models for each language with a fixed budget',\n  'date': '2023-09-16',\n  'title': 'Monolingual or Multilingual Instruction Tuning: Which Makes a Better\\n  Alpaca',\n  'id': '2309.08958'},\n {'from': 'expanding language support through instruction tuning',\n  'to': 'computational resources',\n  'name': 'provided by',\n  'research': 'contribution',\n  'summary': 'Findings serve as a guide for expanding language support through instruction tuning with constrained computational resources',\n  'date': '2023-09-16',\n  'title': 'Monolingual or Multilingual Instruction Tuning: Which Makes a Better\\n  Alpaca',\n  'id': '2309.08958'},\n {'from': 'large language model',\n  'to': 'generating complex structured data',\n  'name': 'struggle with',\n  'research': 'problem',\n  'summary': 'LLMs like GPT-4 struggle with tasks that require generating complex, structured outputs',\n  'date': '2023-09-16',\n  'title': 'Struc-Bench: Are Large Language Models Really Good at Generating Complex\\n  Structured Data?',\n  'id': '2309.08963'},\n {'from': 'structure-aware fine-tuning',\n  'to': 'generating complex structured data',\n  'name': 'proposed to improve',\n  'research': 'solution',\n  'summary': \"A structure-aware fine-tuning approach is proposed to improve LLMs' ability to generate complex structured data\",\n  'date': '2023-09-16',\n  'title': 'Struc-Bench: Are Large Language Models Really Good at Generating Complex\\n  Structured Data?',\n  'id': '2309.08963'},\n {'from': 'Struc-Bench',\n  'to': 'evaluating LLMs',\n  'name': 'proposed for',\n  'research': 'method',\n  'summary': 'Struc-Bench is proposed to perform a comprehensive evaluation of LLMs on generating complex structured data',\n  'date': '2023-09-16',\n  'title': 'Struc-Bench: Are Large Language Models Really Good at Generating Complex\\n  Structured Data?',\n  'id': '2309.08963'},\n {'from': 'GPT-NeoX 20B',\n  'to': 'Struc-Bench',\n  'name': 'evaluated on',\n  'research': 'method',\n  'summary': 'GPT-NeoX 20B is evaluated on datasets spanning raw text, HTML, and LaTeX tables in Struc-Bench',\n  'date': '2023-09-16',\n  'title': 'Struc-Bench: Are Large Language Models Really Good at Generating Complex\\n  Structured Data?',\n  'id': '2309.08963'},\n {'from': 'GPT-3.5',\n  'to': 'Struc-Bench',\n  'name': 'evaluated on',\n  'research': 'method',\n  'summary': 'GPT-3.5 is evaluated on datasets spanning raw text, HTML, and LaTeX tables in Struc-Bench',\n  'date': '2023-09-16',\n  'title': 'Struc-Bench: Are Large Language Models Really Good at Generating Complex\\n  Structured Data?',\n  'id': '2309.08963'},\n {'from': 'GPT-4',\n  'to': 'Struc-Bench',\n  'name': 'evaluated on',\n  'research': 'method',\n  'summary': 'GPT-4 is evaluated on datasets spanning raw text, HTML, and LaTeX tables in Struc-Bench',\n  'date': '2023-09-16',\n  'title': 'Struc-Bench: Are Large Language Models Really Good at Generating Complex\\n  Structured Data?',\n  'id': '2309.08963'},\n {'from': 'Vicuna',\n  'to': 'Struc-Bench',\n  'name': 'evaluated on',\n  'research': 'method',\n  'summary': 'Vicuna is evaluated on datasets spanning raw text, HTML, and LaTeX tables in Struc-Bench',\n  'date': '2023-09-16',\n  'title': 'Struc-Bench: Are Large Language Models Really Good at Generating Complex\\n  Structured Data?',\n  'id': '2309.08963'},\n {'from': 'FormatCOT',\n  'to': 'generating format instructions',\n  'name': 'utilized to',\n  'research': 'method',\n  'summary': 'FormatCOT is utilized to generate format instructions from target outputs to address complex formatting requirements',\n  'date': '2023-09-16',\n  'title': 'Struc-Bench: Are Large Language Models Really Good at Generating Complex\\n  Structured Data?',\n  'id': '2309.08963'},\n {'from': 'LLaMA-7B',\n  'to': 'structure-aware fine-tuning',\n  'name': 'improved by',\n  'research': 'finding',\n  'summary': 'LLaMA-7B significantly improves adherence to natural language constraints with structure-aware fine-tuning',\n  'date': '2023-09-16',\n  'title': 'Struc-Bench: Are Large Language Models Really Good at Generating Complex\\n  Structured Data?',\n  'id': '2309.08963'},\n {'from': 'ability map',\n  'to': 'handling complex structured outputs',\n  'name': 'presents',\n  'research': 'contribution',\n  'summary': 'An ability map of model capabilities is presented, highlighting weaknesses in handling complex structured outputs',\n  'date': '2023-09-16',\n  'title': 'Struc-Bench: Are Large Language Models Really Good at Generating Complex\\n  Structured Data?',\n  'id': '2309.08963'},\n {'from': 'code and models',\n  'to': 'https://github.com/gersteinlab/struc-bench',\n  'name': 'available at',\n  'research': 'resource',\n  'summary': 'The code and models for the study are available at the provided GitHub link',\n  'date': '2023-09-16',\n  'title': 'Struc-Bench: Are Large Language Models Really Good at Generating Complex\\n  Structured Data?',\n  'id': '2309.08963'},\n {'from': 'large language model',\n  'to': 'Natural Language Processing (NLP)',\n  'name': 'has advanced',\n  'research': 'context',\n  'summary': 'The rapid advancement of LLMs has revolutionized NLP',\n  'date': '2023-09-16',\n  'title': 'Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large\\n  Language Models for Dynamic Inference Using Sorted Fine-Tuning (SoFT)',\n  'id': '2309.08968'},\n {'from': 'large language model',\n  'to': 'deployment cost',\n  'name': 'challenge',\n  'research': 'context',\n  'summary': 'Widespread deployment of LLMs can be prohibitively expensive',\n  'date': '2023-09-16',\n  'title': 'Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large\\n  Language Models for Dynamic Inference Using Sorted Fine-Tuning (SoFT)',\n  'id': '2309.08968'},\n {'from': 'SortedNet',\n  'to': 'dynamic inference',\n  'name': 'enables',\n  'research': 'contribution',\n  'summary': 'SortedNet is a training technique enabling dynamic inference for deep neural networks',\n  'date': '2023-09-16',\n  'title': 'Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large\\n  Language Models for Dynamic Inference Using Sorted Fine-Tuning (SoFT)',\n  'id': '2309.08968'},\n {'from': 'SortedNet',\n  'to': 'network modularity',\n  'name': 'leverages',\n  'research': 'method',\n  'summary': 'SortedNet leverages network modularity to create sub-models sorted by computation/accuracy characteristics',\n  'date': '2023-09-16',\n  'title': 'Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large\\n  Language Models for Dynamic Inference Using Sorted Fine-Tuning (SoFT)',\n  'id': '2309.08968'},\n {'from': 'sorted fine-tuning (SoFT)',\n  'to': 'generative NLP tasks',\n  'name': 'extends',\n  'research': 'contribution',\n  'summary': 'Sorted fine-tuning extends SortedNet to generative NLP tasks, making LLMs dynamic without pretraining',\n  'date': '2023-09-16',\n  'title': 'Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large\\n  Language Models for Dynamic Inference Using Sorted Fine-Tuning (SoFT)',\n  'id': '2309.08968'},\n {'from': 'sorted fine-tuning (SoFT)',\n  'to': 'standard supervised fine-tuning (SFT)',\n  'name': 'replaces',\n  'research': 'method',\n  'summary': 'Sorted fine-tuning replaces standard supervised fine-tuning at the same costs',\n  'date': '2023-09-16',\n  'title': 'Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large\\n  Language Models for Dynamic Inference Using Sorted Fine-Tuning (SoFT)',\n  'id': '2309.08968'},\n {'from': 'Transformer',\n  'to': 'sorted fine-tuning (SoFT)',\n  'name': 'potential unlocked by',\n  'research': 'finding',\n  'summary': 'Using sorted fine-tuning, the potential of intermediate layers of Transformers is unlocked for generating target output',\n  'date': '2023-09-16',\n  'title': 'Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large\\n  Language Models for Dynamic Inference Using Sorted Fine-Tuning (SoFT)',\n  'id': '2309.08968'},\n {'from': 'LLaMA 2 13B',\n  'to': 'Stanford ALPACA dataset',\n  'name': 'tuned on',\n  'research': 'method',\n  'summary': 'LLaMA 2 13B is tuned on the Stanford ALPACA dataset using sorted fine-tuning',\n  'date': '2023-09-16',\n  'title': 'Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large\\n  Language Models for Dynamic Inference Using Sorted Fine-Tuning (SoFT)',\n  'id': '2309.08968'},\n {'from': 'sorted fine-tuning (SoFT)',\n  'to': 'PANDALM',\n  'name': 'compared to',\n  'research': 'evaluation',\n  'summary': 'Sorted fine-tuning is compared to normal tuning and early exit via PANDALM benchmark',\n  'date': '2023-09-16',\n  'title': 'Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large\\n  Language Models for Dynamic Inference Using Sorted Fine-Tuning (SoFT)',\n  'id': '2309.08968'},\n {'from': 'sorted fine-tuning (SoFT)',\n  'to': 'efficiency and performance',\n  'name': 'delivers',\n  'research': 'finding',\n  'summary': 'Sorted fine-tuning delivers models twice as fast as the original model while maintaining or exceeding performance',\n  'date': '2023-09-16',\n  'title': 'Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large\\n  Language Models for Dynamic Inference Using Sorted Fine-Tuning (SoFT)',\n  'id': '2309.08968'},\n {'from': 'pre-trained language model',\n  'to': 'NLP benchmarks',\n  'name': 'achieved performance on',\n  'research': 'context',\n  'summary': 'Large pre-trained language models have achieved remarkable performance on many NLP benchmarks',\n  'date': '2023-09-16',\n  'title': 'Context-aware Adversarial Attack on Named Entity Recognition',\n  'id': '2309.08999'},\n {'from': 'pre-trained language model',\n  'to': 'adversarial attacks',\n  'name': 'vulnerable to',\n  'research': 'context',\n  'summary': 'PLMs are vulnerable to attacks from adversarial examples',\n  'date': '2023-09-16',\n  'title': 'Context-aware Adversarial Attack on Named Entity Recognition',\n  'id': '2309.08999'},\n {'from': 'named entity recognition',\n  'to': 'context-aware adversarial attack',\n  'name': 'subject of study for',\n  'research': 'focus',\n  'summary': 'The study focuses on context-aware adversarial attack methods for the named entity recognition task',\n  'date': '2023-09-16',\n  'title': 'Context-aware Adversarial Attack on Named Entity Recognition',\n  'id': '2309.08999'},\n {'from': 'context-aware adversarial attack',\n  'to': 'perturbing informative words',\n  'name': 'proposes',\n  'research': 'contribution',\n  'summary': 'The study proposes perturbing the most informative words for recognizing entities to create adversarial examples',\n  'date': '2023-09-16',\n  'title': 'Context-aware Adversarial Attack on Named Entity Recognition',\n  'id': '2309.08999'},\n {'from': 'perturbing informative words',\n  'to': 'generating adversarial examples',\n  'name': 'used for',\n  'research': 'method',\n  'summary': 'Perturbing informative words is used to investigate different candidate replacement methods to generate natural and plausible adversarial examples',\n  'date': '2023-09-16',\n  'title': 'Context-aware Adversarial Attack on Named Entity Recognition',\n  'id': '2309.08999'},\n {'from': 'candidate replacement',\n  'to': 'generating adversarial examples',\n  'name': 'used to generate',\n  'research': 'method',\n  'summary': 'Different candidate replacement methods are investigated to generate natural and plausible adversarial examples',\n  'date': '2023-09-16',\n  'title': 'Context-aware Adversarial Attack on Named Entity Recognition',\n  'id': '2309.08999'},\n {'from': 'effectiveness testing',\n  'to': 'model deception',\n  'name': 'shows',\n  'research': 'finding',\n  'summary': 'Experiments and analyses show that the proposed methods are more effective in deceiving the model than strong baselines',\n  'date': '2023-09-16',\n  'title': 'Context-aware Adversarial Attack on Named Entity Recognition',\n  'id': '2309.08999'},\n {'from': 'RLHF',\n  'to': 'PPO training',\n  'name': 'final stage involves',\n  'research': 'context',\n  'summary': 'During the last stage of RLHF, a large language model is aligned to human intents via PPO training',\n  'date': '2023-09-16',\n  'title': 'Exploring the impact of low-rank adaptation on the performance,\\n  efficiency, and regularization of RLHF',\n  'id': '2309.09055'},\n {'from': 'PPO training',\n  'to': 'large-scale computational resources',\n  'name': 'requires',\n  'research': 'context',\n  'summary': 'PPO training generally requires large-scale computational resources',\n  'date': '2023-09-16',\n  'title': 'Exploring the impact of low-rank adaptation on the performance,\\n  efficiency, and regularization of RLHF',\n  'id': '2309.09055'},\n {'from': 'efficient RLHF',\n  'to': 'Low-Rank Adaptation (LoRA)',\n  'name': 'employs',\n  'research': 'contribution',\n  'summary': 'An efficient implementation of RLHF using Low-Rank Adaptation (LoRA) is investigated',\n  'date': '2023-09-16',\n  'title': 'Exploring the impact of low-rank adaptation on the performance,\\n  efficiency, and regularization of RLHF',\n  'id': '2309.09055'},\n {'from': 'LLaMA 7B',\n  'to': 'Alpaca',\n  'name': 'aligned using',\n  'research': 'contribution',\n  'summary': 'The LLaMA 7B checkpoint is aligned on the Alpaca dataset using only two A100 GPUs with LoRA',\n  'date': '2023-09-16',\n  'title': 'Exploring the impact of low-rank adaptation on the performance,\\n  efficiency, and regularization of RLHF',\n  'id': '2309.09055'},\n {'from': 'A100 GPU',\n  'to': 'full model fine-tuning',\n  'name': 'reduces need for',\n  'research': 'contribution',\n  'summary': 'Only two A100 GPUs are needed instead of eight for full model fine-tuning with LoRA',\n  'date': '2023-09-16',\n  'title': 'Exploring the impact of low-rank adaptation on the performance,\\n  efficiency, and regularization of RLHF',\n  'id': '2309.09055'},\n {'from': 'better performance',\n  'to': 'LLaMA 7B',\n  'name': 'achieved by',\n  'research': 'finding',\n  'summary': \"Better performance is achieved by tuning only 0.2% of LLaMA 7B's parameters with LoRA implementation\",\n  'date': '2023-09-16',\n  'title': 'Exploring the impact of low-rank adaptation on the performance,\\n  efficiency, and regularization of RLHF',\n  'id': '2309.09055'},\n {'from': 'AlpacaFarm',\n  'to': 'LoRA-based PPO',\n  'name': 'outperformed by',\n  'research': 'finding',\n  'summary': 'The publicly-released AlpacaFarm checkpoint with full model fine-tuning is outperformed by LoRA implementation',\n  'date': '2023-09-16',\n  'title': 'Exploring the impact of low-rank adaptation on the performance,\\n  efficiency, and regularization of RLHF',\n  'id': '2309.09055'},\n {'from': 'LoRA-based PPO',\n  'to': 'KL regularization term',\n  'name': 'varies',\n  'research': 'method',\n  'summary': 'Several configurations of LoRA-based PPO implementation are analyzed, varying the form of the KL regularization term',\n  'date': '2023-09-16',\n  'title': 'Exploring the impact of low-rank adaptation on the performance,\\n  efficiency, and regularization of RLHF',\n  'id': '2309.09055'},\n {'from': 'KL penalty term',\n  'to': 'model performance',\n  'name': 'removal does not harm',\n  'research': 'finding',\n  'summary': 'Removing the KL penalty term does not harm performance under the LoRA setup',\n  'date': '2023-09-16',\n  'title': 'Exploring the impact of low-rank adaptation on the performance,\\n  efficiency, and regularization of RLHF',\n  'id': '2309.09055'},\n {'from': 'Jensen-Shannon divergence',\n  'to': 'model performance',\n  'name': 'leads to',\n  'research': 'finding',\n  'summary': 'Using Jensen-Shannon divergence as a regularizer leads to improved performance',\n  'date': '2023-09-16',\n  'title': 'Exploring the impact of low-rank adaptation on the performance,\\n  efficiency, and regularization of RLHF',\n  'id': '2309.09055'},\n {'from': 'PPO training',\n  'to': 'factuality',\n  'name': 'negatively impacts',\n  'research': 'finding',\n  'summary': 'PPO training negatively impacts the factuality of model-generated responses',\n  'date': '2023-09-16',\n  'title': 'Exploring the impact of low-rank adaptation on the performance,\\n  efficiency, and regularization of RLHF',\n  'id': '2309.09055'},\n {'from': 'Low-Rank Adaptation (LoRA)',\n  'to': 'factuality',\n  'name': 'mitigates',\n  'research': 'finding',\n  'summary': 'Training with LoRA largely mitigates the negative impact on factuality caused by PPO training',\n  'date': '2023-09-16',\n  'title': 'Exploring the impact of low-rank adaptation on the performance,\\n  efficiency, and regularization of RLHF',\n  'id': '2309.09055'},\n {'from': 'code and pretrained checkpoints',\n  'to': 'efficient RLHF research',\n  'name': 'released to',\n  'research': 'contribution',\n  'summary': 'Code and pretrained checkpoints are released to facilitate future research on more efficient RLHF',\n  'date': '2023-09-16',\n  'title': 'Exploring the impact of low-rank adaptation on the performance,\\n  efficiency, and regularization of RLHF',\n  'id': '2309.09055'},\n {'from': 'Nowj1',\n  'to': 'Automated Legal Question Answering Competition (ALQAC) 2023',\n  'name': 'participates in',\n  'research': 'context',\n  'summary': 'The Nowj1 team is participating in the Automated Legal Question Answering Competition (ALQAC) 2023',\n  'date': '2023-09-16',\n  'title': 'NOWJ1@ALQAC 2023: Enhancing Legal Task Performance with Classic\\n  Statistical Models and Pre-trained Language Models',\n  'id': '2309.09070'},\n {'from': 'Nowj1 teams approach',\n  'to': 'pre-trained language model',\n  'name': 'integrates',\n  'research': 'method',\n  'summary': 'The approach integrates classical statistical models with pre-trained language models (PLMs) to enhance legal task performance',\n  'date': '2023-09-16',\n  'title': 'NOWJ1@ALQAC 2023: Enhancing Legal Task Performance with Classic\\n  Statistical Models and Pre-trained Language Models',\n  'id': '2309.09070'},\n {'from': 'Nowj1 teams approach',\n  'to': 'classical statistical model',\n  'name': 'integrates',\n  'research': 'method',\n  'summary': 'The approach integrates classical statistical models with pre-trained language models (PLMs) to enhance legal task performance',\n  'date': '2023-09-16',\n  'title': 'NOWJ1@ALQAC 2023: Enhancing Legal Task Performance with Classic\\n  Statistical Models and Pre-trained Language Models',\n  'id': '2309.09070'},\n {'from': 'document retrieval',\n  'to': 'learning-to-rank',\n  'name': 'uses',\n  'research': 'method',\n  'summary': 'For the document retrieval task, a pre-processing step is implemented to overcome input limitations and learning-to-rank methods are applied',\n  'date': '2023-09-16',\n  'title': 'NOWJ1@ALQAC 2023: Enhancing Legal Task Performance with Classic\\n  Statistical Models and Pre-trained Language Models',\n  'id': '2309.09070'},\n {'from': 'question-answering',\n  'to': 'sentence classification',\n  'name': 'split into',\n  'research': 'method',\n  'summary': 'The question-answering task is split into sentence classification and answer extraction sub-tasks',\n  'date': '2023-09-16',\n  'title': 'NOWJ1@ALQAC 2023: Enhancing Legal Task Performance with Classic\\n  Statistical Models and Pre-trained Language Models',\n  'id': '2309.09070'},\n {'from': 'question-answering',\n  'to': 'answer extraction',\n  'name': 'split into',\n  'research': 'method',\n  'summary': 'The question-answering task is split into sentence classification and answer extraction sub-tasks',\n  'date': '2023-09-16',\n  'title': 'NOWJ1@ALQAC 2023: Enhancing Legal Task Performance with Classic\\n  Statistical Models and Pre-trained Language Models',\n  'id': '2309.09070'},\n {'from': 'distinct systems for sub-tasks',\n  'to': 'pre-trained language model',\n  'name': 'utilizes',\n  'research': 'method',\n  'summary': 'Distinct systems for each sub-task utilize both classic statistical models and pre-trained language models',\n  'date': '2023-09-16',\n  'title': 'NOWJ1@ALQAC 2023: Enhancing Legal Task Performance with Classic\\n  Statistical Models and Pre-trained Language Models',\n  'id': '2309.09070'},\n {'from': 'distinct systems for sub-tasks',\n  'to': 'classical statistical model',\n  'name': 'utilizes',\n  'research': 'method',\n  'summary': 'Distinct systems for each sub-task utilize both classic statistical models and pre-trained language models',\n  'date': '2023-09-16',\n  'title': 'NOWJ1@ALQAC 2023: Enhancing Legal Task Performance with Classic\\n  Statistical Models and Pre-trained Language Models',\n  'id': '2309.09070'},\n {'from': 'experimental results',\n  'to': 'Nowj1 teams approach',\n  'name': 'demonstrate',\n  'research': 'finding',\n  'summary': 'Experimental results demonstrate the promising potential of the proposed methodology in the competition',\n  'date': '2023-09-16',\n  'title': 'NOWJ1@ALQAC 2023: Enhancing Legal Task Performance with Classic\\n  Statistical Models and Pre-trained Language Models',\n  'id': '2309.09070'},\n {'from': 'RMDM',\n  'to': 'performance assessment of LLMs',\n  'name': 'designed to assess',\n  'research': 'context',\n  'summary': 'The RMDM dataset is designed to assess the performance of LLMs in verifying electronic information related to legal contexts',\n  'date': '2023-09-16',\n  'title': 'RMDM: A Multilabel Fakenews Dataset for Vietnamese Evidence Verification',\n  'id': '2309.09071'},\n {'from': 'RMDM',\n  'to': 'fake news',\n  'name': 'focuses on',\n  'research': 'context',\n  'summary': 'RMDM focuses on fake news as potential input for electronic evidence',\n  'date': '2023-09-16',\n  'title': 'RMDM: A Multilabel Fakenews Dataset for Vietnamese Evidence Verification',\n  'id': '2309.09071'},\n {'from': 'RMDM',\n  'to': 'real, mis, dis, mal',\n  'name': 'comprises',\n  'research': 'context',\n  'summary': 'The RMDM dataset comprises four labels: real, mis, dis, and mal',\n  'date': '2023-09-16',\n  'title': 'RMDM: A Multilabel Fakenews Dataset for Vietnamese Evidence Verification',\n  'id': '2309.09071'},\n {'from': 'real, mis, dis, mal',\n  'to': 'real information, misinformation, disinformation, mal-information',\n  'name': 'represent',\n  'research': 'context',\n  'summary': 'The labels represent real information, misinformation, disinformation, and mal-information, respectively',\n  'date': '2023-09-16',\n  'title': 'RMDM: A Multilabel Fakenews Dataset for Vietnamese Evidence Verification',\n  'id': '2309.09071'},\n {'from': 'RMDM',\n  'to': '1,556 samples',\n  'name': 'consists of',\n  'research': 'context',\n  'summary': 'The dataset consists of a total of 1,556 samples, with 389 samples for each label',\n  'date': '2023-09-16',\n  'title': 'RMDM: A Multilabel Fakenews Dataset for Vietnamese Evidence Verification',\n  'id': '2309.09071'},\n {'from': 'GPT-based model',\n  'to': 'RMDM',\n  'name': 'tested on',\n  'research': 'method',\n  'summary': 'Preliminary tests on the RMDM dataset using GPT-based models reveal variations in performance across different labels',\n  'date': '2023-09-16',\n  'title': 'RMDM: A Multilabel Fakenews Dataset for Vietnamese Evidence Verification',\n  'id': '2309.09071'},\n {'from': 'BERT-based model',\n  'to': 'RMDM',\n  'name': 'tested on',\n  'research': 'method',\n  'summary': 'Preliminary tests on the RMDM dataset using BERT-based models reveal variations in performance across different labels',\n  'date': '2023-09-16',\n  'title': 'RMDM: A Multilabel Fakenews Dataset for Vietnamese Evidence Verification',\n  'id': '2309.09071'},\n {'from': 'verifying electronic information',\n  'to': 'large language model',\n  'name': 'is a difficult problem for',\n  'research': 'finding',\n  'summary': 'Verifying electronic information related to legal contexts, including fake news, remains a difficult problem for language models',\n  'date': '2023-09-16',\n  'title': 'RMDM: A Multilabel Fakenews Dataset for Vietnamese Evidence Verification',\n  'id': '2309.09071'},\n {'from': 'research community',\n  'to': 'reliable AI models for legal applications',\n  'name': 'should advance',\n  'research': 'reflection',\n  'summary': 'Further attention from the research community is warranted to advance toward more reliable AI models for potential legal applications',\n  'date': '2023-09-16',\n  'title': 'RMDM: A Multilabel Fakenews Dataset for Vietnamese Evidence Verification',\n  'id': '2309.09071'},\n {'from': 'pre-trained language model',\n  'to': 'large-scale data',\n  'name': 'trained on',\n  'research': 'context',\n  'summary': 'Pre-trained language models are trained on large-scale data',\n  'date': '2023-09-16',\n  'title': 'The Impact of Debiasing on the Performance of Language Models in\\n  Downstream Tasks is Underestimated',\n  'id': '2309.09092'},\n {'from': 'pre-trained language model',\n  'to': 'social biases',\n  'name': 'learned',\n  'research': 'problem',\n  'summary': 'Pre-trained language models have learned serious levels of social biases',\n  'date': '2023-09-16',\n  'title': 'The Impact of Debiasing on the Performance of Language Models in\\n  Downstream Tasks is Underestimated',\n  'id': '2309.09092'},\n {'from': 'debiasing',\n  'to': 'social biases',\n  'name': 'proposed to mitigate',\n  'research': 'solution',\n  'summary': 'Various methods have been proposed to debias pre-trained models',\n  'date': '2023-09-16',\n  'title': 'The Impact of Debiasing on the Performance of Language Models in\\n  Downstream Tasks is Underestimated',\n  'id': '2309.09092'},\n {'from': 'debiasing',\n  'to': 'useful information retention',\n  'name': 'needs to retain',\n  'research': 'requirement',\n  'summary': 'Debiasing methods need to retain useful information for downstream tasks while mitigating discriminatory bias',\n  'date': '2023-09-16',\n  'title': 'The Impact of Debiasing on the Performance of Language Models in\\n  Downstream Tasks is Underestimated',\n  'id': '2309.09092'},\n {'from': 'downstream task performance',\n  'to': 'useful information retention',\n  'name': 'used to confirm',\n  'research': 'method',\n  'summary': 'The performance of downstream tasks in debiased pre-trained models is used to confirm the retention of useful information',\n  'date': '2023-09-16',\n  'title': 'The Impact of Debiasing on the Performance of Language Models in\\n  Downstream Tasks is Underestimated',\n  'id': '2309.09092'},\n {'from': 'downstream task performance',\n  'to': 'social biases',\n  'name': 'questioned for',\n  'research': 'problem',\n  'summary': 'It is not clear whether benchmarks consist of data pertaining to social biases and are appropriate for investigating the impact of debiasing',\n  'date': '2023-09-16',\n  'title': 'The Impact of Debiasing on the Performance of Language Models in\\n  Downstream Tasks is Underestimated',\n  'id': '2309.09092'},\n {'from': 'gender-related social bias data',\n  'to': 'female, male, and stereotypical words',\n  'name': 'contains',\n  'research': 'context',\n  'summary': 'Gender-related social bias data contains female, male, and stereotypical words',\n  'date': '2023-09-16',\n  'title': 'The Impact of Debiasing on the Performance of Language Models in\\n  Downstream Tasks is Underestimated',\n  'id': '2309.09092'},\n {'from': 'this study',\n  'to': 'benchmark datasets',\n  'name': 'compares',\n  'research': 'method',\n  'summary': 'This study compares the impact of debiasing on performance across multiple downstream tasks using a wide-range of benchmark datasets containing female, male, and stereotypical words',\n  'date': '2023-09-16',\n  'title': 'The Impact of Debiasing on the Performance of Language Models in\\n  Downstream Tasks is Underestimated',\n  'id': '2309.09092'},\n {'from': 'effects of debiasing underestimated',\n  'to': 'debiasing',\n  'name': 'shown by',\n  'research': 'finding',\n  'summary': 'Experiments show that the effects of debiasing are consistently underestimated across all tasks',\n  'date': '2023-09-16',\n  'title': 'The Impact of Debiasing on the Performance of Language Models in\\n  Downstream Tasks is Underestimated',\n  'id': '2309.09092'},\n {'from': 'debiasing',\n  'to': 'benchmark datasets',\n  'name': 'evaluated by',\n  'research': 'finding',\n  'summary': 'The effects of debiasing could be reliably evaluated by separately considering instances containing female, male, and stereotypical words than all of the instances in a benchmark dataset',\n  'date': '2023-09-16',\n  'title': 'The Impact of Debiasing on the Performance of Language Models in\\n  Downstream Tasks is Underestimated',\n  'id': '2309.09092'},\n {'from': 'contrastive decoding',\n  'to': 'Li et al 2022',\n  'name': 'proposed by',\n  'research': 'context',\n  'summary': 'Contrastive decoding is a text generation method proposed by Li et al 2022',\n  'date': '2023-09-17',\n  'title': 'Contrastive Decoding Improves Reasoning in Large Language Models',\n  'id': '2309.09117'},\n {'from': 'contrastive decoding',\n  'to': 'simplicity and computational efficiency',\n  'name': 'characterized by',\n  'research': 'context',\n  'summary': 'Contrastive decoding is simple, computationally light, and training-free',\n  'date': '2023-09-17',\n  'title': 'Contrastive Decoding Improves Reasoning in Large Language Models',\n  'id': '2309.09117'},\n {'from': 'contrastive decoding',\n  'to': 'reasoning tasks',\n  'name': 'improves',\n  'research': 'contribution',\n  'summary': 'Contrastive decoding achieves large improvements over greedy decoding on reasoning tasks',\n  'date': '2023-09-17',\n  'title': 'Contrastive Decoding Improves Reasoning in Large Language Models',\n  'id': '2309.09117'},\n {'from': 'contrastive decoding',\n  'to': 'maximizing weighted difference in likelihood',\n  'name': 'searches for',\n  'research': 'context',\n  'summary': 'Contrastive decoding searches for strings that maximize a weighted difference in likelihood between strong and weak models',\n  'date': '2023-09-17',\n  'title': 'Contrastive Decoding Improves Reasoning in Large Language Models',\n  'id': '2309.09117'},\n {'from': 'LLaMA-65B',\n  'to': 'HellaSWAG',\n  'name': 'outperforms',\n  'research': 'finding',\n  'summary': 'LLaMA-65B outperforms LLaMA 2, GPT-3.5, and PaLM 2-L on the HellaSWAG commonsense reasoning benchmark using contrastive decoding',\n  'date': '2023-09-17',\n  'title': 'Contrastive Decoding Improves Reasoning in Large Language Models',\n  'id': '2309.09117'},\n {'from': 'LLaMA-65B',\n  'to': 'GSM8K',\n  'name': 'outperforms',\n  'research': 'finding',\n  'summary': 'LLaMA-65B outperforms LLaMA 2, GPT-3.5, and PaLM-540B on the GSM8K math word reasoning benchmark using contrastive decoding',\n  'date': '2023-09-17',\n  'title': 'Contrastive Decoding Improves Reasoning in Large Language Models',\n  'id': '2309.09117'},\n {'from': 'contrastive decoding',\n  'to': 'abstract reasoning errors',\n  'name': 'prevents',\n  'research': 'finding',\n  'summary': 'Contrastive decoding prevents some abstract reasoning errors and avoids simpler modes such as copying sections of the input during chain-of-thought',\n  'date': '2023-09-17',\n  'title': 'Contrastive Decoding Improves Reasoning in Large Language Models',\n  'id': '2309.09117'},\n {'from': 'contrastive decoding',\n  'to': 'nucleus sampling and greedy decoding',\n  'name': 'outperforms',\n  'research': 'finding',\n  'summary': 'Contrastive decoding outperforms nucleus sampling for long-form generation and greedy decoding for reasoning tasks',\n  'date': '2023-09-17',\n  'title': 'Contrastive Decoding Improves Reasoning in Large Language Models',\n  'id': '2309.09117'},\n {'from': 'contrastive decoding',\n  'to': 'general-purpose text generation',\n  'name': 'is a',\n  'research': 'reflection',\n  'summary': 'Contrastive decoding is a powerful general-purpose method for generating text from language models',\n  'date': '2023-09-17',\n  'title': 'Contrastive Decoding Improves Reasoning in Large Language Models',\n  'id': '2309.09117'},\n {'from': 'large language model',\n  'to': 'gender bias',\n  'name': 'demonstrate',\n  'research': 'finding',\n  'summary': 'Large language models demonstrate gender bias in their responses',\n  'date': '2023-09-17',\n  'title': 'Public Perceptions of Gender Bias in Large Language Models: Cases of\\n  ChatGPT and Ernie',\n  'id': '2309.09120'},\n {'from': 'content analysis',\n  'to': 'public perception of gender bias',\n  'name': 'used to gauge',\n  'research': 'method',\n  'summary': 'Content analysis of social media discussions was conducted to gauge public perceptions of gender bias in LLMs',\n  'date': '2023-09-17',\n  'title': 'Public Perceptions of Gender Bias in Large Language Models: Cases of\\n  ChatGPT and Ernie',\n  'id': '2309.09120'},\n {'from': 'ChatGPT',\n  'to': 'US-based',\n  'name': 'trained in',\n  'research': 'context',\n  'summary': 'ChatGPT is a US-based LLM trained in different cultural contexts',\n  'date': '2023-09-17',\n  'title': 'Public Perceptions of Gender Bias in Large Language Models: Cases of\\n  ChatGPT and Ernie',\n  'id': '2309.09120'},\n {'from': 'ERNIE',\n  'to': 'China-based',\n  'name': 'trained in',\n  'research': 'context',\n  'summary': 'ERNIE is a China-based LLM trained in different cultural contexts',\n  'date': '2023-09-17',\n  'title': 'Public Perceptions of Gender Bias in Large Language Models: Cases of\\n  ChatGPT and Ernie',\n  'id': '2309.09120'},\n {'from': 'public perception of gender bias',\n  'to': 'gender bias',\n  'name': 'includes',\n  'research': 'finding',\n  'summary': 'People shared observations of gender bias in personal use and scientific findings about gender bias in LLMs',\n  'date': '2023-09-17',\n  'title': 'Public Perceptions of Gender Bias in Large Language Models: Cases of\\n  ChatGPT and Ernie',\n  'id': '2309.09120'},\n {'from': 'ChatGPT',\n  'to': 'implicit gender bias',\n  'name': 'found to have',\n  'research': 'finding',\n  'summary': 'ChatGPT was more often found to carry implicit gender bias, associating men and women with different profession titles',\n  'date': '2023-09-17',\n  'title': 'Public Perceptions of Gender Bias in Large Language Models: Cases of\\n  ChatGPT and Ernie',\n  'id': '2309.09120'},\n {'from': 'ERNIE',\n  'to': 'explicit gender bias',\n  'name': 'found to have',\n  'research': 'finding',\n  'summary': \"Explicit gender bias was found in ERNIE's responses, e.g., overly promoting women's pursuit of marriage over career\",\n  'date': '2023-09-17',\n  'title': 'Public Perceptions of Gender Bias in Large Language Models: Cases of\\n  ChatGPT and Ernie',\n  'id': '2309.09120'},\n {'from': 'culture on gender bias',\n  'to': 'gender bias',\n  'name': 'reflected on',\n  'research': 'reflection',\n  'summary': 'The impact of culture on gender bias in LLMs is reflected upon based on the findings',\n  'date': '2023-09-17',\n  'title': 'Public Perceptions of Gender Bias in Large Language Models: Cases of\\n  ChatGPT and Ernie',\n  'id': '2309.09120'},\n {'from': 'governance recommendations',\n  'to': 'gender bias',\n  'name': 'proposed to regulate',\n  'research': 'contribution',\n  'summary': 'Governance recommendations are proposed to regulate gender bias in LLMs',\n  'date': '2023-09-17',\n  'title': 'Public Perceptions of Gender Bias in Large Language Models: Cases of\\n  ChatGPT and Ernie',\n  'id': '2309.09120'},\n {'from': 'evaluating outputs',\n  'to': 'large language model',\n  'name': 'is challenging for',\n  'research': 'context',\n  'summary': 'Evaluating outputs of LLMs is challenging and requires making sense of many responses',\n  'date': '2023-09-17',\n  'title': 'ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis\\n  Testing',\n  'id': '2309.09128'},\n {'from': 'ChainForge',\n  'to': 'prompt engineering and hypothesis testing',\n  'name': 'provides',\n  'research': 'contribution',\n  'summary': 'ChainForge provides an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs',\n  'date': '2023-09-17',\n  'title': 'ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis\\n  Testing',\n  'id': '2309.09128'},\n {'from': 'ChainForge',\n  'to': 'graphical interface for comparison',\n  'name': 'offers',\n  'research': 'contribution',\n  'summary': 'ChainForge offers a graphical interface for comparison of responses across models and prompt variations',\n  'date': '2023-09-17',\n  'title': 'ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis\\n  Testing',\n  'id': '2309.09128'},\n {'from': 'ChainForge',\n  'to': 'model selection, prompt template design, hypothesis testing',\n  'name': 'designed to support',\n  'research': 'contribution',\n  'summary': 'ChainForge is designed to support tasks such as model selection, prompt template design, and hypothesis testing',\n  'date': '2023-09-17',\n  'title': 'ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis\\n  Testing',\n  'id': '2309.09128'},\n {'from': 'ChainForge',\n  'to': 'academics and online users',\n  'name': 'used by',\n  'research': 'finding',\n  'summary': 'A range of people could use ChainForge to investigate hypotheses that matter to them, including in real-world settings',\n  'date': '2023-09-17',\n  'title': 'ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis\\n  Testing',\n  'id': '2309.09128'},\n {'from': 'prompt engineering and LLM hypothesis testing',\n  'to': 'opportunistic exploration, limited evaluation, iterative refinement',\n  'name': 'includes',\n  'research': 'finding',\n  'summary': 'Three modes of prompt engineering and LLM hypothesis testing are identified: opportunistic exploration, limited evaluation, and iterative refinement',\n  'date': '2023-09-17',\n  'title': 'ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis\\n  Testing',\n  'id': '2309.09128'},\n {'from': 'large language model',\n  'to': 'understanding human instructions',\n  'name': 'can understand',\n  'research': 'context',\n  'summary': 'LLMs can understand human instructions, indicating potential for pragmatic applications beyond traditional NLP tasks',\n  'date': '2023-09-17',\n  'title': 'Can Large Language Models Understand Real-World Complex Instructions?',\n  'id': '2309.09150'},\n {'from': 'large language model',\n  'to': 'complex instructions',\n  'name': 'struggle with',\n  'research': 'problem',\n  'summary': 'LLMs struggle with complex instructions, including complex task descriptions and complex input',\n  'date': '2023-09-17',\n  'title': 'Can Large Language Models Understand Real-World Complex Instructions?',\n  'id': '2309.09150'},\n {'from': 'complex instructions',\n  'to': 'complexity features',\n  'name': 'characterized by',\n  'research': 'context',\n  'summary': 'Complex instructions can involve multiple tasks and constraints, long context, noise, heterogeneous information, and multi-turn format',\n  'date': '2023-09-17',\n  'title': 'Can Large Language Models Understand Real-World Complex Instructions?',\n  'id': '2309.09150'},\n {'from': 'existing benchmarks',\n  'to': 'understanding complex instructions',\n  'name': 'insufficient for',\n  'research': 'problem',\n  'summary': \"Existing benchmarks are insufficient to assess LLMs' ability to understand complex instructions as they are close-ended and simple\",\n  'date': '2023-09-17',\n  'title': 'Can Large Language Models Understand Real-World Complex Instructions?',\n  'id': '2309.09150'},\n {'from': 'CELLO',\n  'to': 'understanding complex instructions',\n  'name': 'proposed for',\n  'research': 'contribution',\n  'summary': \"CELLO is proposed as a benchmark for evaluating LLMs' ability to follow complex instructions systematically\",\n  'date': '2023-09-17',\n  'title': 'Can Large Language Models Understand Real-World Complex Instructions?',\n  'id': '2309.09150'},\n {'from': 'CELLO',\n  'to': 'complexity features',\n  'name': 'features',\n  'research': 'method',\n  'summary': 'CELLO includes eight features for complex instructions and constructs a comprehensive evaluation dataset from real-world scenarios',\n  'date': '2023-09-17',\n  'title': 'Can Large Language Models Understand Real-World Complex Instructions?',\n  'id': '2309.09150'},\n {'from': 'CELLO evaluation criteria',\n  'to': 'CELLO',\n  'name': 'established for',\n  'research': 'method',\n  'summary': 'Four criteria are established for CELLO to address inadequacies of current metrics which are biased or too strict and coarse-grained',\n  'date': '2023-09-17',\n  'title': 'Can Large Language Models Understand Real-World Complex Instructions?',\n  'id': '2309.09150'},\n {'from': 'performance comparison',\n  'to': 'CELLO',\n  'name': 'conducted for',\n  'research': 'method',\n  'summary': 'Extensive experiments are conducted to compare the performance of representative Chinese-oriented and English-oriented models in following complex instructions',\n  'date': '2023-09-17',\n  'title': 'Can Large Language Models Understand Real-World Complex Instructions?',\n  'id': '2309.09150'},\n {'from': 'CELLO resources',\n  'to': 'GitHub',\n  'name': 'available at',\n  'research': 'context',\n  'summary': 'Resources of CELLO are publicly available at the provided GitHub URL',\n  'date': '2023-09-17',\n  'title': 'Can Large Language Models Understand Real-World Complex Instructions?',\n  'id': '2309.09150'},\n {'from': 'robotic cooking',\n  'to': 'task planning',\n  'name': 'involves',\n  'research': 'context',\n  'summary': 'Task planning for robotic cooking involves generating a sequence of actions for a robot to prepare a meal successfully',\n  'date': '2023-09-17',\n  'title': 'From Cooking Recipes to Robot Task Trees -- Improving Planning\\n  Correctness and Task Efficiency by Leveraging LLMs with a Knowledge Network',\n  'id': '2309.09181'},\n {'from': 'task planning',\n  'to': 'task tree',\n  'name': 'produces',\n  'research': 'contribution',\n  'summary': 'The novel task tree generation pipeline produces correct planning and efficient execution for cooking tasks',\n  'date': '2023-09-17',\n  'title': 'From Cooking Recipes to Robot Task Trees -- Improving Planning\\n  Correctness and Task Efficiency by Leveraging LLMs with a Knowledge Network',\n  'id': '2309.09181'},\n {'from': 'large language model',\n  'to': 'recipe instructions',\n  'name': 'used to retrieve',\n  'research': 'method',\n  'summary': 'A large language model is used to retrieve recipe instructions',\n  'date': '2023-09-17',\n  'title': 'From Cooking Recipes to Robot Task Trees -- Improving Planning\\n  Correctness and Task Efficiency by Leveraging LLMs with a Knowledge Network',\n  'id': '2309.09181'},\n {'from': 'GPT-3',\n  'to': 'task tree',\n  'name': 'fine-tuned to convert',\n  'research': 'method',\n  'summary': 'A fine-tuned GPT-3 is used to convert recipe instructions into a task tree',\n  'date': '2023-09-17',\n  'title': 'From Cooking Recipes to Robot Task Trees -- Improving Planning\\n  Correctness and Task Efficiency by Leveraging LLMs with a Knowledge Network',\n  'id': '2309.09181'},\n {'from': 'task tree',\n  'to': 'subtask dependencies',\n  'name': 'captures',\n  'research': 'feature',\n  'summary': 'The task tree captures sequential and parallel dependencies among subtasks',\n  'date': '2023-09-17',\n  'title': 'From Cooking Recipes to Robot Task Trees -- Improving Planning\\n  Correctness and Task Efficiency by Leveraging LLMs with a Knowledge Network',\n  'id': '2309.09181'},\n {'from': 'task tree retrieval',\n  'to': 'LLM output uncertainty',\n  'name': 'mitigates',\n  'research': 'method',\n  'summary': 'Task tree retrieval mitigates the uncertainty and unreliable features of LLM outputs',\n  'date': '2023-09-17',\n  'title': 'From Cooking Recipes to Robot Task Trees -- Improving Planning\\n  Correctness and Task Efficiency by Leveraging LLMs with a Knowledge Network',\n  'id': '2309.09181'},\n {'from': 'task tree',\n  'to': 'task tree retrieval',\n  'name': 'improved by',\n  'research': 'method',\n  'summary': 'Combining multiple LLM task tree outputs into a graph and performing task tree retrieval improves planning correctness and execution efficiency',\n  'date': '2023-09-17',\n  'title': 'From Cooking Recipes to Robot Task Trees -- Improving Planning\\n  Correctness and Task Efficiency by Leveraging LLMs with a Knowledge Network',\n  'id': '2309.09181'},\n {'from': 'performance evaluation',\n  'to': 'task tree',\n  'name': 'shows',\n  'research': 'finding',\n  'summary': 'Evaluation results show superior performance compared to previous works in task planning accuracy and efficiency',\n  'date': '2023-09-17',\n  'title': 'From Cooking Recipes to Robot Task Trees -- Improving Planning\\n  Correctness and Task Efficiency by Leveraging LLMs with a Knowledge Network',\n  'id': '2309.09181'},\n {'from': 'autonomous robotics',\n  'to': 'semantic concept grounding',\n  'name': 'equipped with',\n  'research': 'context',\n  'summary': 'Recent advances have equipped autonomous robots with semantic concept grounding capabilities',\n  'date': '2023-09-17',\n  'title': 'Optimal Scene Graph Planning with Large Language Model Guidance',\n  'id': '2309.09182'},\n {'from': 'semantic concept grounding',\n  'to': 'natural language task interpretation',\n  'name': 'enables',\n  'research': 'context',\n  'summary': 'Semantic concept grounding enables interpretation of natural language tasks',\n  'date': '2023-09-17',\n  'title': 'Optimal Scene Graph Planning with Large Language Model Guidance',\n  'id': '2309.09182'},\n {'from': 'this work',\n  'to': 'hierarchical metric-semantic model',\n  'name': 'aims to leverage',\n  'research': 'goal',\n  'summary': 'This work aims to leverage new capabilities with an efficient task planning algorithm for hierarchical metric-semantic models',\n  'date': '2023-09-17',\n  'title': 'Optimal Scene Graph Planning with Large Language Model Guidance',\n  'id': '2309.09182'},\n {'from': 'scene graph',\n  'to': 'environment representation',\n  'name': 'used for',\n  'research': 'method',\n  'summary': 'A scene graph representation of the environment is considered for task interpretation',\n  'date': '2023-09-17',\n  'title': 'Optimal Scene Graph Planning with Large Language Model Guidance',\n  'id': '2309.09182'},\n {'from': 'large language model',\n  'to': 'Linear Temporal Logic automaton',\n  'name': 'converts',\n  'research': 'method',\n  'summary': 'A large language model is used to convert a natural language task into a Linear Temporal Logic automaton',\n  'date': '2023-09-17',\n  'title': 'Optimal Scene Graph Planning with Large Language Model Guidance',\n  'id': '2309.09182'},\n {'from': 'main contribution',\n  'to': 'hierarchical LTL planning with LLM guidance',\n  'name': 'is',\n  'research': 'contribution',\n  'summary': 'The main contribution is to enable optimal hierarchical LTL planning with LLM guidance over scene graphs',\n  'date': '2023-09-17',\n  'title': 'Optimal Scene Graph Planning with Large Language Model Guidance',\n  'id': '2309.09182'},\n {'from': 'hierarchical planning domain',\n  'to': 'scene graph',\n  'name': 'constructed to capture',\n  'research': 'method',\n  'summary': 'A hierarchical planning domain is constructed to capture attributes and connectivity of the scene graph and the task automaton',\n  'date': '2023-09-17',\n  'title': 'Optimal Scene Graph Planning with Large Language Model Guidance',\n  'id': '2309.09182'},\n {'from': 'LLM heuristic function',\n  'to': 'hierarchical planning domain',\n  'name': 'provides',\n  'research': 'method',\n  'summary': 'An LLM heuristic function provides semantic guidance for planning',\n  'date': '2023-09-17',\n  'title': 'Optimal Scene Graph Planning with Large Language Model Guidance',\n  'id': '2309.09182'},\n {'from': 'LTL heuristic function',\n  'to': 'optimality',\n  'name': 'designed to guarantee',\n  'research': 'method',\n  'summary': 'An LTL heuristic function is designed to guarantee optimality and supplement inadmissible LLM guidance',\n  'date': '2023-09-17',\n  'title': 'Optimal Scene Graph Planning with Large Language Model Guidance',\n  'id': '2309.09182'},\n {'from': 'natural language task planning',\n  'to': 'scene graph',\n  'name': 'demonstrated for',\n  'research': 'result',\n  'summary': 'Efficient planning of complex natural language tasks is demonstrated in scene graphs of virtualized real environments',\n  'date': '2023-09-17',\n  'title': 'Optimal Scene Graph Planning with Large Language Model Guidance',\n  'id': '2309.09182'},\n {'from': 'sequential recommendation',\n  'to': 'research community',\n  'name': 'receiving attention in',\n  'research': 'context',\n  'summary': 'Sequential recommendation problems have received increasing attention in research',\n  'date': '2023-09-17',\n  'title': 'Leveraging Large Language Models for Sequential Recommendation',\n  'id': '2309.09261'},\n {'from': 'large language model',\n  'to': 'sequential recommendation',\n  'name': 'used in',\n  'research': 'contribution',\n  'summary': 'LLMs are used to build or improve sequential recommendation approaches',\n  'date': '2023-09-17',\n  'title': 'Leveraging Large Language Models for Sequential Recommendation',\n  'id': '2309.09261'},\n {'from': 'large language model',\n  'to': 'AI-based applications',\n  'name': 'introduces effects in',\n  'research': 'context',\n  'summary': 'LLMs are introducing disruptive effects in many AI-based applications',\n  'date': '2023-09-17',\n  'title': 'Leveraging Large Language Models for Sequential Recommendation',\n  'id': '2309.09261'},\n {'from': 'LLM-based sequential recommendation',\n  'to': 'sequential recommendation',\n  'name': 'evaluated in',\n  'research': 'method',\n  'summary': 'Three approaches to leverage the power of LLMs in sequential recommendation are devised and evaluated',\n  'date': '2023-09-17',\n  'title': 'Leveraging Large Language Models for Sequential Recommendation',\n  'id': '2309.09261'},\n {'from': 'BERT4Rec',\n  'to': 'LLM embeddings',\n  'name': 'initialized with',\n  'research': 'finding',\n  'summary': 'Initializing BERT4Rec with embeddings from an LLM improves NDCG by 15-20% compared to the vanilla BERT4Rec model',\n  'date': '2023-09-17',\n  'title': 'Leveraging Large Language Models for Sequential Recommendation',\n  'id': '2309.09261'},\n {'from': 'NDCG',\n  'to': 'LLM embeddings',\n  'name': 'improved by',\n  'research': 'finding',\n  'summary': 'NDCG is improved by 15-20% when using LLM embeddings in BERT4Rec',\n  'date': '2023-09-17',\n  'title': 'Leveraging Large Language Models for Sequential Recommendation',\n  'id': '2309.09261'},\n {'from': 'LLM embeddings for recommendations',\n  'to': 'competitive performance',\n  'name': 'provides',\n  'research': 'finding',\n  'summary': 'A simple approach leveraging LLM embeddings for producing recommendations provides competitive performance by highlighting semantically related items',\n  'date': '2023-09-17',\n  'title': 'Leveraging Large Language Models for Sequential Recommendation',\n  'id': '2309.09261'},\n {'from': 'code and data',\n  'to': 'reproducibility',\n  'name': 'shared for',\n  'research': 'practice',\n  'summary': 'The code and data of the experiments are publicly shared to ensure reproducibility',\n  'date': '2023-09-17',\n  'title': 'Leveraging Large Language Models for Sequential Recommendation',\n  'id': '2309.09261'},\n {'from': 'IT operations',\n  'to': 'data management and analysis',\n  'name': 'require',\n  'research': 'context',\n  'summary': 'Efficient management and analysis of large volumes of data is crucial for IT operations',\n  'date': '2023-09-17',\n  'title': 'OWL: A Large Language Model for IT Operations',\n  'id': '2309.09298'},\n {'from': 'natural language processing',\n  'to': 'NLP tasks',\n  'name': 'applied in',\n  'research': 'context',\n  'summary': 'NLP techniques are applied in various tasks such as named entity recognition, machine translation, and dialogue systems',\n  'date': '2023-09-17',\n  'title': 'OWL: A Large Language Model for IT Operations',\n  'id': '2309.09298'},\n {'from': 'large language model',\n  'to': 'NLP tasks',\n  'name': 'achieves improvements in',\n  'research': 'context',\n  'summary': 'LLMs have achieved significant improvements across various NLP downstream tasks',\n  'date': '2023-09-17',\n  'title': 'OWL: A Large Language Model for IT Operations',\n  'id': '2309.09298'},\n {'from': 'large language model',\n  'to': 'IT operations',\n  'name': 'lack of specialization for',\n  'research': 'problem',\n  'summary': 'There is a lack of specialized LLMs for IT operations',\n  'date': '2023-09-17',\n  'title': 'OWL: A Large Language Model for IT Operations',\n  'id': '2309.09298'},\n {'from': 'OWL',\n  'to': 'IT operations',\n  'name': 'introduced for',\n  'research': 'contribution',\n  'summary': 'OWL is a large language model trained on the OWL-instruct dataset for IT-related information',\n  'date': '2023-09-17',\n  'title': 'OWL: A Large Language Model for IT Operations',\n  'id': '2309.09298'},\n {'from': 'OWL-instruct',\n  'to': 'OWL',\n  'name': 'used for training',\n  'research': 'method',\n  'summary': 'OWL-instruct dataset is used for training the OWL model with IT-related information',\n  'date': '2023-09-17',\n  'title': 'OWL: A Large Language Model for IT Operations',\n  'id': '2309.09298'},\n {'from': 'mixture-of-adapter',\n  'to': 'OWL',\n  'name': 'proposed to improve',\n  'research': 'method',\n  'summary': 'Mixture-of-adapter strategy is proposed to improve parameter-efficient tuning across different domains or tasks',\n  'date': '2023-09-17',\n  'title': 'OWL: A Large Language Model for IT Operations',\n  'id': '2309.09298'},\n {'from': 'OWL-bench',\n  'to': 'OWL',\n  'name': 'established to evaluate',\n  'research': 'method',\n  'summary': 'OWL-bench is established to evaluate the performance of the OWL model',\n  'date': '2023-09-17',\n  'title': 'OWL: A Large Language Model for IT Operations',\n  'id': '2309.09298'},\n {'from': 'OWL',\n  'to': 'IT-related benchmarks',\n  'name': 'outperforms',\n  'research': 'finding',\n  'summary': 'OWL demonstrates superior performance results on IT tasks, outperforming existing models by significant margins',\n  'date': '2023-09-17',\n  'title': 'OWL: A Large Language Model for IT Operations',\n  'id': '2309.09298'},\n {'from': 'this paper',\n  'to': 'IT operations',\n  'name': 'aims to provide insights for',\n  'research': 'reflection',\n  'summary': 'The findings of this work aim to provide insights to revolutionize IT operations with specialized LLMs',\n  'date': '2023-09-17',\n  'title': 'OWL: A Large Language Model for IT Operations',\n  'id': '2309.09298'},\n {'from': 'automated program repair',\n  'to': 'fixing software bugs',\n  'name': 'aims to',\n  'research': 'context',\n  'summary': 'Automated program repair aims to fix software bugs without human intervention',\n  'date': '2023-09-17',\n  'title': 'GAMMA: Revisiting Template-based Automated Program Repair via Mask\\n  Prediction',\n  'id': '2309.09308'},\n {'from': 'template-based automated program repair',\n  'to': 'fixing software bugs',\n  'name': 'investigated for',\n  'research': 'context',\n  'summary': 'Template-based APR has been widely investigated with promising results',\n  'date': '2023-09-17',\n  'title': 'GAMMA: Revisiting Template-based Automated Program Repair via Mask\\n  Prediction',\n  'id': '2309.09308'},\n {'from': 'selecting appropriate donor code',\n  'to': 'template-based automated program repair',\n  'name': 'limits',\n  'research': 'context',\n  'summary': 'Inappropriate donor code may cause plausible but incorrect patch generation, limiting the repair performance',\n  'date': '2023-09-17',\n  'title': 'GAMMA: Revisiting Template-based Automated Program Repair via Mask\\n  Prediction',\n  'id': '2309.09308'},\n {'from': 'GAMMA',\n  'to': 'template-based automated program repair',\n  'name': 'proposed for',\n  'research': 'contribution',\n  'summary': 'GAMMA is proposed to leverage large pre-trained language models for donor code generation in template-based APR',\n  'date': '2023-09-17',\n  'title': 'GAMMA: Revisiting Template-based Automated Program Repair via Mask\\n  Prediction',\n  'id': '2309.09308'},\n {'from': 'cloze task',\n  'to': 'GAMMA',\n  'name': 'used by',\n  'research': 'method',\n  'summary': 'GAMMA predicts the correct code tokens based on context code snippets and repair patterns by a cloze task',\n  'date': '2023-09-17',\n  'title': 'GAMMA: Revisiting Template-based Automated Program Repair via Mask\\n  Prediction',\n  'id': '2309.09308'},\n {'from': 'fix templates',\n  'to': 'GAMMA',\n  'name': 'revised and transformed by',\n  'research': 'method',\n  'summary': 'GAMMA revises and transforms fix templates into mask patterns',\n  'date': '2023-09-17',\n  'title': 'GAMMA: Revisiting Template-based Automated Program Repair via Mask\\n  Prediction',\n  'id': '2309.09308'},\n {'from': 'pre-trained language model',\n  'to': 'GAMMA',\n  'name': 'adopted by',\n  'research': 'method',\n  'summary': 'GAMMA adopts a pre-trained language model to predict the correct code for masked code',\n  'date': '2023-09-17',\n  'title': 'GAMMA: Revisiting Template-based Automated Program Repair via Mask\\n  Prediction',\n  'id': '2309.09308'},\n {'from': 'GAMMA',\n  'to': 'TBAR',\n  'name': 'outperforms',\n  'research': 'finding',\n  'summary': 'GAMMA correctly repairs more bugs than previous state-of-the-art approaches TBAR and Recoder',\n  'date': '2023-09-17',\n  'title': 'GAMMA: Revisiting Template-based Automated Program Repair via Mask\\n  Prediction',\n  'id': '2309.09308'},\n {'from': 'GAMMA',\n  'to': 'Recoder',\n  'name': 'outperforms',\n  'research': 'finding',\n  'summary': 'GAMMA correctly repairs more bugs than previous state-of-the-art approaches TBAR and Recoder',\n  'date': '2023-09-17',\n  'title': 'GAMMA: Revisiting Template-based Automated Program Repair via Mask\\n  Prediction',\n  'id': '2309.09308'},\n {'from': 'GAMMA',\n  'to': 'Defects4J-v2.0 and QuixBugs',\n  'name': 'demonstrates generalizability',\n  'research': 'finding',\n  'summary': 'GAMMA repairs bugs from additional datasets, indicating its generalizability in addressing the dataset overfitting issue',\n  'date': '2023-09-17',\n  'title': 'GAMMA: Revisiting Template-based Automated Program Repair via Mask\\n  Prediction',\n  'id': '2309.09308'},\n {'from': 'CodeBERT-based GAMMA',\n  'to': 'GAMMA',\n  'name': 'provides advancement',\n  'research': 'finding',\n  'summary': 'Adopting CodeBERT-based GAMMA can fix a substantial number of bugs, indicating scalability',\n  'date': '2023-09-17',\n  'title': 'GAMMA: Revisiting Template-based Automated Program Repair via Mask\\n  Prediction',\n  'id': '2309.09308'},\n {'from': 'ChatGPT-based GAMMA',\n  'to': 'GAMMA',\n  'name': 'provides advancement',\n  'research': 'finding',\n  'summary': 'Adopting ChatGPT-based GAMMA can fix a substantial number of bugs, indicating scalability',\n  'date': '2023-09-17',\n  'title': 'GAMMA: Revisiting Template-based Automated Program Repair via Mask\\n  Prediction',\n  'id': '2309.09308'},\n {'from': 'this study',\n  'to': 'using pre-trained models for patch generation',\n  'name': 'highlights',\n  'research': 'reflection',\n  'summary': 'The study highlights the promising future of adopting pre-trained models to generate correct patches on top of fix patterns',\n  'date': '2023-09-17',\n  'title': 'GAMMA: Revisiting Template-based Automated Program Repair via Mask\\n  Prediction',\n  'id': '2309.09308'},\n {'from': 'automated short answer grading',\n  'to': 'machine learning',\n  'name': 'is an area of',\n  'research': 'context',\n  'summary': 'Automated short answer grading (ASAG) has been an active area of machine-learning research for over a decade',\n  'date': '2023-09-17',\n  'title': 'Performance of the Pre-Trained Large Language Model GPT-4 on Automated\\n  Short Answer Grading',\n  'id': '2309.09338'},\n {'from': 'automated short answer grading',\n  'to': 'educational scalability',\n  'name': 'promises to',\n  'research': 'context',\n  'summary': 'ASAG promises to let educators grade and give feedback on free-form responses in large-enrollment courses despite limited human graders',\n  'date': '2023-09-17',\n  'title': 'Performance of the Pre-Trained Large Language Model GPT-4 on Automated\\n  Short Answer Grading',\n  'id': '2309.09338'},\n {'from': 'large language model',\n  'to': 'commoditization of LLMs',\n  'name': 'emerged as',\n  'research': 'context',\n  'summary': 'Pre-trained large language models (LLMs) emerged as a commodity',\n  'date': '2023-09-17',\n  'title': 'Performance of the Pre-Trained Large Language Model GPT-4 on Automated\\n  Short Answer Grading',\n  'id': '2309.09338'},\n {'from': 'GPT-4',\n  'to': 'SciEntsBank and Beetle',\n  'name': 'studied for performance on',\n  'research': 'contribution',\n  'summary': 'The performance of GPT-4 was studied on standard benchmark datasets SciEntsBank and Beetle',\n  'date': '2023-09-17',\n  'title': 'Performance of the Pre-Trained Large Language Model GPT-4 on Automated\\n  Short Answer Grading',\n  'id': '2309.09338'},\n {'from': 'grading alignment with reference answer',\n  'to': 'automated short answer grading',\n  'name': 'is a standard task in',\n  'research': 'context',\n  'summary': 'Grading the alignment of the student answer with a reference answer is a standard task in ASAG',\n  'date': '2023-09-17',\n  'title': 'Performance of the Pre-Trained Large Language Model GPT-4 on Automated\\n  Short Answer Grading',\n  'id': '2309.09338'},\n {'from': 'GPT-4',\n  'to': 'hand-engineered model',\n  'name': 'compared to',\n  'research': 'finding',\n  'summary': 'The performance of the pre-trained general-purpose GPT-4 LLM is comparable to hand-engineered models but worse than pre-trained LLMs with specialized training',\n  'date': '2023-09-17',\n  'title': 'Performance of the Pre-Trained Large Language Model GPT-4 on Automated\\n  Short Answer Grading',\n  'id': '2309.09338'},\n {'from': 'GPT-4',\n  'to': 'specialized trained large language model',\n  'name': 'compared to',\n  'research': 'finding',\n  'summary': 'The performance of the pre-trained general-purpose GPT-4 LLM is comparable to hand-engineered models but worse than pre-trained LLMs with specialized training',\n  'date': '2023-09-17',\n  'title': 'Performance of the Pre-Trained Large Language Model GPT-4 on Automated\\n  Short Answer Grading',\n  'id': '2309.09338'},\n {'from': 'GPT-4',\n  'to': 'withholding reference answer',\n  'name': 'investigated for',\n  'research': 'method',\n  'summary': 'The study also investigated the performance of GPT-4 when withholding the reference answer',\n  'date': '2023-09-17',\n  'title': 'Performance of the Pre-Trained Large Language Model GPT-4 on Automated\\n  Short Answer Grading',\n  'id': '2309.09338'},\n {'from': 'basic messaging and phone calls',\n  'to': 'communication inefficiencies',\n  'name': 'suffer from',\n  'research': 'context',\n  'summary': 'Basic messaging and phone calls suffer from limited availability, information loss, and process inefficiencies',\n  'date': '2023-09-17',\n  'title': 'Talk2Care: Facilitating Asynchronous Patient-Provider Communication with\\n  Large-Language-Model',\n  'id': '2309.09357'},\n {'from': 'LLM-powered communication system',\n  'to': 'patient-provider communication',\n  'name': 'proposed for',\n  'research': 'context',\n  'summary': 'A promising solution to facilitate patient-provider communication is to leverage LLMs with their powerful natural conversation and summarization capability',\n  'date': '2023-09-17',\n  'title': 'Talk2Care: Facilitating Asynchronous Patient-Provider Communication with\\n  Large-Language-Model',\n  'id': '2309.09357'},\n {'from': 'large language model',\n  'to': 'patient-provider communication',\n  'name': 'role in',\n  'research': 'question',\n  'summary': \"There is a limited understanding of LLMs' role during patient-provider communication\",\n  'date': '2023-09-17',\n  'title': 'Talk2Care: Facilitating Asynchronous Patient-Provider Communication with\\n  Large-Language-Model',\n  'id': '2309.09357'},\n {'from': 'older adults',\n  'to': 'large language model',\n  'name': 'needs and opportunities studied for',\n  'research': 'method',\n  'summary': 'Interview studies conducted with older adults to understand their needs and opportunities for LLMs in asynchronous communication',\n  'date': '2023-09-17',\n  'title': 'Talk2Care: Facilitating Asynchronous Patient-Provider Communication with\\n  Large-Language-Model',\n  'id': '2309.09357'},\n {'from': 'healthcare providers',\n  'to': 'large language model',\n  'name': 'needs and opportunities studied for',\n  'research': 'method',\n  'summary': 'Interview studies conducted with healthcare providers to understand their needs and opportunities for LLMs in asynchronous communication',\n  'date': '2023-09-17',\n  'title': 'Talk2Care: Facilitating Asynchronous Patient-Provider Communication with\\n  Large-Language-Model',\n  'id': '2309.09357'},\n {'from': 'Talk2Care',\n  'to': 'interview study insights',\n  'name': 'developed based on',\n  'research': 'contribution',\n  'summary': 'Talk2Care, an LLM-powered communication system, was built based on insights from interview studies',\n  'date': '2023-09-17',\n  'title': 'Talk2Care: Facilitating Asynchronous Patient-Provider Communication with\\n  Large-Language-Model',\n  'id': '2309.09357'},\n {'from': 'LLM-powered VA interface',\n  'to': 'older adults',\n  'name': 'designed for',\n  'research': 'contribution',\n  'summary': 'An LLM-powered VA interface was designed for older adults for effective information collection',\n  'date': '2023-09-17',\n  'title': 'Talk2Care: Facilitating Asynchronous Patient-Provider Communication with\\n  Large-Language-Model',\n  'id': '2309.09357'},\n {'from': 'LLM-based dashboard',\n  'to': 'healthcare providers',\n  'name': 'designed for',\n  'research': 'contribution',\n  'summary': 'An LLM-based dashboard was built for healthcare providers to summarize and present important health information',\n  'date': '2023-09-17',\n  'title': 'Talk2Care: Facilitating Asynchronous Patient-Provider Communication with\\n  Large-Language-Model',\n  'id': '2309.09357'},\n {'from': 'Talk2Care',\n  'to': 'older adults and healthcare providers',\n  'name': 'evaluated by',\n  'research': 'method',\n  'summary': 'User studies with older adults and providers were conducted to evaluate the usability of Talk2Care',\n  'date': '2023-09-17',\n  'title': 'Talk2Care: Facilitating Asynchronous Patient-Provider Communication with\\n  Large-Language-Model',\n  'id': '2309.09357'},\n {'from': 'Talk2Care',\n  'to': 'patient-provider communication',\n  'name': 'facilitates',\n  'research': 'finding',\n  'summary': \"Talk2Care could facilitate the communication process, enrich health information collection, and save providers' efforts and time\",\n  'date': '2023-09-17',\n  'title': 'Talk2Care: Facilitating Asynchronous Patient-Provider Communication with\\n  Large-Language-Model',\n  'id': '2309.09357'},\n {'from': 'this research',\n  'to': 'healthcare and interpersonal communication',\n  'name': 'envisioned as',\n  'research': 'reflection',\n  'summary': \"The work is seen as an initial exploration of LLMs' capability in healthcare and interpersonal communication\",\n  'date': '2023-09-17',\n  'title': 'Talk2Care: Facilitating Asynchronous Patient-Provider Communication with\\n  Large-Language-Model',\n  'id': '2309.09357'},\n {'from': 'large language model',\n  'to': 'healthcare',\n  'name': 'potential tool for',\n  'research': 'context',\n  'summary': 'LLMs are becoming increasingly relevant as a potential tool for healthcare',\n  'date': '2023-09-17',\n  'title': 'Language models are susceptible to incorrect patient self-diagnosis in\\n  medical applications',\n  'id': '2309.09362'},\n {'from': 'healthcare',\n  'to': 'clinicians, researchers, patients',\n  'name': 'communication between',\n  'research': 'context',\n  'summary': 'LLMs aid communication between clinicians, researchers, and patients in healthcare',\n  'date': '2023-09-17',\n  'title': 'Language models are susceptible to incorrect patient self-diagnosis in\\n  medical applications',\n  'id': '2309.09362'},\n {'from': 'medical exam questions',\n  'to': 'patient-doctor interactions',\n  'name': 'does not reflect',\n  'research': 'problem',\n  'summary': 'Traditional evaluations of LLMs on medical exam questions do not reflect the complexity of real patient-doctor interactions',\n  'date': '2023-09-17',\n  'title': 'Language models are susceptible to incorrect patient self-diagnosis in\\n  medical applications',\n  'id': '2309.09362'},\n {'from': 'patient self-diagnosis',\n  'to': 'patient-doctor interactions',\n  'name': 'introduces complexity to',\n  'research': 'context',\n  'summary': 'Patient self-diagnosis introduces complexity to patient-doctor interactions',\n  'date': '2023-09-17',\n  'title': 'Language models are susceptible to incorrect patient self-diagnosis in\\n  medical applications',\n  'id': '2309.09362'},\n {'from': 'patient self-diagnosis',\n  'to': 'misdiagnosis',\n  'name': 'leads to',\n  'research': 'finding',\n  'summary': 'Patient self-diagnosis often leads to misdiagnosis due to over-emphasis on bias validating information',\n  'date': '2023-09-17',\n  'title': 'Language models are susceptible to incorrect patient self-diagnosis in\\n  medical applications',\n  'id': '2309.09362'},\n {'from': 'modified medical board exam questions',\n  'to': 'patient self-diagnosis',\n  'name': 'include',\n  'research': 'method',\n  'summary': 'Modified medical board exam questions include self-diagnostic reports from patients to test LLMs',\n  'date': '2023-09-17',\n  'title': 'Language models are susceptible to incorrect patient self-diagnosis in\\n  medical applications',\n  'id': '2309.09362'},\n {'from': 'large language model',\n  'to': 'patient self-diagnosis',\n  'name': 'diagnostic accuracy drops due to',\n  'research': 'finding',\n  'summary': 'The diagnostic accuracy of LLMs drops dramatically when a patient proposes incorrect bias-validating information',\n  'date': '2023-09-17',\n  'title': 'Language models are susceptible to incorrect patient self-diagnosis in\\n  medical applications',\n  'id': '2309.09362'},\n {'from': 'large language model',\n  'to': 'errors in self-diagnosis',\n  'name': 'susceptible to',\n  'research': 'finding',\n  'summary': 'LLMs show a high susceptibility to errors in self-diagnosis',\n  'date': '2023-09-17',\n  'title': 'Language models are susceptible to incorrect patient self-diagnosis in\\n  medical applications',\n  'id': '2309.09362'},\n {'from': 'multi-document news summarization',\n  'to': 'agreement-based summarization',\n  'name': 'focuses on',\n  'research': 'context',\n  'summary': 'Previous research typically concentrated on collating information that all sources agree upon',\n  'date': '2023-09-17',\n  'title': 'Embrace Divergence for Richer Insights: A Multi-document Summarization\\n  Benchmark and a Case Study on Summarizing Diverse Information from News\\n  Articles',\n  'id': '2309.09369'},\n {'from': 'summarizing diverse information',\n  'to': 'multi-document news summarization',\n  'name': 'not previously investigated',\n  'research': 'gap',\n  'summary': 'The summarization of diverse information dispersed across multiple articles about an event has not been previously investigated',\n  'date': '2023-09-17',\n  'title': 'Embrace Divergence for Richer Insights: A Multi-document Summarization\\n  Benchmark and a Case Study on Summarizing Diverse Information from News\\n  Articles',\n  'id': '2309.09369'},\n {'from': 'summarizing diverse information',\n  'to': 'DiverseSumm',\n  'name': 'proposed as new task',\n  'research': 'contribution',\n  'summary': 'A new task of summarizing diverse information encountered in multiple news articles about the same event is proposed',\n  'date': '2023-09-17',\n  'title': 'Embrace Divergence for Richer Insights: A Multi-document Summarization\\n  Benchmark and a Case Study on Summarizing Diverse Information from News\\n  Articles',\n  'id': '2309.09369'},\n {'from': 'DiverseSumm',\n  'to': 'summarizing diverse information',\n  'name': 'created for',\n  'research': 'contribution',\n  'summary': 'DiverseSumm dataset includes 245 news stories, each with 10 articles and a human-validated reference, to facilitate the task of summarizing diverse information',\n  'date': '2023-09-17',\n  'title': 'Embrace Divergence for Richer Insights: A Multi-document Summarization\\n  Benchmark and a Case Study on Summarizing Diverse Information from News\\n  Articles',\n  'id': '2309.09369'},\n {'from': 'position and verbosity biases',\n  'to': 'LLM-based evaluation metrics',\n  'name': 'analyzed for',\n  'research': 'method',\n  'summary': 'A comprehensive analysis was conducted to pinpoint position and verbosity biases in LLM-based metrics for evaluating summaries',\n  'date': '2023-09-17',\n  'title': 'Embrace Divergence for Richer Insights: A Multi-document Summarization\\n  Benchmark and a Case Study on Summarizing Diverse Information from News\\n  Articles',\n  'id': '2309.09369'},\n {'from': 'LLM-based evaluation metrics',\n  'to': 'human assessments',\n  'name': 'correlation with',\n  'research': 'finding',\n  'summary': 'The correlation of LLM-based metrics with human assessments was analyzed',\n  'date': '2023-09-17',\n  'title': 'Embrace Divergence for Richer Insights: A Multi-document Summarization\\n  Benchmark and a Case Study on Summarizing Diverse Information from News\\n  Articles',\n  'id': '2309.09369'},\n {'from': 'large language model',\n  'to': 'summarizing diverse information',\n  'name': 'analyzed for',\n  'research': 'method',\n  'summary': 'LLMs were analyzed to see how they summarize multiple news articles and which type of diverse information they can identify',\n  'date': '2023-09-17',\n  'title': 'Embrace Divergence for Richer Insights: A Multi-document Summarization\\n  Benchmark and a Case Study on Summarizing Diverse Information from News\\n  Articles',\n  'id': '2309.09369'},\n {'from': 'GPT-4',\n  'to': 'limited coverage in summarization',\n  'name': 'limited by',\n  'research': 'finding',\n  'summary': 'GPT-4 is only able to cover less than 40% of the diverse information on average, indicating a challenge in summarizing diverse information',\n  'date': '2023-09-17',\n  'title': 'Embrace Divergence for Richer Insights: A Multi-document Summarization\\n  Benchmark and a Case Study on Summarizing Diverse Information from News\\n  Articles',\n  'id': '2309.09369'},\n {'from': 'large language model',\n  'to': 'spurious correlations',\n  'name': 'relies on',\n  'research': 'finding',\n  'summary': 'Large language models rely on spurious correlations in the data for NLU tasks',\n  'date': '2023-09-17',\n  'title': 'Mitigating Shortcuts in Language Models with Soft Label Encoding',\n  'id': '2309.09380'},\n {'from': 'natural language understanding',\n  'to': 'spurious correlations',\n  'name': 'affected by',\n  'research': 'context',\n  'summary': 'NLU tasks are affected by spurious correlations in the data',\n  'date': '2023-09-17',\n  'title': 'Mitigating Shortcuts in Language Models with Soft Label Encoding',\n  'id': '2309.09380'},\n {'from': 'reducing spurious correlations',\n  'to': 'spurious correlations',\n  'name': 'explores',\n  'research': 'objective',\n  'summary': 'The research question explores whether modifying ground truth labels of training data can reduce spurious correlations',\n  'date': '2023-09-17',\n  'title': 'Mitigating Shortcuts in Language Models with Soft Label Encoding',\n  'id': '2309.09380'},\n {'from': 'Soft Label Encoding (SoftLE)',\n  'to': 'spurious correlations',\n  'name': 'proposed to',\n  'research': 'contribution',\n  'summary': 'SoftLE is proposed as a debiasing framework to reduce spurious correlations',\n  'date': '2023-09-17',\n  'title': 'Mitigating Shortcuts in Language Models with Soft Label Encoding',\n  'id': '2309.09380'},\n {'from': 'teacher model',\n  'to': 'hard label training',\n  'name': 'trained with',\n  'research': 'method',\n  'summary': \"A teacher model is trained with hard labels to determine each sample's degree of relying on shortcuts\",\n  'date': '2023-09-17',\n  'title': 'Mitigating Shortcuts in Language Models with Soft Label Encoding',\n  'id': '2309.09380'},\n {'from': 'soft label training',\n  'to': 'dummy class encoding',\n  'name': 'involves',\n  'research': 'method',\n  'summary': 'Soft label training involves adding a dummy class to encode the shortcut degree and smoothing other dimensions in the ground truth label',\n  'date': '2023-09-17',\n  'title': 'Mitigating Shortcuts in Language Models with Soft Label Encoding',\n  'id': '2309.09380'},\n {'from': 'student model',\n  'to': 'soft label training',\n  'name': 'trained with',\n  'research': 'method',\n  'summary': 'A more robust student model is trained with the new ground truth label generated by SoftLE',\n  'date': '2023-09-17',\n  'title': 'Mitigating Shortcuts in Language Models with Soft Label Encoding',\n  'id': '2309.09380'},\n {'from': 'Soft Label Encoding (SoftLE)',\n  'to': 'out-of-distribution generalization',\n  'name': 'improves',\n  'research': 'finding',\n  'summary': 'SoftLE significantly improves out-of-distribution generalization',\n  'date': '2023-09-17',\n  'title': 'Mitigating Shortcuts in Language Models with Soft Label Encoding',\n  'id': '2309.09380'},\n {'from': 'Soft Label Encoding (SoftLE)',\n  'to': 'in-distribution accuracy',\n  'name': 'maintains',\n  'research': 'finding',\n  'summary': 'SoftLE maintains satisfactory in-distribution accuracy',\n  'date': '2023-09-17',\n  'title': 'Mitigating Shortcuts in Language Models with Soft Label Encoding',\n  'id': '2309.09380'},\n {'from': 'extensive experiments',\n  'to': 'Soft Label Encoding (SoftLE)',\n  'name': 'conducted to',\n  'research': 'method',\n  'summary': 'Extensive experiments are conducted on two NLU benchmark tasks to demonstrate the effectiveness of SoftLE',\n  'date': '2023-09-17',\n  'title': 'Mitigating Shortcuts in Language Models with Soft Label Encoding',\n  'id': '2309.09380'},\n {'from': 'spoken semantic parsing',\n  'to': 'speech-transcript-semantic parse data',\n  'name': 'requires',\n  'research': 'context',\n  'summary': 'Training robust models for SSP requires triplets of speech-transcript-semantic parse data',\n  'date': '2023-09-17',\n  'title': 'Augmenting text for spoken language understanding with Large Language\\n  Models',\n  'id': '2309.09390'},\n {'from': 'speech-transcript-semantic parse data',\n  'to': 'data acquisition cost',\n  'name': 'is expensive to obtain',\n  'research': 'challenge',\n  'summary': 'Obtaining speech-transcript-semantic parse data is expensive',\n  'date': '2023-09-17',\n  'title': 'Augmenting text for spoken language understanding with Large Language\\n  Models',\n  'id': '2309.09390'},\n {'from': 'using unpaired text',\n  'to': 'spoken semantic parsing',\n  'name': 'addresses',\n  'research': 'contribution',\n  'summary': 'The paper examines methods that can use transcript-semantic parse data without corresponding speech to address the challenge in SSP',\n  'date': '2023-09-17',\n  'title': 'Augmenting text for spoken language understanding with Large Language\\n  Models',\n  'id': '2309.09390'},\n {'from': 'joint audio text',\n  'to': 'text-to-speech',\n  'name': 'compared with',\n  'research': 'method',\n  'summary': 'JAT is compared with TTS as ways to generate speech representations for unpaired text',\n  'date': '2023-09-17',\n  'title': 'Augmenting text for spoken language understanding with Large Language\\n  Models',\n  'id': '2309.09390'},\n {'from': 'STOP',\n  'to': 'using unpaired text',\n  'name': 'used in experiments',\n  'research': 'method',\n  'summary': 'Experiments on the STOP dataset show improvements in performance using unpaired text',\n  'date': '2023-09-17',\n  'title': 'Augmenting text for spoken language understanding with Large Language\\n  Models',\n  'id': '2309.09390'},\n {'from': 'exact match',\n  'to': 'using unpaired text',\n  'name': 'improved by',\n  'research': 'finding',\n  'summary': 'Performance by absolute exact match (EM) is improved by 2% and 30% for existing and new domains respectively using unpaired text',\n  'date': '2023-09-17',\n  'title': 'Augmenting text for spoken language understanding with Large Language\\n  Models',\n  'id': '2309.09390'},\n {'from': 'large language model',\n  'to': 'unpaired text',\n  'name': 'used to generate',\n  'research': 'contribution',\n  'summary': 'LLMs are prompted to generate unpaired text for existing and new domains when such text is not available in existing corpora',\n  'date': '2023-09-17',\n  'title': 'Augmenting text for spoken language understanding with Large Language\\n  Models',\n  'id': '2309.09390'},\n {'from': 'LaMDA 2.0',\n  'to': 'unpaired text',\n  'name': 'used for',\n  'research': 'method',\n  'summary': 'LaMDA 2.0 is used to generate examples and words that co-occur with intents to create unpaired text',\n  'date': '2023-09-17',\n  'title': 'Augmenting text for spoken language understanding with Large Language\\n  Models',\n  'id': '2309.09390'},\n {'from': 'using generated text',\n  'to': 'spoken semantic parsing',\n  'name': 'improves',\n  'research': 'finding',\n  'summary': 'Using generated text with JAT and TTS for SSP improves EM on STOP by 1.4% and 2.6% absolute for existing and new domains respectively',\n  'date': '2023-09-17',\n  'title': 'Augmenting text for spoken language understanding with Large Language\\n  Models',\n  'id': '2309.09390'},\n {'from': 'large language model',\n  'to': 'artificial intelligence systems',\n  'name': 'deployed in',\n  'research': 'context',\n  'summary': 'Large language models are increasingly deployed within AI systems integrated with human society',\n  'date': '2023-09-17',\n  'title': 'Do Large GPT Models Discover Moral Dimensions in Language\\n  Representations? A Topological Study Of Sentence Embeddings',\n  'id': '2309.09397'},\n {'from': 'GPT-3.5',\n  'to': 'language representations',\n  'name': 'induces',\n  'research': 'context',\n  'summary': 'GPT-3.5 induces informative language representations from raw text data during pre-training on trillions of words',\n  'date': '2023-09-17',\n  'title': 'Do Large GPT Models Discover Moral Dimensions in Language\\n  Representations? A Topological Study Of Sentence Embeddings',\n  'id': '2309.09397'},\n {'from': 'language representations',\n  'to': 'vector spaces',\n  'name': 'exist in',\n  'research': 'context',\n  'summary': 'Language representations exist in vector spaces of several thousand dimensions',\n  'date': '2023-09-17',\n  'title': 'Do Large GPT Models Discover Moral Dimensions in Language\\n  Representations? A Topological Study Of Sentence Embeddings',\n  'id': '2309.09397'},\n {'from': 'vector spaces',\n  'to': 'mapping',\n  'name': 'involve',\n  'research': 'context',\n  'summary': 'Processing of language representations involves mapping between multiple vector spaces',\n  'date': '2023-09-17',\n  'title': 'Do Large GPT Models Discover Moral Dimensions in Language\\n  Representations? A Topological Study Of Sentence Embeddings',\n  'id': '2309.09397'},\n {'from': 'large language model',\n  'to': 'parameters',\n  'name': 'has parameters',\n  'research': 'context',\n  'summary': 'LLMs have a total number of parameters on the order of trillions',\n  'date': '2023-09-17',\n  'title': 'Do Large GPT Models Discover Moral Dimensions in Language\\n  Representations? A Topological Study Of Sentence Embeddings',\n  'id': '2309.09397'},\n {'from': 'gradient optimization',\n  'to': 'black box system',\n  'name': 'results in',\n  'research': 'context',\n  'summary': 'Gradient optimization results in a black box system that is hard to interpret',\n  'date': '2023-09-17',\n  'title': 'Do Large GPT Models Discover Moral Dimensions in Language\\n  Representations? A Topological Study Of Sentence Embeddings',\n  'id': '2309.09397'},\n {'from': 'ChatGPT',\n  'to': 'neuronal activity topology',\n  'name': 'analyzed for',\n  'research': 'contribution',\n  'summary': \"The paper analyzes the topological structure of neuronal activity in ChatGPT's foundation language model with respect to fairness\",\n  'date': '2023-09-17',\n  'title': 'Do Large GPT Models Discover Moral Dimensions in Language\\n  Representations? A Topological Study Of Sentence Embeddings',\n  'id': '2309.09397'},\n {'from': 'fairness metric',\n  'to': 'social psychology',\n  'name': 'inspired by',\n  'research': 'method',\n  'summary': 'A fairness metric is computed, inspired by social psychology literature, to identify factors influencing fairness assessments in humans',\n  'date': '2023-09-17',\n  'title': 'Do Large GPT Models Discover Moral Dimensions in Language\\n  Representations? A Topological Study Of Sentence Embeddings',\n  'id': '2309.09397'},\n {'from': 'simplicial complex',\n  'to': 'manifold shape',\n  'name': 'summarizes',\n  'research': 'method',\n  'summary': \"The manifold's shape is summarized using a lower-dimensional simplicial complex, whose topology is derived from the fairness metric\",\n  'date': '2023-09-17',\n  'title': 'Do Large GPT Models Discover Moral Dimensions in Language\\n  Representations? A Topological Study Of Sentence Embeddings',\n  'id': '2309.09397'},\n {'from': 'sentence manifold',\n  'to': 'heat map visualization',\n  'name': 'visualized with',\n  'research': 'contribution',\n  'summary': 'Human-readable visualizations of the high-dimensional sentence manifold are produced using a heat map associated with the fairness metric',\n  'date': '2023-09-17',\n  'title': 'Do Large GPT Models Discover Moral Dimensions in Language\\n  Representations? A Topological Study Of Sentence Embeddings',\n  'id': '2309.09397'},\n {'from': 'GPT-3.5',\n  'to': 'submanifolds',\n  'name': 'embeddings decomposed into',\n  'research': 'finding',\n  'summary': 'Sentence embeddings based on GPT-3.5 can be decomposed into two submanifolds corresponding to fair and unfair moral judgments',\n  'date': '2023-09-17',\n  'title': 'Do Large GPT Models Discover Moral Dimensions in Language\\n  Representations? A Topological Study Of Sentence Embeddings',\n  'id': '2309.09397'},\n {'from': 'large language model',\n  'to': 'moral dimension',\n  'name': 'develops',\n  'research': 'finding',\n  'summary': 'GPT-based language models develop a moral dimension within their representation spaces and induce an understanding of fairness during their training process',\n  'date': '2023-09-17',\n  'title': 'Do Large GPT Models Discover Moral Dimensions in Language\\n  Representations? A Topological Study Of Sentence Embeddings',\n  'id': '2309.09397'},\n {'from': 'large language model',\n  'to': 'model size and training dataset',\n  'name': 'driven by',\n  'research': 'context',\n  'summary': 'The development of LLMs is driven by their colossal model sizes and extensive training datasets',\n  'date': '2023-09-17',\n  'title': 'CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large\\n  Language Models in 167 Languages',\n  'id': '2309.09400'},\n {'from': 'Natural Language Processing (NLP)',\n  'to': 'large language model',\n  'name': 'progress in',\n  'research': 'context',\n  'summary': 'Progress in NLP has led to LLMs being made accessible to the public',\n  'date': '2023-09-17',\n  'title': 'CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large\\n  Language Models in 167 Languages',\n  'id': '2309.09400'},\n {'from': 'large language model',\n  'to': 'lack of transparency',\n  'name': 'training datasets',\n  'research': 'challenge',\n  'summary': 'Training datasets for state-of-the-art LLMs are often not fully disclosed, hampering research on hallucination and bias issues',\n  'date': '2023-09-17',\n  'title': 'CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large\\n  Language Models in 167 Languages',\n  'id': '2309.09400'},\n {'from': 'training data creation',\n  'to': 'cleaning and deduplication',\n  'name': 'involves',\n  'research': 'context',\n  'summary': 'Creating training data for high-performing LLMs involves extensive cleaning and deduplication',\n  'date': '2023-09-17',\n  'title': 'CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large\\n  Language Models in 167 Languages',\n  'id': '2309.09400'},\n {'from': 'lack of transparency',\n  'to': 'NLP community',\n  'name': 'hampers',\n  'research': 'challenge',\n  'summary': 'Lack of transparency in training data hampers replication efforts and advancements in the NLP community',\n  'date': '2023-09-17',\n  'title': 'CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large\\n  Language Models in 167 Languages',\n  'id': '2309.09400'},\n {'from': 'multilingual learning',\n  'to': 'lack of transparency',\n  'name': 'exacerbates',\n  'research': 'challenge',\n  'summary': 'Challenges in transparency and dataset quality are more pronounced in multilingual learning scenarios',\n  'date': '2023-09-17',\n  'title': 'CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large\\n  Language Models in 167 Languages',\n  'id': '2309.09400'},\n {'from': 'multilingual text datasets',\n  'to': 'inadequate data collection and cleaning',\n  'name': 'often',\n  'research': 'challenge',\n  'summary': 'Multilingual text datasets are often inadequately collected and cleaned',\n  'date': '2023-09-17',\n  'title': 'CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large\\n  Language Models in 167 Languages',\n  'id': '2309.09400'},\n {'from': 'Culturax',\n  'to': 'multilingual dataset',\n  'name': 'presented as',\n  'research': 'contribution',\n  'summary': 'Culturax is presented as a substantial multilingual dataset tailored for LLM development, with 6.3 trillion tokens in 167 languages',\n  'date': '2023-09-17',\n  'title': 'CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large\\n  Language Models in 167 Languages',\n  'id': '2309.09400'},\n {'from': 'Culturax',\n  'to': 'cleaning and deduplication',\n  'name': 'undergoes',\n  'research': 'method',\n  'summary': 'Culturax undergoes meticulous cleaning and deduplication through a rigorous pipeline to ensure quality for model training',\n  'date': '2023-09-17',\n  'title': 'CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large\\n  Language Models in 167 Languages',\n  'id': '2309.09400'},\n {'from': 'Culturax',\n  'to': 'Huggingface',\n  'name': 'released on',\n  'research': 'contribution',\n  'summary': 'Culturax is fully released to the public on Huggingface to facilitate research in multilingual LLMs',\n  'date': '2023-09-17',\n  'title': 'CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large\\n  Language Models in 167 Languages',\n  'id': '2309.09400'},\n {'from': 'formal property verification',\n  'to': 'bug finding',\n  'name': 'has been',\n  'research': 'context',\n  'summary': 'Formal Property Verification has been effective at finding intricate RTL bugs',\n  'date': '2023-09-18',\n  'title': 'Using LLMs to Facilitate Formal Verification of RTL',\n  'id': '2309.09437'},\n {'from': 'SystemVerilog Assertions',\n  'to': 'time-consuming and error-prone',\n  'name': 'are',\n  'research': 'challenge',\n  'summary': 'Writing SystemVerilog Assertions is time-consuming and error-prone, even for experienced users',\n  'date': '2023-09-18',\n  'title': 'Using LLMs to Facilitate Formal Verification of RTL',\n  'id': '2309.09437'},\n {'from': 'abstraction level raising',\n  'to': 'SVA generation',\n  'name': 'aims to',\n  'research': 'context',\n  'summary': 'Prior work has attempted to lighten the burden of writing SVA by raising the abstraction level',\n  'date': '2023-09-18',\n  'title': 'Using LLMs to Facilitate Formal Verification of RTL',\n  'id': '2309.09437'},\n {'from': 'large language model',\n  'to': 'SVA generation',\n  'name': 'explored for',\n  'research': 'contribution',\n  'summary': 'The study explores whether LLMs can capture RTL behavior and generate correct SVA properties',\n  'date': '2023-09-18',\n  'title': 'Using LLMs to Facilitate Formal Verification of RTL',\n  'id': '2309.09437'},\n {'from': 'FPV-based evaluation framework',\n  'to': 'correctness and completeness of SVA',\n  'name': 'measures',\n  'research': 'method',\n  'summary': 'An FPV-based evaluation framework is designed to measure the correctness and completeness of SVA',\n  'date': '2023-09-18',\n  'title': 'Using LLMs to Facilitate Formal Verification of RTL',\n  'id': '2309.09437'},\n {'from': 'GPT-4',\n  'to': 'SVA generation',\n  'name': 'used in',\n  'research': 'method',\n  'summary': 'GPT-4 is iteratively evaluated to craft syntax and semantic rules for better SVA generation',\n  'date': '2023-09-18',\n  'title': 'Using LLMs to Facilitate Formal Verification of RTL',\n  'id': '2309.09437'},\n {'from': 'AutoSVA',\n  'to': 'GPT-4',\n  'name': 'extended by',\n  'research': 'contribution',\n  'summary': 'The AutoSVA framework is extended by integrating an improved GPT-4-based flow to generate safety properties',\n  'date': '2023-09-18',\n  'title': 'Using LLMs to Facilitate Formal Verification of RTL',\n  'id': '2309.09437'},\n {'from': 'FPV coverage evaluation',\n  'to': 'GPT-4',\n  'name': 'involves',\n  'research': 'method',\n  'summary': 'Use cases evaluate the FPV coverage of GPT-4-generated SVA on complex open-source RTL',\n  'date': '2023-09-18',\n  'title': 'Using LLMs to Facilitate Formal Verification of RTL',\n  'id': '2309.09437'},\n {'from': 'RTL generation from SVA',\n  'to': 'GPT-4',\n  'name': 'involves',\n  'research': 'method',\n  'summary': 'Use cases involve using generated SVA to prompt GPT-4 to create RTL from scratch',\n  'date': '2023-09-18',\n  'title': 'Using LLMs to Facilitate Formal Verification of RTL',\n  'id': '2309.09437'},\n {'from': 'GPT-4',\n  'to': 'correct SVA',\n  'name': 'generates',\n  'research': 'finding',\n  'summary': 'GPT-4 can generate correct SVA even for flawed RTL, without mirroring design errors',\n  'date': '2023-09-18',\n  'title': 'Using LLMs to Facilitate Formal Verification of RTL',\n  'id': '2309.09437'},\n {'from': 'RISC-V CVA6 core bug',\n  'to': 'GPT-4',\n  'name': 'exposed by',\n  'research': 'finding',\n  'summary': \"GPT-4 generated SVA that exposed a bug in the RISC-V CVA6 core that eluded prior work's evaluation\",\n  'date': '2023-09-18',\n  'title': 'Using LLMs to Facilitate Formal Verification of RTL',\n  'id': '2309.09437'},\n {'from': 'text-to-image generation',\n  'to': 'rapid evolution',\n  'name': 'evolving',\n  'research': 'context',\n  'summary': 'The landscape of text-to-image generation is rapidly evolving',\n  'date': '2023-09-18',\n  'title': 'Progressive Text-to-Image Diffusion with Soft Latent Direction',\n  'id': '2309.09466'},\n {'from': 'synthesis and manipulation of multiple entities',\n  'to': 'adhering to relational constraints',\n  'name': 'poses challenges',\n  'research': 'context',\n  'summary': 'Synthesizing and manipulating multiple entities with specific relational constraints is challenging',\n  'date': '2023-09-18',\n  'title': 'Progressive Text-to-Image Diffusion with Soft Latent Direction',\n  'id': '2309.09466'},\n {'from': 'progressive synthesis and editing',\n  'to': 'entity incorporation with constraints',\n  'name': 'introduced to',\n  'research': 'contribution',\n  'summary': 'An innovative progressive synthesis and editing operation is introduced to incorporate entities into images while adhering to spatial and relational constraints',\n  'date': '2023-09-18',\n  'title': 'Progressive Text-to-Image Diffusion with Soft Latent Direction',\n  'id': '2309.09466'},\n {'from': 'pre-trained text-to-image diffusion model',\n  'to': 'handling multiple entities',\n  'name': 'handles entities',\n  'research': 'context',\n  'summary': 'A pre-trained text-to-image diffusion model adeptly handles one or two entities but often falters with more',\n  'date': '2023-09-18',\n  'title': 'Progressive Text-to-Image Diffusion with Soft Latent Direction',\n  'id': '2309.09466'},\n {'from': 'large language model',\n  'to': 'decomposing text descriptions',\n  'name': 'used to',\n  'research': 'method',\n  'summary': 'A large language model is proposed to decompose complex text descriptions into coherent directives',\n  'date': '2023-09-18',\n  'title': 'Progressive Text-to-Image Diffusion with Soft Latent Direction',\n  'id': '2309.09466'},\n {'from': 'Stimulus, Response, and Fusion (SRF)',\n  'to': 'semantic operations execution',\n  'name': 'formulated for',\n  'research': 'contribution',\n  'summary': 'The SRF framework is formulated to execute directives involving insertion, editing, and erasing',\n  'date': '2023-09-18',\n  'title': 'Progressive Text-to-Image Diffusion with Soft Latent Direction',\n  'id': '2309.09466'},\n {'from': 'Stimulus, Response, and Fusion (SRF)',\n  'to': 'latent region manipulation',\n  'name': 'involves',\n  'research': 'method',\n  'summary': 'Within the SRF framework, latent regions are stimulated and fused to achieve cohesive entity manipulation',\n  'date': '2023-09-18',\n  'title': 'Progressive Text-to-Image Diffusion with Soft Latent Direction',\n  'id': '2309.09466'},\n {'from': 'Stimulus, Response, and Fusion (SRF)',\n  'to': 'object synthesis',\n  'name': 'yields advancements in',\n  'research': 'finding',\n  'summary': 'The SRF framework yields notable advancements in object synthesis for complex textual inputs',\n  'date': '2023-09-18',\n  'title': 'Progressive Text-to-Image Diffusion with Soft Latent Direction',\n  'id': '2309.09466'},\n {'from': 'Stimulus, Response, and Fusion (SRF)',\n  'to': 'text-to-image generation',\n  'name': 'establishes benchmark for',\n  'research': 'impact',\n  'summary': 'The SRF framework establishes a new benchmark for text-to-image generation tasks',\n  'date': '2023-09-18',\n  'title': 'Progressive Text-to-Image Diffusion with Soft Latent Direction',\n  'id': '2309.09466'},\n {'from': 'large language model',\n  'to': 'software engineering',\n  'name': 'revolutionized',\n  'research': 'context',\n  'summary': 'Large language models have revolutionized programming and software engineering',\n  'date': '2023-09-18',\n  'title': 'PwR: Exploring the Role of Representations in Conversational Programming',\n  'id': '2309.09495'},\n {'from': 'AI programming assistant',\n  'to': 'conversational programming',\n  'name': 'enables',\n  'research': 'context',\n  'summary': 'AI programming assistants such as GitHub Copilot X enable conversational programming',\n  'date': '2023-09-18',\n  'title': 'PwR: Exploring the Role of Representations in Conversational Programming',\n  'id': '2309.09495'},\n {'from': 'GitHub Copilot X',\n  'to': 'AI programming assistant',\n  'name': 'example of',\n  'research': 'context',\n  'summary': 'GitHub Copilot X is an example of an AI programming assistant that enables conversational programming',\n  'date': '2023-09-18',\n  'title': 'PwR: Exploring the Role of Representations in Conversational Programming',\n  'id': '2309.09495'},\n {'from': 'gap in system understanding',\n  'to': 'software engineering',\n  'name': 'identified in',\n  'research': 'context',\n  'summary': \"There is a gap between the user's mental model and the AI system's actual understanding after a sequence of natural language utterances\",\n  'date': '2023-09-18',\n  'title': 'PwR: Exploring the Role of Representations in Conversational Programming',\n  'id': '2309.09495'},\n {'from': 'Programming with Representations (PWR)',\n  'to': 'gap in system understanding',\n  'name': 'introduced to address',\n  'research': 'contribution',\n  'summary': \"PWR is introduced to address the gap in system understanding by using representations to convey the system's understanding back to the user in natural language\",\n  'date': '2023-09-18',\n  'title': 'PwR: Exploring the Role of Representations in Conversational Programming',\n  'id': '2309.09495'},\n {'from': 'in-lab task-centered study',\n  'to': 'Programming with Representations (PWR)',\n  'name': 'conducted to test',\n  'research': 'method',\n  'summary': 'An in-lab task-centered study with 14 users of varying programming proficiency was conducted to test PWR',\n  'date': '2023-09-18',\n  'title': 'PwR: Exploring the Role of Representations in Conversational Programming',\n  'id': '2309.09495'},\n {'from': 'improved understandability',\n  'to': 'Programming with Representations (PWR)',\n  'name': 'result of',\n  'research': 'finding',\n  'summary': 'Representations significantly improve understandability and instill a sense of agency among participants',\n  'date': '2023-09-18',\n  'title': 'PwR: Exploring the Role of Representations in Conversational Programming',\n  'id': '2309.09495'},\n {'from': 'expert programmers',\n  'to': 'Programming with Representations (PWR)',\n  'name': 'use PWR for',\n  'research': 'finding',\n  'summary': 'Expert programmers use representations for verification',\n  'date': '2023-09-18',\n  'title': 'PwR: Exploring the Role of Representations in Conversational Programming',\n  'id': '2309.09495'},\n {'from': 'intermediate programmers',\n  'to': 'Programming with Representations (PWR)',\n  'name': 'benefit from PWR through',\n  'research': 'finding',\n  'summary': 'Intermediate programmers benefit from confirmation through representations',\n  'date': '2023-09-18',\n  'title': 'PwR: Exploring the Role of Representations in Conversational Programming',\n  'id': '2309.09495'},\n {'from': 'natural language-based development with large language models',\n  'to': 'Programming with Representations (PWR)',\n  'name': 'coupled with',\n  'research': 'reflection',\n  'summary': 'Natural language-based development with LLMs, coupled with representations, promises to transform software development',\n  'date': '2023-09-18',\n  'title': 'PwR: Exploring the Role of Representations in Conversational Programming',\n  'id': '2309.09495'},\n {'from': 'graphic layout generation',\n  'to': 'user engagement and information perception',\n  'name': 'plays a role in',\n  'research': 'context',\n  'summary': 'Graphic layout generation plays a significant role in user engagement and information perception',\n  'date': '2023-09-18',\n  'title': 'LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language\\n  Models',\n  'id': '2309.09506'},\n {'from': 'numerical optimization',\n  'to': 'layout generation',\n  'name': 'treats',\n  'research': 'context',\n  'summary': 'Existing methods primarily treat layout generation as a numerical optimization task',\n  'date': '2023-09-18',\n  'title': 'LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language\\n  Models',\n  'id': '2309.09506'},\n {'from': 'quantitative',\n  'to': 'numerical optimization',\n  'name': 'focused on by',\n  'research': 'context',\n  'summary': 'Existing methods focus on quantitative aspects of layout generation',\n  'date': '2023-09-18',\n  'title': 'LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language\\n  Models',\n  'id': '2309.09506'},\n {'from': 'semantic information',\n  'to': 'numerical optimization',\n  'name': 'overlooked by',\n  'research': 'context',\n  'summary': 'Existing methods overlook the semantic information of layout',\n  'date': '2023-09-18',\n  'title': 'LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language\\n  Models',\n  'id': '2309.09506'},\n {'from': 'LayoutNuwa',\n  'to': 'code generation',\n  'name': 'is',\n  'research': 'contribution',\n  'summary': 'LayoutNuwa is the first model that treats layout generation as a code generation task to enhance semantic information',\n  'date': '2023-09-18',\n  'title': 'LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language\\n  Models',\n  'id': '2309.09506'},\n {'from': 'large language model',\n  'to': 'LayoutNuwa',\n  'name': 'harnesses',\n  'research': 'contribution',\n  'summary': 'LayoutNuwa harnesses the hidden layout expertise of large language models',\n  'date': '2023-09-18',\n  'title': 'LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language\\n  Models',\n  'id': '2309.09506'},\n {'from': 'Code Instruct Tuning (CIT)',\n  'to': 'LayoutNuwa',\n  'name': 'comprises',\n  'research': 'method',\n  'summary': 'The Code Instruct Tuning (CIT) approach comprises three interconnected modules for layout generation',\n  'date': '2023-09-18',\n  'title': 'LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language\\n  Models',\n  'id': '2309.09506'},\n {'from': 'Code Initialization (CI)',\n  'to': 'Code Instruct Tuning (CIT)',\n  'name': 'quantifies and initializes',\n  'research': 'method',\n  'summary': 'The CI module quantifies numerical conditions and initializes them as HTML code with strategically placed masks',\n  'date': '2023-09-18',\n  'title': 'LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language\\n  Models',\n  'id': '2309.09506'},\n {'from': 'Code Completion (CC)',\n  'to': 'Code Instruct Tuning (CIT)',\n  'name': 'employs',\n  'research': 'method',\n  'summary': 'The CC module employs the formatting knowledge of LLMs to fill in the masked portions within the HTML code',\n  'date': '2023-09-18',\n  'title': 'LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language\\n  Models',\n  'id': '2309.09506'},\n {'from': 'Code Rendering (CR)',\n  'to': 'Code Instruct Tuning (CIT)',\n  'name': 'transforms',\n  'research': 'method',\n  'summary': 'The CR module transforms the completed code into the final layout output, ensuring an interpretable and transparent generation process',\n  'date': '2023-09-18',\n  'title': 'LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language\\n  Models',\n  'id': '2309.09506'},\n {'from': 'state-of-the-art',\n  'to': 'LayoutNuwa',\n  'name': 'attained by',\n  'research': 'result',\n  'summary': 'LayoutNuwa attains significant state-of-the-art performance on multiple datasets',\n  'date': '2023-09-18',\n  'title': 'LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language\\n  Models',\n  'id': '2309.09506'},\n {'from': 'code',\n  'to': 'https://github.com/projectnuwa/layoutnuwa',\n  'name': 'available at',\n  'research': 'additional information',\n  'summary': 'The code for LayoutNuwa is available on GitHub',\n  'date': '2023-09-18',\n  'title': 'LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language\\n  Models',\n  'id': '2309.09506'},\n {'from': 'large language model',\n  'to': 'NLP tasks',\n  'name': 'demonstrate capabilities in',\n  'research': 'context',\n  'summary': 'Large language models with tens of billions of parameters have demonstrated impressive capabilities in various NLP tasks',\n  'date': '2023-09-18',\n  'title': 'Pruning Large Language Models via Accuracy Predictor',\n  'id': '2309.09507'},\n {'from': 'large language model',\n  'to': 'model training, inference, and deployment',\n  'name': 'pose challenges to',\n  'research': 'problem',\n  'summary': 'Substantial model size of LLMs poses challenges to training, inference, and deployment',\n  'date': '2023-09-18',\n  'title': 'Pruning Large Language Models via Accuracy Predictor',\n  'id': '2309.09507'},\n {'from': 'model compression',\n  'to': 'large language model',\n  'name': 'required for',\n  'research': 'context',\n  'summary': 'Model compression is necessary for large language models due to their substantial size',\n  'date': '2023-09-18',\n  'title': 'Pruning Large Language Models via Accuracy Predictor',\n  'id': '2309.09507'},\n {'from': 'manual pruning',\n  'to': 'complex optimization pipeline and difficulty in retaining capabilities',\n  'name': 'has problems such as',\n  'research': 'problem',\n  'summary': 'Most model compression for LLMs requires manual design of pruning features, which leads to a complex optimization pipeline and difficulty in retaining capabilities of certain model parts',\n  'date': '2023-09-18',\n  'title': 'Pruning Large Language Models via Accuracy Predictor',\n  'id': '2309.09507'},\n {'from': 'novel pruning',\n  'to': 'model compression',\n  'name': 'proposed for',\n  'research': 'contribution',\n  'summary': 'A novel pruning approach is proposed to address the challenges of model compression for LLMs',\n  'date': '2023-09-18',\n  'title': 'Pruning Large Language Models via Accuracy Predictor',\n  'id': '2309.09507'},\n {'from': 'accuracy predictor',\n  'to': 'search space optimization',\n  'name': 'trained as',\n  'research': 'method',\n  'summary': 'A non-neural model is trained as an accuracy predictor to optimize the search space and search for the optimal model',\n  'date': '2023-09-18',\n  'title': 'Pruning Large Language Models via Accuracy Predictor',\n  'id': '2309.09507'},\n {'from': 'WikiText-2',\n  'to': 'perplexity reduction',\n  'name': 'perplexity reduced by',\n  'research': 'result',\n  'summary': 'The perplexity (PPL) on WikiText-2 dropped by 9.48% using the proposed pruning approach',\n  'date': '2023-09-18',\n  'title': 'Pruning Large Language Models via Accuracy Predictor',\n  'id': '2309.09507'},\n {'from': 'PTB',\n  'to': 'perplexity reduction',\n  'name': 'perplexity reduced by',\n  'research': 'result',\n  'summary': 'The perplexity (PPL) on PTB dropped by 5.76% using the proposed pruning approach',\n  'date': '2023-09-18',\n  'title': 'Pruning Large Language Models via Accuracy Predictor',\n  'id': '2309.09507'},\n {'from': 'MMLU average accuracy',\n  'to': 'accuracy increase',\n  'name': 'increased by',\n  'research': 'result',\n  'summary': 'The average accuracy of MMLU increased by 6.28% with the proposed pruning approach',\n  'date': '2023-09-18',\n  'title': 'Pruning Large Language Models via Accuracy Predictor',\n  'id': '2309.09507'},\n {'from': 'text language model',\n  'to': 'zero-shot generalization',\n  'name': 'has capability',\n  'research': 'context',\n  'summary': 'Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks with well-formulated instructions',\n  'date': '2023-09-18',\n  'title': 'Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive\\n  Instruction-Tuning Benchmark for Speech',\n  'id': '2309.09510'},\n {'from': 'speech processing',\n  'to': 'specific tasks',\n  'name': 'focuses on',\n  'research': 'context',\n  'summary': 'Existing studies in speech processing primarily focus on limited or specific tasks',\n  'date': '2023-09-18',\n  'title': 'Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive\\n  Instruction-Tuning Benchmark for Speech',\n  'id': '2309.09510'},\n {'from': 'lack of standardized benchmarks',\n  'to': 'fair comparison',\n  'name': 'hinders',\n  'research': 'context',\n  'summary': 'The lack of standardized benchmarks hinders fair comparison across different speech processing approaches',\n  'date': '2023-09-18',\n  'title': 'Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive\\n  Instruction-Tuning Benchmark for Speech',\n  'id': '2309.09510'},\n {'from': 'Dynamic-SuPERB',\n  'to': 'universal speech model',\n  'name': 'designed for',\n  'research': 'contribution',\n  'summary': 'Dynamic-SuPERB is designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion',\n  'date': '2023-09-18',\n  'title': 'Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive\\n  Instruction-Tuning Benchmark for Speech',\n  'id': '2309.09510'},\n {'from': 'Dynamic-SuPERB',\n  'to': 'research community',\n  'name': 'invites',\n  'research': 'contribution',\n  'summary': 'Dynamic-SuPERB invites community collaboration to facilitate its dynamic growth',\n  'date': '2023-09-18',\n  'title': 'Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive\\n  Instruction-Tuning Benchmark for Speech',\n  'id': '2309.09510'},\n {'from': 'Dynamic-SuPERB',\n  'to': '55 evaluation instances',\n  'name': 'features',\n  'research': 'contribution',\n  'summary': 'Dynamic-SuPERB features 55 evaluation instances combining 33 tasks and 22 datasets',\n  'date': '2023-09-18',\n  'title': 'Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive\\n  Instruction-Tuning Benchmark for Speech',\n  'id': '2309.09510'},\n {'from': 'Dynamic-SuPERB',\n  'to': 'comprehensive evaluation platform',\n  'name': 'provides',\n  'research': 'contribution',\n  'summary': 'Dynamic-SuPERB provides a comprehensive platform for evaluation across a broad spectrum of speech tasks',\n  'date': '2023-09-18',\n  'title': 'Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive\\n  Instruction-Tuning Benchmark for Speech',\n  'id': '2309.09510'},\n {'from': 'benchmark baselines',\n  'to': 'Dynamic-SuPERB',\n  'name': 'established by',\n  'research': 'method',\n  'summary': 'Several approaches are proposed to establish benchmark baselines, including the use of speech models, text language models, and the multimodal encoder',\n  'date': '2023-09-18',\n  'title': 'Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive\\n  Instruction-Tuning Benchmark for Speech',\n  'id': '2309.09510'},\n {'from': 'multimodal encoder',\n  'to': 'Dynamic-SuPERB',\n  'name': 'utilized in',\n  'research': 'method',\n  'summary': 'The multimodal encoder is utilized to establish benchmark baselines for Dynamic-SuPERB',\n  'date': '2023-09-18',\n  'title': 'Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive\\n  Instruction-Tuning Benchmark for Speech',\n  'id': '2309.09510'},\n {'from': 'ablation study',\n  'to': 'performance improvement',\n  'name': 'conducted to',\n  'research': 'method',\n  'summary': 'An ablation study was conducted to assess robustness and seek performance improvements',\n  'date': '2023-09-18',\n  'title': 'Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive\\n  Instruction-Tuning Benchmark for Speech',\n  'id': '2309.09510'},\n {'from': 'research community',\n  'to': 'Dynamic-SuPERB',\n  'name': 'invited to',\n  'research': 'invitation',\n  'summary': 'Researchers are welcomed to collaborate on the Dynamic-SuPERB project, advancing technologies together',\n  'date': '2023-09-18',\n  'title': 'Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive\\n  Instruction-Tuning Benchmark for Speech',\n  'id': '2309.09510'},\n {'from': 'continued pre-training',\n  'to': 'large language model',\n  'name': 'influences',\n  'research': 'finding',\n  'summary': 'Continued pre-training on domain-specific corpora influences large language models',\n  'date': '2023-09-18',\n  'title': 'Adapting Large Language Models via Reading Comprehension',\n  'id': '2309.09530'},\n {'from': 'large language model',\n  'to': 'domain knowledge',\n  'name': 'endowed with',\n  'research': 'finding',\n  'summary': 'Training on raw corpora endows the model with domain knowledge',\n  'date': '2023-09-18',\n  'title': 'Adapting Large Language Models via Reading Comprehension',\n  'id': '2309.09530'},\n {'from': 'large language model',\n  'to': 'question answering',\n  'name': 'hurts ability in',\n  'research': 'finding',\n  'summary': \"Training on raw corpora drastically hurts the model's prompting ability for question answering\",\n  'date': '2023-09-18',\n  'title': 'Adapting Large Language Models via Reading Comprehension',\n  'id': '2309.09530'},\n {'from': 'reading comprehension-based training',\n  'to': 'human learning',\n  'name': 'inspired by',\n  'research': 'context',\n  'summary': 'Inspired by human learning via reading comprehension, where practice after reading improves the ability to answer questions',\n  'date': '2023-09-18',\n  'title': 'Adapting Large Language Models via Reading Comprehension',\n  'id': '2309.09530'},\n {'from': 'reading comprehension-based training',\n  'to': 'reading comprehension texts',\n  'name': 'transforms',\n  'research': 'contribution',\n  'summary': 'A simple method for transforming raw corpora into reading comprehension texts, enriching each raw text with a series of tasks related to its content',\n  'date': '2023-09-18',\n  'title': 'Adapting Large Language Models via Reading Comprehension',\n  'id': '2309.09530'},\n {'from': 'reading comprehension-based training',\n  'to': 'biomedicine, finance, and law',\n  'name': 'enhances performance in',\n  'research': 'finding',\n  'summary': 'The method consistently enhances performance across various tasks in three different domains: biomedicine, finance, and law',\n  'date': '2023-09-18',\n  'title': 'Adapting Large Language Models via Reading Comprehension',\n  'id': '2309.09530'},\n {'from': '7B language model',\n  'to': 'BloombergGPT-50B',\n  'name': 'achieves performance',\n  'research': 'finding',\n  'summary': 'The 7B language model achieves competitive performance with domain-specific models of much larger scales, such as BloombergGPT-50B',\n  'date': '2023-09-18',\n  'title': 'Adapting Large Language Models via Reading Comprehension',\n  'id': '2309.09530'},\n {'from': 'domain-specific reading comprehension-based large language model',\n  'to': 'general benchmarks',\n  'name': 'improves performance on',\n  'research': 'finding',\n  'summary': \"Domain-specific reading comprehension texts can improve the model's performance even on general benchmarks\",\n  'date': '2023-09-18',\n  'title': 'Adapting Large Language Models via Reading Comprehension',\n  'id': '2309.09530'},\n {'from': 'general large language model',\n  'to': 'more domains',\n  'name': 'potential for development across',\n  'research': 'reflection',\n  'summary': 'There is potential to develop a general model across even more domains',\n  'date': '2023-09-18',\n  'title': 'Adapting Large Language Models via Reading Comprehension',\n  'id': '2309.09530'},\n {'from': 'model, code, and data',\n  'to': 'https://github.com/microsoft/lmops',\n  'name': 'will be available at',\n  'research': 'resource',\n  'summary': 'The model, code, and data will be available at the provided GitHub link',\n  'date': '2023-09-18',\n  'title': 'Adapting Large Language Models via Reading Comprehension',\n  'id': '2309.09530'},\n {'from': 'large language model',\n  'to': 'summarization',\n  'name': 'evaluated for',\n  'research': 'contribution',\n  'summary': 'LLMs are evaluated for their zero-shot generation capability across five distinct summarization tasks',\n  'date': '2023-09-18',\n  'title': 'Summarization is (Almost) Dead',\n  'id': '2309.09558'},\n {'from': 'human evaluation',\n  'to': 'large language model',\n  'name': 'conducted to assess',\n  'research': 'method',\n  'summary': 'Human evaluation experiments are conducted to evaluate the summarization capability of LLMs',\n  'date': '2023-09-18',\n  'title': 'Summarization is (Almost) Dead',\n  'id': '2309.09558'},\n {'from': 'human evaluators',\n  'to': 'LLM-generated summaries',\n  'name': 'prefer',\n  'research': 'finding',\n  'summary': 'Human evaluators show a clear preference for LLM-generated summaries over human-written summaries and summaries generated by fine-tuned models',\n  'date': '2023-09-18',\n  'title': 'Summarization is (Almost) Dead',\n  'id': '2309.09558'},\n {'from': 'LLM-generated summaries',\n  'to': 'factual consistency',\n  'name': 'exhibit',\n  'research': 'finding',\n  'summary': 'LLM-generated summaries exhibit better factual consistency and fewer instances of extrinsic hallucinations',\n  'date': '2023-09-18',\n  'title': 'Summarization is (Almost) Dead',\n  'id': '2309.09558'},\n {'from': 'LLM-generated summaries',\n  'to': 'extrinsic hallucinations',\n  'name': 'have fewer',\n  'research': 'finding',\n  'summary': 'LLM-generated summaries have fewer instances of extrinsic hallucinations',\n  'date': '2023-09-18',\n  'title': 'Summarization is (Almost) Dead',\n  'id': '2309.09558'},\n {'from': 'LLMs in summarization tasks',\n  'to': 'reference summaries',\n  'name': 'surpasses',\n  'research': 'finding',\n  'summary': 'The performance of LLMs in summarization tasks surpasses the benchmark of reference summaries',\n  'date': '2023-09-18',\n  'title': 'Summarization is (Almost) Dead',\n  'id': '2309.09558'},\n {'from': 'text summarization',\n  'to': 'large language model',\n  'name': 'challenged by',\n  'research': 'reflection',\n  'summary': 'Most conventional works in text summarization are no longer necessary in the era of LLMs',\n  'date': '2023-09-18',\n  'title': 'Summarization is (Almost) Dead',\n  'id': '2309.09558'},\n {'from': 'novel dataset creation',\n  'to': 'text summarization',\n  'name': 'worth exploring',\n  'research': 'reflection',\n  'summary': 'The creation of novel datasets with higher quality is a direction worth exploring',\n  'date': '2023-09-18',\n  'title': 'Summarization is (Almost) Dead',\n  'id': '2309.09558'},\n ...]"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rels = [{'from':fix_name(r.nodes[0]['name']), 'to':fix_name(r.nodes[1]['name']), **dict(r)} for r in existing_relations]\n",
    "rels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T10:48:47.811526Z",
     "start_time": "2023-12-10T10:48:47.481525900Z"
    }
   },
   "id": "61b1cd34db1e8fac"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "rels=pd.DataFrame.from_dict(rels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T10:49:15.096305800Z",
     "start_time": "2023-12-10T10:49:15.037809500Z"
    }
   },
   "id": "d4891e9184724621"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              from  \\\n0        clinical trial eligibility identification   \n1                      text classification methods   \n2                Phase III cancer trial exclusions   \n3                                      Transformer   \n4                              Clinical Trial BERT   \n...                                            ...   \n29524        universal visual in-context prompting   \n29525           referring and generic segmentation   \n29526                      competitive performance   \n29527  universal visual in-context prompting model   \n29528                                         code   \n\n                                              to                  name  \\\n0          natural language eligibility criteria     is complicated by   \n1      clinical trial eligibility identification          employed for   \n2                    764 Phase III cancer trials           consists of   \n3              exclusion criteria classification     experimented with   \n4                                    Transformer                  is a   \n...                                          ...                   ...   \n29524                   reference image segments      enhanced to take   \n29525      universal visual in-context prompting           elicited by   \n29526      universal visual in-context prompting           achieved by   \n29527                      PQ (Panoptic Quality)              achieves   \n29528        https://github.com/ux-decoder/dinov  will be available at   \n\n                     research  \\\n0                     problem   \n1                    approach   \n2                     context   \n3                      method   \n4                     context   \n...                       ...   \n29524                  method   \n29525                 finding   \n29526                 finding   \n29527                  result   \n29528  additional information   \n\n                                                 summary        date  \\\n0      Automatic identification of clinical trials fo...  2023-09-14   \n1      Text classification methods are employed for c...  2023-09-14   \n2      The dataset consists of 764 Phase III cancer t...  2023-09-14   \n3      Common Transformer models are experimented wit...  2023-09-14   \n4      A new pre-trained Clinical Trial BERT model is...  2023-09-14   \n...                                                  ...         ...   \n29524  The framework is enhanced to take an arbitrary...  2023-11-22   \n29525  The proposed visual in-context prompting elici...  2023-11-22   \n29526  The model yields competitive performance to cl...  2023-11-22   \n29527  By joint training on COCO and SA-1B, the model...  2023-11-22   \n29528  Code for the universal visual in-context promp...  2023-11-22   \n\n                                                   title          id  \n0      Text Classification of Cancer Clinical Trial E...  2309.07812  \n1      Text Classification of Cancer Clinical Trial E...  2309.07812  \n2      Text Classification of Cancer Clinical Trial E...  2309.07812  \n3      Text Classification of Cancer Clinical Trial E...  2309.07812  \n4      Text Classification of Cancer Clinical Trial E...  2309.07812  \n...                                                  ...         ...  \n29524                        Visual In-Context Prompting  2311.13601  \n29525                        Visual In-Context Prompting  2311.13601  \n29526                        Visual In-Context Prompting  2311.13601  \n29527                        Visual In-Context Prompting  2311.13601  \n29528                        Visual In-Context Prompting  2311.13601  \n\n[29529 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>from</th>\n      <th>to</th>\n      <th>name</th>\n      <th>research</th>\n      <th>summary</th>\n      <th>date</th>\n      <th>title</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>clinical trial eligibility identification</td>\n      <td>natural language eligibility criteria</td>\n      <td>is complicated by</td>\n      <td>problem</td>\n      <td>Automatic identification of clinical trials fo...</td>\n      <td>2023-09-14</td>\n      <td>Text Classification of Cancer Clinical Trial E...</td>\n      <td>2309.07812</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>text classification methods</td>\n      <td>clinical trial eligibility identification</td>\n      <td>employed for</td>\n      <td>approach</td>\n      <td>Text classification methods are employed for c...</td>\n      <td>2023-09-14</td>\n      <td>Text Classification of Cancer Clinical Trial E...</td>\n      <td>2309.07812</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Phase III cancer trial exclusions</td>\n      <td>764 Phase III cancer trials</td>\n      <td>consists of</td>\n      <td>context</td>\n      <td>The dataset consists of 764 Phase III cancer t...</td>\n      <td>2023-09-14</td>\n      <td>Text Classification of Cancer Clinical Trial E...</td>\n      <td>2309.07812</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Transformer</td>\n      <td>exclusion criteria classification</td>\n      <td>experimented with</td>\n      <td>method</td>\n      <td>Common Transformer models are experimented wit...</td>\n      <td>2023-09-14</td>\n      <td>Text Classification of Cancer Clinical Trial E...</td>\n      <td>2309.07812</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Clinical Trial BERT</td>\n      <td>Transformer</td>\n      <td>is a</td>\n      <td>context</td>\n      <td>A new pre-trained Clinical Trial BERT model is...</td>\n      <td>2023-09-14</td>\n      <td>Text Classification of Cancer Clinical Trial E...</td>\n      <td>2309.07812</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>29524</th>\n      <td>universal visual in-context prompting</td>\n      <td>reference image segments</td>\n      <td>enhanced to take</td>\n      <td>method</td>\n      <td>The framework is enhanced to take an arbitrary...</td>\n      <td>2023-11-22</td>\n      <td>Visual In-Context Prompting</td>\n      <td>2311.13601</td>\n    </tr>\n    <tr>\n      <th>29525</th>\n      <td>referring and generic segmentation</td>\n      <td>universal visual in-context prompting</td>\n      <td>elicited by</td>\n      <td>finding</td>\n      <td>The proposed visual in-context prompting elici...</td>\n      <td>2023-11-22</td>\n      <td>Visual In-Context Prompting</td>\n      <td>2311.13601</td>\n    </tr>\n    <tr>\n      <th>29526</th>\n      <td>competitive performance</td>\n      <td>universal visual in-context prompting</td>\n      <td>achieved by</td>\n      <td>finding</td>\n      <td>The model yields competitive performance to cl...</td>\n      <td>2023-11-22</td>\n      <td>Visual In-Context Prompting</td>\n      <td>2311.13601</td>\n    </tr>\n    <tr>\n      <th>29527</th>\n      <td>universal visual in-context prompting model</td>\n      <td>PQ (Panoptic Quality)</td>\n      <td>achieves</td>\n      <td>result</td>\n      <td>By joint training on COCO and SA-1B, the model...</td>\n      <td>2023-11-22</td>\n      <td>Visual In-Context Prompting</td>\n      <td>2311.13601</td>\n    </tr>\n    <tr>\n      <th>29528</th>\n      <td>code</td>\n      <td>https://github.com/ux-decoder/dinov</td>\n      <td>will be available at</td>\n      <td>additional information</td>\n      <td>Code for the universal visual in-context promp...</td>\n      <td>2023-11-22</td>\n      <td>Visual In-Context Prompting</td>\n      <td>2311.13601</td>\n    </tr>\n  </tbody>\n</table>\n<p>29529 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T10:49:17.877774Z",
     "start_time": "2023-12-10T10:49:17.800270300Z"
    }
   },
   "id": "2654d0323b36978e"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "reinforcement learning from human feedback    62\npretrained language model                     60\ninstruction tuning                            58\nartificial intelligence                       57\nNLP tasks                                     55\nreinforcement learning                        53\ncode generation                               53\nhallucinations                                52\nstate-of-the-art                              50\nLLaMA                                         49\nhuman evaluation                              48\nextensive experiments                         47\ndownstream tasks                              47\nBERT                                          47\nlogical reasoning                             45\nGPT-3                                         45\ntext generation                               43\nthis study                                    41\nprompting                                     41\nfew-shot learning                             40\naccuracy                                      39\nthis work                                     39\nprivacy                                       39\nfoundation model                              38\nzero-shot learning                            38\nmulti-modal large language model              38\nreasoning                                     38\nparameter-efficient fine-tuning               37\nhealthcare                                    37\ndata augmentation                             34\nName: count, dtype: int64"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_con = rels[['from', 'to']].stack().reset_index(drop=True)\n",
    "node_con.value_counts().iloc[20:50]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T13:45:00.182104600Z",
     "start_time": "2023-12-10T13:45:00.054608100Z"
    }
   },
   "id": "ce8c51a1941accc4"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "rels.to_csv('..\\\\data\\\\rels.csv', index=False, quotechar='\"')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T12:33:56.578308900Z",
     "start_time": "2023-12-10T12:33:56.197309500Z"
    }
   },
   "id": "cb8fddfce91c2bc7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LOAD CSV WITH HEADERS FROM 'https://drive.google.com/uc?export=download&id=1eKfvVWMlbtRZCV3HZHtwDAnW6GAhOsVm' AS row CALL {with row call apoc.create.node(row.label, {name:row.name}) yield node return null as n} return count(n)\n",
    "\n",
    "create index node_name if not exists for (n:ALL) on (n.name)\n",
    "\n",
    "LOAD CSV WITH HEADERS FROM 'https://drive.google.com/uc?export=download&id=1AAgs7x9DLO7lecNcVfL-Zgz02ZamAAFv' AS row MATCH (n1 {name:row.from}) MATCH (n2 {name:row.to}) CALL apoc.create.relationship(n1, row.name, {research: row.research, summary:row.summary, date:row.date, title:row.title, id:row.id}, n2) YIELD rel RETURN count(rel)\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e345e74484667bce"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "recreate_db(graph, existing_nodes, existing_relations)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T18:33:40.680157700Z",
     "start_time": "2023-12-09T18:31:06.345616700Z"
    }
   },
   "id": "6ae6ce2829fc65db"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "8983"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(existing_relations)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T22:47:41.725770200Z",
     "start_time": "2023-12-03T22:47:41.651272200Z"
    }
   },
   "id": "76fb8aa2cdf30cd5"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "recreate_db(graph, existing_nodes, existing_relations)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T16:36:00.666165300Z",
     "start_time": "2023-10-09T16:28:10.548094300Z"
    }
   },
   "id": "d77cd1e1674b61d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "existing_nodes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85be657dac022f82"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "node_names = [er['name'] for er in existing_nodes.values()]\n",
    "node_names = set(node_names[:500])\n",
    "len(node_names)\n",
    "messages = [\n",
    "        SystemMessage(\n",
    "            content='This is a list of nodes in a graph database. Identify the ones that are identical in meaning and could be merged into a single node.'\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=str(node_names)\n",
    "        ),\n",
    "    ]\n",
    "response = cllm(messages, temperature=0.1).content\n",
    "response"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0fc98566b22b612"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "'  We present an approach to improving the precision of an initial document\\nranking wherein we utilize cluster information within a graph-based framework.\\nThe main idea is to perform re-ranking based on centrality within bipartite\\ngraphs of documents (on one side) and clusters (on the other side), on the\\npremise that these are mutually reinforcing entities. Links between entities\\nare created via consideration of language models induced from them.\\n  We find that our cluster-document graphs give rise to much better retrieval\\nperformance than previously proposed document-only graphs do. For example,\\nauthority-based re-ranking of documents via a HITS-style cluster-based approach\\noutperforms a previously-proposed PageRank-inspired algorithm applied to\\nsolely-document graphs. Moreover, we also show that computing authority scores\\nfor clusters constitutes an effective method for identifying clusters\\ncontaining a large percentage of relevant documents.\\n'"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['abstract'][3 ]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T18:09:04.958140100Z",
     "start_time": "2023-09-07T18:09:04.932372200Z"
    }
   },
   "id": "17e4a959657909f1"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['topic', 'graphical modeling'], [' topic', 'non-negative matrix factorization'], [' application', 'computational auditory scene analysis'], [' application', 'music transcription'], [' application', 'source separation'], [' application', 'speech recognition'], [' contribution', 'inference and learning algorithms'], [' contribution', 'hierarchical state transition model'], [' contribution', 'target tracking'], [' contribution', 'extracting hierarchical features'], [' contribution', 'language modeling\\nAbstract', '   We present a novel approach for sentiment analysis using deep learning\\ntechniques. Our model utilizes a convolutional neural network (CNN) to capture\\nlocal patterns in text data and a long short-term memory (LSTM) network to\\ncapture long-range dependencies. We also incorporate word embeddings to\\nrepresent the semantic meaning of words. The model is trained on a large\\nlabeled dataset and achieves state-of-the-art performance on several benchmark\\ndatasets. We conduct extensive experiments to analyze the effectiveness of\\ndifferent components of our model and compare it with existing approaches. Our\\nresults demonstrate the superiority of our approach in terms of accuracy and\\nrobustness. We also provide insights into the learned representations and\\nhighlight the importance of different features in sentiment analysis.\\n\\ntopic', 'sentiment analysis'], [' using', 'deep learning'], [' using', 'convolutional neural network'], [' using', 'long short-term memory network'], [' using', 'word embeddings'], [' contribution', 'state-of-the-art performance'], [' contribution', 'effectiveness analysis'], [' contribution', 'feature importance analysis']]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "2 columns passed, passed data had 4 columns",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[1;32m~\\Documents\\constellation\\venv39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:939\u001B[0m, in \u001B[0;36m_finalize_columns_and_data\u001B[1;34m(content, columns, dtype)\u001B[0m\n\u001B[0;32m    938\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 939\u001B[0m     columns \u001B[38;5;241m=\u001B[39m \u001B[43m_validate_or_indexify_columns\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    940\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m    941\u001B[0m     \u001B[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\constellation\\venv39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:986\u001B[0m, in \u001B[0;36m_validate_or_indexify_columns\u001B[1;34m(content, columns)\u001B[0m\n\u001B[0;32m    984\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_mi_list \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(columns) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(content):  \u001B[38;5;66;03m# pragma: no cover\u001B[39;00m\n\u001B[0;32m    985\u001B[0m     \u001B[38;5;66;03m# caller's responsibility to check for this...\u001B[39;00m\n\u001B[1;32m--> 986\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[0;32m    987\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(columns)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m columns passed, passed data had \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    988\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(content)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m columns\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    989\u001B[0m     )\n\u001B[0;32m    990\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_mi_list:\n\u001B[0;32m    991\u001B[0m     \u001B[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001B[39;00m\n",
      "\u001B[1;31mAssertionError\u001B[0m: 2 columns passed, passed data had 4 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[77], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m df_list \u001B[38;5;241m=\u001B[39m [t\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m}\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m texts]\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(df_list)\n\u001B[1;32m----> 4\u001B[0m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mKey\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mValue\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\constellation\\venv39\\lib\\site-packages\\pandas\\core\\frame.py:809\u001B[0m, in \u001B[0;36mDataFrame.__init__\u001B[1;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[0;32m    807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m columns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    808\u001B[0m         columns \u001B[38;5;241m=\u001B[39m ensure_index(columns)\n\u001B[1;32m--> 809\u001B[0m     arrays, columns, index \u001B[38;5;241m=\u001B[39m \u001B[43mnested_data_to_arrays\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    810\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001B[39;49;00m\n\u001B[0;32m    811\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001B[39;49;00m\n\u001B[0;32m    812\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    813\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    814\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[0;32m    815\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    816\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    817\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m arrays_to_mgr(\n\u001B[0;32m    818\u001B[0m         arrays,\n\u001B[0;32m    819\u001B[0m         columns,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    822\u001B[0m         typ\u001B[38;5;241m=\u001B[39mmanager,\n\u001B[0;32m    823\u001B[0m     )\n\u001B[0;32m    824\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Documents\\constellation\\venv39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:520\u001B[0m, in \u001B[0;36mnested_data_to_arrays\u001B[1;34m(data, columns, index, dtype)\u001B[0m\n\u001B[0;32m    517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_named_tuple(data[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;129;01mand\u001B[39;00m columns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    518\u001B[0m     columns \u001B[38;5;241m=\u001B[39m ensure_index(data[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39m_fields)\n\u001B[1;32m--> 520\u001B[0m arrays, columns \u001B[38;5;241m=\u001B[39m \u001B[43mto_arrays\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    521\u001B[0m columns \u001B[38;5;241m=\u001B[39m ensure_index(columns)\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Documents\\constellation\\venv39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:845\u001B[0m, in \u001B[0;36mto_arrays\u001B[1;34m(data, columns, dtype)\u001B[0m\n\u001B[0;32m    842\u001B[0m     data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mtuple\u001B[39m(x) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m data]\n\u001B[0;32m    843\u001B[0m     arr \u001B[38;5;241m=\u001B[39m _list_to_arrays(data)\n\u001B[1;32m--> 845\u001B[0m content, columns \u001B[38;5;241m=\u001B[39m \u001B[43m_finalize_columns_and_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    846\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m content, columns\n",
      "File \u001B[1;32m~\\Documents\\constellation\\venv39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:942\u001B[0m, in \u001B[0;36m_finalize_columns_and_data\u001B[1;34m(content, columns, dtype)\u001B[0m\n\u001B[0;32m    939\u001B[0m     columns \u001B[38;5;241m=\u001B[39m _validate_or_indexify_columns(contents, columns)\n\u001B[0;32m    940\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m    941\u001B[0m     \u001B[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001B[39;00m\n\u001B[1;32m--> 942\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(err) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m    944\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(contents) \u001B[38;5;129;01mand\u001B[39;00m contents[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m np\u001B[38;5;241m.\u001B[39mobject_:\n\u001B[0;32m    945\u001B[0m     contents \u001B[38;5;241m=\u001B[39m convert_object_array(contents, dtype\u001B[38;5;241m=\u001B[39mdtype)\n",
      "\u001B[1;31mValueError\u001B[0m: 2 columns passed, passed data had 4 columns"
     ]
    }
   ],
   "source": [
    "texts = response.content.replace('Result: ', '').split(',')\n",
    "df_list = [t.replace('{', '').replace('}','').split(':') for t in texts]\n",
    "print(df_list)\n",
    "#pd.DataFrame(df_list, columns=['Key', 'Value'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T18:13:09.835096400Z",
     "start_time": "2023-09-07T18:13:09.667099100Z"
    }
   },
   "id": "90868e48f1d8de17"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='This paper improves document ranking precision using cluster information in a graph-based framework.', additional_kwargs={}, example=False)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = f'What does this paper improve? Answer in as few words as possible. Abstract: {ds[\"abstract\"][3]}'\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content='None'\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=content\n",
    "    ),\n",
    "]\n",
    "cllm(messages, temperature=0.4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T13:31:30.356185Z",
     "start_time": "2023-09-07T13:31:28.416802Z"
    }
   },
   "id": "26a76661afe9933e"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='The main contributions of this paper are:\\n- Utilizing cluster information to improve document ranking\\n- Re-ranking based on centrality in bipartite graphs of documents and clusters\\n- Creating links between entities using language models\\n- Outperforming previously proposed algorithms for document ranking\\n- Identifying clusters with a high percentage of relevant documents using authority scores.', additional_kwargs={}, example=False)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = f'What are the main contributions of this paperr? Answer in as few words as possible. Abstract: {ds[\"abstract\"][3]}'\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content='None'\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=content\n",
    "    ),\n",
    "]\n",
    "cllm(messages, temperature=0.4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T14:50:14.994256900Z",
     "start_time": "2023-09-07T14:50:09.894369600Z"
    }
   },
   "id": "c3b19395d1c811af"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is the main topic or subject of the research?\n",
      "word prediction systems and the use of Latent Semantic Analysis (LSA)\n",
      "2. What is the objective or purpose of the research?\n",
      "to explore the predictive powers of LSA and evaluate its integration with a standard language model\n",
      "3. What are the key findings or results mentioned in the abstract?\n",
      "all methods integrating LSA-based information showed significant improvements compared to the baseline and a simple cache model\n",
      "4. What methodologies or techniques were used in the study?\n",
      "Latent Semantic Analysis (LSA) and integration with a standard language model\n",
      "5. What are the datasets or resources mentioned?\n",
      "not specified\n",
      "6. What are the implications or applications of the research mentioned?\n",
      "improving word prediction systems by integrating LSA-based information\n"
     ]
    },
    {
     "data": {
      "text/plain": "AIMessage(content='Entities:\\n1. Latent Semantic Analysis (LSA)\\n2. word prediction systems\\n3. standard language model\\n4. baseline\\n5. cache model', additional_kwargs={}, example=False)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "system_msg = 'Answer each question about the given abstract in as few words as possible, without repeating words from the question: 1. What is the main topic or subject of the research?\\n2. What is the objective or purpose of the research?\\n3. What are the key findings or results mentioned in the abstract?\\n4. What methodologies or techniques were used in the study?\\n5. What are the datasets or resources mentioned?\\n6. What are the implications or applications of the research mentioned? Example: \"This paper describes experiments on identifying the language of a single name\\nin isolation or in a document written in a different language. A new corpus has\\nbeen compiled and made available, matching names against languages. This corpus\\nis used in a series of experiments measuring the performance of general\\nlanguage models and names-only language models on the language identification\\ntask. Conclusions are drawn from the comparison between using general language\\nmodels and names-only language models and between identifying the language of\\nisolated names and the language of very short document fragments. Future\\nresearch directions are outlined.\\n\" should result in \"1. language_identification.names; 2.comparison; 3. not specified; 4. not specified; 5. new_dataset.noname; 6. clarify '\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=system_msg\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=ds['abstract'][2]\n",
    "    ),\n",
    "]\n",
    "response = cllm(messages, temperature=0.1)\n",
    "print(response.content)\n",
    "system_msg = 'Get the entities from the response'\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=system_msg\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=response.content\n",
    "    ),\n",
    "]\n",
    "cllm(messages, temperature=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T15:19:52.147841100Z",
     "start_time": "2023-09-01T15:19:44.532726600Z"
    }
   },
   "id": "d5d88fcb94ffc7aa"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "'  This paper describes experiments on identifying the language of a single name\\nin isolation or in a document written in a different language. A new corpus has\\nbeen compiled and made available, matching names against languages. This corpus\\nis used in a series of experiments measuring the performance of general\\nlanguage models and names-only language models on the language identification\\ntask. Conclusions are drawn from the comparison between using general language\\nmodels and names-only language models and between identifying the language of\\nisolated names and the language of very short document fragments. Future\\nresearch directions are outlined.\\n'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['abstract'][1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:29:21.546372900Z",
     "start_time": "2023-09-07T09:29:21.535873200Z"
    }
   },
   "id": "968ffe69319fcfc"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='1. Language identification task\\n2. Names\\n3. Languages\\n4. Corpus\\n5. General language models\\n6. Names-only language models\\n7. Performance measurement\\n8. Document fragments\\n9. Comparison analysis\\n10. Conclusions\\n11. Future research directions', additional_kwargs={}, example=False)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_msg = 'List the main entities from a research perspective.'\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=system_msg\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=ds['abstract'][1].replace('\\n', ' ')\n",
    "    ),\n",
    "]\n",
    "cllm(messages, temperature=0.1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:29:49.602606800Z",
     "start_time": "2023-09-07T09:29:46.567488200Z"
    }
   },
   "id": "e2ff24c483c7890"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='1. Identification task\\n2. Language\\n3. Name\\n4. Experiments\\n5. Isolation\\n6. Document\\n7. Corpus\\n8. Performance\\n9. Models\\n10. Comparison\\n11. Fragments\\n12. Research', additional_kwargs={}, example=False)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_msg = 'List relations between the following 2 entities from the given abstract, using at most 2 words per relation: 1. Language identification task\\n2. Names'\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=system_msg\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=ds['abstract'][1].replace('\\n', ' ')\n",
    "    ),\n",
    "]\n",
    "cllm(messages, temperature=0.1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:33:45.728322100Z",
     "start_time": "2023-09-07T09:33:42.982893700Z"
    }
   },
   "id": "22ca5dafabb1f1c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dc80e2890d4db192"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
